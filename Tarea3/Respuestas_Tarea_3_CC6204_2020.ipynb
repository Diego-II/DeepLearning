{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Respuestas_Tarea_3_CC6204_2020.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEJLFL-H5axT"
      },
      "source": [
        "# Tarea 3: Regularización y Optimización <br/> CC6204 Deep Learning, Universidad de Chile  <br/> Hoja de respuestas\n",
        "## Nombre: Diego Irarrazaval\n",
        "\n",
        "**Fecha de entrega: 13 de noviembre de 2020**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54d9w9Y7XXmc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5192be4-fc04-4879-ca18-3a80ea9692d3"
      },
      "source": [
        "# Este notebook está pensado para correr en CoLaboratory. \n",
        "# Lo único imprescindible por importar es torch\n",
        "import torch\n",
        "\n",
        "# Posiblemenete quieras instalar e importar ipdb para debuggear.\n",
        "# Si es así, descomenta lo siguiente:\n",
        "# !pip install -q ipdb\n",
        "# import ipdb\n",
        "\n",
        "# Aqui instalamos la libreria de correccion del curso\n",
        "!pip install -U \"git+https://github.com/dccuchile/CC6204.git@master#egg=cc6204&subdirectory=autocorrect\"\n",
        "from timeit import default_timer as timer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cc6204\n",
            "  Cloning https://github.com/dccuchile/CC6204.git (to revision master) to /tmp/pip-install-2toovxlp/cc6204\n",
            "  Running command git clone -q https://github.com/dccuchile/CC6204.git /tmp/pip-install-2toovxlp/cc6204\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from cc6204) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from cc6204) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from cc6204) (1.7.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->cc6204) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->cc6204) (2020.11.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->cc6204) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->cc6204) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->cc6204) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->cc6204) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch->cc6204) (0.16.0)\n",
            "Building wheels for collected packages: cc6204\n",
            "  Building wheel for cc6204 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cc6204: filename=cc6204-0.5.0-cp36-none-any.whl size=5801 sha256=8c934e15707fc942025fd8a8c44dfbe4ff51dbc3d9396f868c28cbf440f5f626\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-i5z63xeg/wheels/62/f0/30/aadcb7ce24a2f9c935890518e902d4e23bf97b80f47bb64414\n",
            "Successfully built cc6204\n",
            "Installing collected packages: cc6204\n",
            "Successfully installed cc6204-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDYIQbJuXY9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b638c55-f53b-4f83-b6ac-81cbe84e4a68"
      },
      "source": [
        "# importamos las herramientas del curso\n",
        "from cc6204 import AutoCorrect, FailedTest\n",
        "\n",
        "# ingresa el host y port que posteamos en u-cursos\n",
        "corrector = AutoCorrect(host=\"cc6204.dcc.uchile.cl\", port=443)\n",
        "\n",
        "# anota el token que te daremos en u-cursos\n",
        "token = \"]ye/Ox;nsz\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Connection stablished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg8i3agyXDSr"
      },
      "source": [
        "# Parte 1: Regularización y Generalización\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbzcMAkOXDSr"
      },
      "source": [
        "## 1a) Regularización por *weight decay*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QfQVggvNuv9"
      },
      "source": [
        "# Tu código debiera continuar así\n",
        "\n",
        "class SGD():\n",
        "  def __init__(self, parameters, lr, beta=0):\n",
        "    # lo que sea necesario inicializar\n",
        "    self.params = [p for p in parameters if p is not None]\n",
        "    self.lr = lr\n",
        "    self.beta = beta\n",
        "\n",
        "  def step(self):\n",
        "    # actualiza acá los parámetros a partir de los gradientes\n",
        "    # y considera el nuevo valor beta\n",
        "    for p in self.params:\n",
        "      p.data = (1 - self.beta)*p.data - self.lr*p.grad"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJMKrtjZcXDH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "bef7d4fc-c760-4212-99f7-c22969ae9b0a"
      },
      "source": [
        "# Tests del API del curso\n",
        "weight, grad = corrector.get_test_data(homework=3, question=\"1a\", test=1, token=token)\n",
        "\n",
        "weight = torch.tensor(weight, requires_grad=True)\n",
        "weight.grad = torch.tensor(grad)\n",
        "\n",
        "optimizer = SGD([weight], lr=0.1, beta=0.1)\n",
        "optimizer.step()\n",
        "\n",
        "# Submit\n",
        "corrector.submit(homework=3, question=\"1a\", test=1, token=token, answer=weight)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cached test data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-57e5ee29aef9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'beta'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifPp_c1eXDSt"
      },
      "source": [
        "## 1b) Regularización por dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InfEB_1uPfcA"
      },
      "source": [
        "Para esta parte de la tarea, va a ser necesario modificar el método `forward` para que entregue el valor a la salida de la i-esima capa escondida. Para esto se modifica el método forward para que reciba un parámetro `output_layer` que indica luego de que capa escondida se espera el output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVqNgzZ7xqXK"
      },
      "source": [
        "# Los gradientes se pueden implementar como parte de cada una de las funciones de activación\n",
        "# En el caso de swish y celu devuelven una tupla (d_dx, d_dp)\n",
        "\n",
        "def sig(T, gradient=False):\n",
        "  if gradient:\n",
        "    sigT = sig(T)\n",
        "    return sigT * (1 - sigT)\n",
        "  return torch.reciprocal(1 + torch.exp(-1 * T))\n",
        "  \n",
        "def tanh(T, gradient=False):\n",
        "  if gradient:\n",
        "    tanhT = tanh(T)\n",
        "    return 1 - tanhT * tanhT\n",
        "  E = torch.exp(T)\n",
        "  e = torch.exp(-1 * T)\n",
        "  return (E - e) * torch.reciprocal(E + e)\n",
        "\n",
        "def relu(T, gradient=False):\n",
        "  if gradient:\n",
        "    outT = torch.zeros_like(T)\n",
        "    outT[T>=0] = 1\n",
        "    return outT\n",
        "  return torch.max(T, torch.zeros_like(T))\n",
        "\n",
        "def swish(T, beta=1, gradient=False):\n",
        "  if gradient:\n",
        "    sigbT = sig(beta * T)\n",
        "    swishT = T * sigbT\n",
        "    return sigbT + beta * swishT * (1 - sigbT), swishT * (T - swishT)\n",
        "  return T * torch.reciprocal(1 + torch.exp(-beta * T))\n",
        "\n",
        "def celu(T, alpha=1, gradient=False):\n",
        "  if alpha == 0:\n",
        "    raise ValueError(\"alpha cannot be 0\")\n",
        "\n",
        "  zeros = torch.zeros_like(T)\n",
        "  Talpha = T / alpha\n",
        "  \n",
        "  if gradient:\n",
        "    e = Talpha.exp()\n",
        "    d_dx = torch.ones_like(T)\n",
        "    d_dx[T<0] = e[T<0]\n",
        "    zeros[T<0] = (celu(T)[T<0] - T[T<0] * e[T<0]) / alpha\n",
        "    return d_dx, zeros # d_dx, d_da\n",
        "  \n",
        "  return torch.max(zeros, T) + torch.min(zeros, alpha * (Talpha).expm1())\n",
        "\n",
        "def softmax(T, dim, estable=True):\n",
        "  if estable:\n",
        "    T -= T.max(dim=dim, keepdim=True)[0]  # keepdim=True => output has dim with size 1. Otherwise, dim is squeezed\n",
        "  exp = torch.exp(T)\n",
        "  return exp / torch.sum(exp, dim=dim, keepdim=True)  # keepdim=True => output has dim with size 1. Otherwise, dim is squeezed"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4wXj3AUXDSu"
      },
      "source": [
        "# Tu código debiera continuar como sigue\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter, ParameterList\n",
        "import math\n",
        "\n",
        "def get_init_weights(shape):\n",
        "  W = torch.randn(shape)\n",
        "  # stdv = 1. / math.sqrt(W.size(1))\n",
        "  # W.data.uniform_(-stdv, stdv)\n",
        "  return Parameter(W)\n",
        "\n",
        "\n",
        "class FFNN(nn.Module):\n",
        "  def __init__(self, F, l_h, l_a, C, keep_prob=None, l_a_params=None):\n",
        "    super(FFNN, self).__init__()    \n",
        "  \n",
        "    sizes = [F] + l_h + [C]\n",
        "    self.Ws = ParameterList([get_init_weights((sizes[i], sizes[i+1])) for i in range(len(sizes)-1)])\n",
        "    self.bs = ParameterList([Parameter(torch.zeros(h)) for h in sizes[1:]])\n",
        "    self.fs = l_a\n",
        "    if l_a_params is not None:\n",
        "      self.fs_ps_mask = [Parameter(torch.tensor(p)) if p else None for p in l_a_params]\n",
        "    else:\n",
        "      self.fs_ps_mask = [None for _ in l_a]\n",
        "    self.fs_ps = ParameterList([p for p in self.fs_ps_mask if p])\n",
        "    self.keep_prob = keep_prob    \n",
        "  \n",
        "    self.drop_masks = [torch.zeros_like(w) for w in self.Ws]\n",
        "\n",
        "  @property\n",
        "  def in_size(self):\n",
        "    return self.Ws[0].shape[0]\n",
        "    \n",
        "  def load_weights(self, Ws, U, bs, c):\n",
        "    self.Ws = ParameterList([Parameter(W) for W in Ws + [U]])\n",
        "    self.bs = ParameterList([Parameter(b) for b in bs + [c]])\n",
        "  \n",
        "  def resumen(self):\n",
        "    # Usa self.parameters() o self.named_parameters().\n",
        "    for name, p in self.named_parameters():\n",
        "      print('{}:\\t{}'.format(name, p.size()))\n",
        "    pass\n",
        "\n",
        "  def forward(self, x, predict=False):\n",
        "    if predict:\n",
        "      self.cacheU = [] #, self.cacheH = [], []\n",
        "      for W, b, f, p in zip(self.Ws[:-1], self.bs[:-1], self.fs, self.fs_ps_mask):\n",
        "        x = torch.mm(x, W) + b\n",
        "        self.cacheU.append(x)\n",
        "        x = f(x, p.item()) if p else f(x)\n",
        "  #       self.cacheH.append(x)\n",
        "      return softmax(torch.mm(x, self.Ws[-1]) + self.bs[-1], dim=1)\n",
        "    \n",
        "    else:\n",
        "      self.cacheU = [] #, self.cacheH = [], []\n",
        "      i = 0\n",
        "      for W, b, f, p, keep_prob in zip(self.Ws[:-1], self.bs[:-1], self.fs, self.fs_ps_mask, self.keep_prob):\n",
        "        x = torch.mm(x, W) + b\n",
        "        self.cacheU.append(x)\n",
        "        x = f(x, p.item()) if p else f(x)\n",
        "        self.drop_masks[i] = (torch.rand_like(x) < 0).to(x.device)\n",
        "        if keep_prob > 0:\n",
        "          scale = 1/keep_prob\n",
        "        else:\n",
        "          scale = 0\n",
        "        x = self.drop_masks[i] * x * scale\n",
        "        i+=1\n",
        "  #       self.cacheH.append(x)\n",
        "      return softmax(torch.mm(x, self.Ws[-1]) + self.bs[-1], dim=1)\n",
        "  \n",
        "  # nuevo código Tarea 2\n",
        "  def backward(self, x, y, y_pred, predict=False):\n",
        "    current_grad =  (y_pred - y) / y.size(0)\n",
        "\n",
        "    for i in range(len(self.Ws)-1, 0, -1):\n",
        "      if self.fs_ps_mask[i-1] is None:\n",
        "        self.Ws[i].grad = self.fs[i-1](self.cacheU[i-1]).t() @ current_grad\n",
        "        self.Ws[i].grad *= self.drop_masks[i].to(current_grad.device)\n",
        "      else:\n",
        "        self.Ws[i].grad = self.fs[i-1](self.cacheU[i-1], self.fs_ps_mask[i-1].item()).t()  @ current_grad\n",
        "        self.Ws[i].grad *= self.drop_masks[i].to(current_grad.device)\n",
        "      self.bs[i].grad = current_grad.sum(dim=0)\n",
        "      h_grad = current_grad @ self.Ws[i].t()\n",
        "      \n",
        "      if self.fs_ps_mask[i-1] is None:\n",
        "        current_grad = self.fs[i-1](self.cacheU[i-1], gradient=True) * h_grad\n",
        "      else:\n",
        "        current_grad, p_grad = self.fs[i-1](self.cacheU[i-1], self.fs_ps_mask[i-1], gradient=True)\n",
        "        current_grad *= h_grad\n",
        "        self.fs_ps_mask[i-1].grad = (p_grad * h_grad).sum()\n",
        "    \n",
        "    self.Ws[0].grad = (x.t() @ current_grad) * self.drop_masks[0].to(x.device)\n",
        "    self.bs[0].grad = current_grad.sum(dim=0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNFLN9sbyh-x"
      },
      "source": [
        "# Otro intento de dropout\n",
        "class MyDropout(nn.Module):\n",
        "  def __init__(self, layer_size, keep_prob):\n",
        "    super(MyDropout, self).__init__()\n",
        "    self.keep_prob = keep_prob\n",
        "    self.mask = torch.zeros(size = layer_size)\n",
        "\n",
        "  def forward(self, x, predict=False):\n",
        "    if ~predict:\n",
        "      self.mask = (torch.randn_like(x) < 0)\n",
        "      if self.keep_prob > 0:\n",
        "        scale = 1/self.keep_prob\n",
        "      else:\n",
        "        scale = 0\n",
        "      return self.mask * x * scale\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "class FFNN2(nn.Module):\n",
        "  def __init__(self, F, l_h, l_a, C, keep_prob=None, l_a_params=None):\n",
        "    super(FFNN2, self).__init__()    \n",
        "  \n",
        "    sizes = [F] + l_h + [C]\n",
        "    self.Ws = ParameterList([get_init_weights((sizes[i], sizes[i+1])) for i in range(len(sizes)-1)])\n",
        "    self.bs = ParameterList([Parameter(torch.zeros(h)) for h in sizes[1:]])\n",
        "    self.fs = l_a\n",
        "    if l_a_params is not None:\n",
        "      self.fs_ps_mask = [Parameter(torch.tensor(p)) if p else None for p in l_a_params]\n",
        "    else:\n",
        "      self.fs_ps_mask = [None for _ in l_a]\n",
        "    self.fs_ps = ParameterList([p for p in self.fs_ps_mask if p])\n",
        "    self.keep_prob = keep_prob    \n",
        "  \n",
        "    self.drop_masks = [MyDropout(w.shape, p) for w, p in zip(self.Ws, self.keep_prob)]\n",
        "\n",
        "  @property\n",
        "  def in_size(self):\n",
        "    return self.Ws[0].shape[0]\n",
        "    \n",
        "  def load_weights(self, Ws, U, bs, c):\n",
        "    self.Ws = ParameterList([Parameter(W) for W in Ws + [U]])\n",
        "    self.bs = ParameterList([Parameter(b) for b in bs + [c]])\n",
        "  \n",
        "  def resumen(self):\n",
        "    # Usa self.parameters() o self.named_parameters().\n",
        "    for name, p in self.named_parameters():\n",
        "      print('{}:\\t{}'.format(name, p.size()))\n",
        "    pass\n",
        "\n",
        "  def forward(self, x, predict=False):\n",
        "    if predict:\n",
        "      self.cacheU = [] #, self.cacheH = [], []\n",
        "      for W, b, f, p in zip(self.Ws[:-1], self.bs[:-1], self.fs, self.fs_ps_mask):\n",
        "        x = torch.mm(x, W) + b\n",
        "        self.cacheU.append(x)\n",
        "        x = f(x, p.item()) if p else f(x)\n",
        "  #       self.cacheH.append(x)\n",
        "      return softmax(torch.mm(x, self.Ws[-1]) + self.bs[-1], dim=1)\n",
        "    \n",
        "    else:\n",
        "      self.cacheU = [] #, self.cacheH = [], []\n",
        "      i = 0\n",
        "      for W, b, f, p, dropMask in zip(self.Ws[:-1], self.bs[:-1], self.fs, self.fs_ps_mask, self.drop_masks):\n",
        "        x = dropMask.forward(torch.mm(x, W) + b)\n",
        "        self.cacheU.append(x)\n",
        "        x = f(x, p.item()) if p else f(x)\n",
        "      return softmax(torch.mm(x, self.Ws[-1]) + self.bs[-1], dim=1)\n",
        "  \n",
        "  # nuevo código Tarea 2\n",
        "  def backward(self, x, y, y_pred, predict=False):\n",
        "    current_grad =  (y_pred - y) / y.size(0)\n",
        "\n",
        "    for i in range(len(self.Ws)-1, 0, -1):\n",
        "      if self.fs_ps_mask[i-1] is None:\n",
        "        self.Ws[i].grad = self.drop_masks[i].forward( self.fs[i-1](self.cacheU[i-1]).t() ) @ current_grad\n",
        "        #self.Ws[i].grad *= self.drop_masks[i].mask.to(current_grad.device)\n",
        "      else:\n",
        "        self.Ws[i].grad = self.drop_masks[i].forward( self.fs[i-1](self.cacheU[i-1], self.fs_ps_mask[i-1].item()).t() )  @ current_grad\n",
        "        # self.Ws[i].grad *= self.drop_masks[i].mask.to(current_grad.device)\n",
        "      self.bs[i].grad = current_grad.sum(dim=0)\n",
        "      h_grad = current_grad @ self.Ws[i].t()\n",
        "      \n",
        "      if self.fs_ps_mask[i-1] is None:\n",
        "        current_grad = self.fs[i-1](self.cacheU[i-1], gradient=True) * h_grad\n",
        "      else:\n",
        "        current_grad, p_grad = self.fs[i-1](self.cacheU[i-1], self.fs_ps_mask[i-1], gradient=True)\n",
        "        current_grad *= h_grad\n",
        "        self.fs_ps_mask[i-1].grad = (p_grad * h_grad).sum()\n",
        "    \n",
        "    # self.Ws[0].grad = (x.t() @ current_grad) * self.drop_masks[0].to(x.device)\n",
        "    self.Ws[0].grad = self.drop_masks[i].forward( (x.t() @ current_grad) )\n",
        "    self.bs[0].grad = current_grad.sum(dim=0)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJot--zz_QNT"
      },
      "source": [
        "# tercer intento de dropout:\n",
        "# Siguiendo el ejeplo de https://stackoverflow.com/questions/54109617/implementing-dropout-from-scratch\n",
        "class MyDropout2(nn.Module): \n",
        "  def __init__(self, p: float = 0.5):\n",
        "    super(MyDropout2, self).__init__()\n",
        "    if p < 0 or p > 1:\n",
        "      raise ValueError(\"dropout probability has to be between 0 and 1, \" \"but got {}\".format(p))\n",
        "    self.p = p\n",
        "\n",
        "  def forward(self, X, predict=False):\n",
        "    if ~predict:\n",
        "      eps = 1e-8\n",
        "      binomial = torch.distributions.binomial.Binomial(probs=1-self.p)\n",
        "      return X * binomial.sample(X.size()).to(X.device) * (1.0/((1-self.p)+eps))\n",
        "    return weights\n",
        "\n",
        "\n",
        "class FFNN3(nn.Module):\n",
        "  def __init__(self, F, l_h, l_a, C, keep_prob=None, l_a_params=None):\n",
        "    super(FFNN3, self).__init__()    \n",
        "  \n",
        "    sizes = [F] + l_h + [C]\n",
        "    self.Ws = ParameterList([get_init_weights((sizes[i], sizes[i+1])) for i in range(len(sizes)-1)])\n",
        "    self.bs = ParameterList([Parameter(torch.zeros(h)) for h in sizes[1:]])\n",
        "    self.fs = l_a\n",
        "    if l_a_params is not None:\n",
        "      self.fs_ps_mask = [Parameter(torch.tensor(p)) if p else None for p in l_a_params]\n",
        "    else:\n",
        "      self.fs_ps_mask = [None for _ in l_a]\n",
        "    self.fs_ps = ParameterList([p for p in self.fs_ps_mask if p])\n",
        "    self.keep_prob = keep_prob    \n",
        "  \n",
        "    self.drop_masks = [MyDropout2(p) for p in self.keep_prob]\n",
        "\n",
        "  @property\n",
        "  def in_size(self):\n",
        "    return self.Ws[0].shape[0]\n",
        "    \n",
        "  def load_weights(self, Ws, U, bs, c):\n",
        "    self.Ws = ParameterList([Parameter(W) for W in Ws + [U]])\n",
        "    self.bs = ParameterList([Parameter(b) for b in bs + [c]])\n",
        "  \n",
        "  def resumen(self):\n",
        "    # Usa self.parameters() o self.named_parameters().\n",
        "    for name, p in self.named_parameters():\n",
        "      print('{}:\\t{}'.format(name, p.size()))\n",
        "    pass\n",
        "\n",
        "  def forward(self, x, predict=False):\n",
        "    if predict:\n",
        "      self.cacheU = [] #, self.cacheH = [], []\n",
        "      for W, b, f, p in zip(self.Ws[:-1], self.bs[:-1], self.fs, self.fs_ps_mask):\n",
        "        x = torch.mm(x, W) + b\n",
        "        self.cacheU.append(x)\n",
        "        x = f(x, p.item()) if p else f(x)\n",
        "  #       self.cacheH.append(x)\n",
        "      return softmax(torch.mm(x, self.Ws[-1]) + self.bs[-1], dim=1)\n",
        "    \n",
        "    else:\n",
        "      self.cacheU = [] #, self.cacheH = [], []\n",
        "      i = 0\n",
        "      for W, b, f, p, dropMask in zip(self.Ws[:-1], self.bs[:-1], self.fs, self.fs_ps_mask, self.drop_masks):\n",
        "        x = dropMask.forward(torch.mm(x, W) + b)\n",
        "        self.cacheU.append(x)\n",
        "        x = f(x, p.item()) if p else f(x)\n",
        "      return softmax(torch.mm(x, self.Ws[-1]) + self.bs[-1], dim=1)\n",
        "  \n",
        "  # nuevo código Tarea 2\n",
        "  def backward(self, x, y, y_pred, predict=False):\n",
        "    current_grad =  (y_pred - y) / y.size(0)\n",
        "\n",
        "    for i in range(len(self.Ws)-1, 0, -1):\n",
        "      if self.fs_ps_mask[i-1] is None:\n",
        "        self.Ws[i].grad = self.drop_masks[i].forward( self.fs[i-1](self.cacheU[i-1]).t() ) @ current_grad\n",
        "        #self.Ws[i].grad *= self.drop_masks[i].mask.to(current_grad.device)\n",
        "      else:\n",
        "        self.Ws[i].grad = self.drop_masks[i].forward( self.fs[i-1](self.cacheU[i-1], self.fs_ps_mask[i-1].item()).t() )  @ current_grad\n",
        "        # self.Ws[i].grad *= self.drop_masks[i].mask.to(current_grad.device)\n",
        "      self.bs[i].grad = current_grad.sum(dim=0)\n",
        "      h_grad = current_grad @ self.Ws[i].t()\n",
        "      \n",
        "      if self.fs_ps_mask[i-1] is None:\n",
        "        current_grad = self.fs[i-1](self.cacheU[i-1], gradient=True) * h_grad\n",
        "      else:\n",
        "        current_grad, p_grad = self.fs[i-1](self.cacheU[i-1], self.fs_ps_mask[i-1], gradient=True)\n",
        "        current_grad *= h_grad\n",
        "        self.fs_ps_mask[i-1].grad = (p_grad * h_grad).sum()\n",
        "    \n",
        "    # self.Ws[0].grad = (x.t() @ current_grad) * self.drop_masks[0].to(x.device)\n",
        "    self.Ws[0].grad = self.drop_masks[i].forward( (x.t() @ current_grad) )\n",
        "    self.bs[0].grad = current_grad.sum(dim=0)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-tOz3LeV4fk",
        "outputId": "56caa864-06a6-4a43-8c2e-11be48652bfe"
      },
      "source": [
        "X = torch.rand(1, 10).to('cuda')\n",
        "p = 0.5\n",
        "\n",
        "\n",
        "print( X * (torch.randn_like(X)>p))"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrjGpVdg4xi1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39481b31-f698-4472-cb55-0711c8c8048d"
      },
      "source": [
        "# Tests del API del curso\n",
        "torch.manual_seed(0)\n",
        "sample = torch.rand(1, 10)\n",
        "red = FFNN3(10, [1000], [sig], 1, keep_prob=[1.0, 0.5])\n",
        "y = red.forward(sample, predict=False)\n",
        "output_mask = (y == 0)\n",
        "percent = torch.sum(output_mask).item() / list(output_mask.size())[-1]\n",
        "\n",
        "# Submit\n",
        "corrector.submit(homework=3, question=\"1b\", test=1, token=token, answer=percent)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Correct Test!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlLnZM7hi0C2"
      },
      "source": [
        "## 1c) Entrenamiento y generalización sobre MNIST "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZa9l6S5CrEa"
      },
      "source": [
        "# Tu código acá\n",
        "def CELoss(Q, P, estable=True, epsilon=1e-8):\n",
        "  N = Q.shape[0]\n",
        "  if estable:\n",
        "    Q = Q.clamp(epsilon, 1-epsilon)\n",
        "  return -(P * Q.log()).sum()/N"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3w9IFHmFP4S"
      },
      "source": [
        "import sys\n",
        "\n",
        "# Tu código acá\n",
        "def entrenar_FFNN(red, dataset, optimizador, epochs=1, batch_size=1, reports_every=1, device='cuda'):\n",
        "  device = device\n",
        "  red.to(device)\n",
        "  data = DataLoader(dataset, batch_size, shuffle=True)\n",
        "  total=len(dataset)\n",
        "  tiempo_epochs = 0\n",
        "  loss, acc = [], []\n",
        "  for e in range(1, epochs+1):  \n",
        "    inicio_epoch = timer()\n",
        "    \n",
        "    for x, y in data:\n",
        "      x, y = x.view(x.size(0), -1).float().to(device), y.to(device)\n",
        "      \n",
        "      y_pred = red.forward(x,predict=False)\n",
        "      \n",
        "      y_onehot = torch.zeros_like(y_pred)\n",
        "      y_onehot[torch.arange(x.size(0)), y] = 1.\n",
        "    \n",
        "      red.backward(x, y_onehot, y_pred)\n",
        "\n",
        "      optimizador.step()\n",
        "      \n",
        "    tiempo_epochs += timer() - inicio_epoch\n",
        "    \n",
        "    if e % reports_every == 0:\n",
        "      X = dataset.data.view(len(dataset), -1).float().to(device)\n",
        "      Y = dataset.targets.to(device)\n",
        "      \n",
        "      Y_PRED = red.forward(X, predict = True).to(device)\n",
        "      \n",
        "      Y_onehot = torch.zeros_like(Y_PRED)\n",
        "      Y_onehot[torch.arange(X.size(0)), Y] = 1.\n",
        "\n",
        "      L_total = CELoss(Y_PRED, Y_onehot)\n",
        "      loss.append(L_total)\n",
        "      diff = Y-torch.argmax(Y_PRED,1)\n",
        "      errores = torch.nonzero(diff).size(0)\n",
        "      \n",
        "      Acc=100*(total-errores)/total\n",
        "      acc.append(Acc)\n",
        "\n",
        "      sys.stdout.write(\n",
        "            '\\rEpoch:{0:03d}'.format(e) + ' Acc:{0:.2f}%'.format(Acc)\n",
        "            + ' Loss:{0:.4f}'.format(L_total) \n",
        "            + ' Tiempo/epoch:{0:.3f}s'.format(tiempo_epochs/e))\n",
        "  \n",
        "  return loss, acc"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt-ozTVvFWg3"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_results(loss, acc):\n",
        "  f1 = plt.figure(1)\n",
        "  ax1 = f1.add_subplot(111)\n",
        "  ax1.set_title(\"Loss\")    \n",
        "  ax1.set_xlabel('epochs')\n",
        "  ax1.set_ylabel('loss')\n",
        "  ax1.plot(loss, c='r')\n",
        "  f1.show()\n",
        "\n",
        "  f2 = plt.figure(2)\n",
        "  ax2 = f2.add_subplot(111)\n",
        "  ax2.set_title(\"Accuracy\")    \n",
        "  ax2.set_xlabel('epochs')\n",
        "  ax2.set_ylabel('acc')\n",
        "  ax2.plot(acc, c='b')\n",
        "  f2.show()"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qj1GI81izYt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8975f86-0dd8-41de-98f6-55134606b211"
      },
      "source": [
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Importamos funcionalidades útiles para mirar los datos.\n",
        "from matplotlib.pyplot import subplots\n",
        "from random import randint\n",
        "\n",
        "# Descarga y almacena el conjunto de entrenamiento de MNIST.\n",
        "mnist_dataset = MNIST('mnist', train=True, transform=ToTensor(), download=True)\n",
        "print('Cantidad total de datos:',len(mnist_dataset))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cantidad total de datos: 60000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9RksqzxFneV",
        "outputId": "d244c2a4-9b3f-4967-dabc-42001d00feb4"
      },
      "source": [
        "import os\n",
        "from numpy import loadtxt\n",
        "\n",
        "# if not os.path.exists('/content/CC6204'):\n",
        "#   !git clone https://github.com/dccuchile/CC6204.git\n",
        "\n",
        "# dir_path = '/content/CC6204/2020/tareas/tarea1/mnist_weights'\n",
        "# Ws = [torch.from_numpy(loadtxt(os.path.join(dir_path, '{}.txt'.format(name)))).float() for name in ['W1', 'W2']]\n",
        "# bs = [torch.from_numpy(loadtxt(os.path.join(dir_path, '{}.txt'.format(name)))).float() for name in ['b1', 'b2']]\n",
        "# U = torch.from_numpy(loadtxt(os.path.join(dir_path, 'U.txt'))).float()\n",
        "# c = torch.from_numpy(loadtxt(os.path.join(dir_path, 'c.txt'))).float()\n",
        "\n",
        "# mnist_model = FFNN(784, [32, 16], [relu, relu], 10)\n",
        "# mnist_model.load_weights(Ws, U, bs, c)\n",
        "\n",
        "mnist_model = FFNN2(784, [512, 1024,  128], [relu, relu,  relu], 10, keep_prob=[1, .5, .5, .25])\n",
        "\n",
        "mnist_optimizer = SGD(mnist_model.parameters(), lr=1e-5)\n",
        "with torch.no_grad():\n",
        "  mnist_loss, mnist_acc = entrenar_FFNN(mnist_model, mnist_dataset, mnist_optimizer, epochs=30, batch_size=32)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:030 Acc:9.43% Loss:16.6830 Tiempo/epoch:14.838s"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwedMCZjXDSw"
      },
      "source": [
        "# Parte 2: Optimización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nftKKxlBXDSx"
      },
      "source": [
        "## 2a) Inicialización de Xavier/He"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxEVEoeJTq81"
      },
      "source": [
        "Para los test de esta parte vamos a necesitar que modifiques tu código para que se pueda entregar valores predeterminados de `r`. Ahora tu código para las inicializaciones debe ser: `xavier_init(first_dim, second_dim, r=None)`, `he_init(first_dim, second_dim, r=None)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7DHcaQDTimb"
      },
      "source": [
        "# Tu código debiera continuar como sigue\n",
        "def xavier_init(first_dim, second_dim, r=None):\n",
        "  from math import sqrt\n",
        "  factor = sqrt(1/(first_dim))\n",
        "\n",
        "  if r==None:\n",
        "    return factor * torch.normal(mean=0, std=1, size=(first_dim,second_dim))\n",
        "  else:\n",
        "    return torch.tensor(factor) * r\n",
        "\n",
        "def he_init(first_dim, second_dim, r=None):\n",
        "  from math import sqrt\n",
        "  factor = sqrt(2/(first_dim))\n",
        "\n",
        "  if r==None:\n",
        "    return factor * torch.normal(mean=0, std=1, size=(first_dim,second_dim))\n",
        "  else:\n",
        "    return torch.tensor(factor) * r\n"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Rl25Jr5UAgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0bd064f-9c40-445f-e7ee-619ade7c69bf"
      },
      "source": [
        "# Tests del API del curso\n",
        "r_xavier = corrector.get_test_data(homework=3, question=\"2a\", test=1, token=token)\n",
        "r_he = corrector.get_test_data(homework=3, question=\"2a\", test=2, token=token)\n",
        "\n",
        "w_xavier = xavier_init(50, 50, torch.tensor(r_xavier))\n",
        "w_he = he_init(50, 50, torch.tensor(r_he))\n",
        "\n",
        "corrector.submit(homework=3, question=\"2a\", test=1, token=token, answer=w_xavier)\n",
        "corrector.submit(homework=3, question=\"2a\", test=2, token=token, answer=w_he)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cached test data\n",
            "Using cached test data\n",
            "Correct Test!\n",
            "Correct Test!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nzp5SWKBXDSy"
      },
      "source": [
        "## 2b) Descenso de gradiente con momentum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whoBUewrUgWB"
      },
      "source": [
        "# Tu código debiera continuar así\n",
        "\n",
        "class SGD():\n",
        "  def __init__(self, parameters, lr, momentum=0):\n",
        "    # lo que sea necesario inicializar\n",
        "    self.params = [p for p in parameters if p is not None]\n",
        "    self.speed = [torch.zeros_like(p) for p in parameters if p is not None]\n",
        "    self.lr = lr\n",
        "    self.momentum = momentum\n",
        "    \n",
        "  \n",
        "  def step(self):\n",
        "    # actualiza acá los parámetros a partir de los gradientes\n",
        "    # y considerando el valor de momentum que acabámos de agregar\n",
        "    for p, s in zip(self.params, self.speed):\n",
        "      s.data = self.momentum * s.data - self.lr * p.grad\n",
        "      p.data += s.data"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WIcvmWUVl9r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d55d02-4a2f-4157-a56a-07f3da618086"
      },
      "source": [
        "# Tests del API del curso\n",
        "weight, grad = corrector.get_test_data(homework=3, question=\"2b\", test=1, token=token)\n",
        "\n",
        "weight = torch.tensor(weight, requires_grad=True)\n",
        "weight.grad = torch.tensor(grad)\n",
        "\n",
        "optimizer = SGD([weight], lr=0.1, momentum=0.9)\n",
        "optimizer.step()\n",
        "\n",
        "# Submit\n",
        "corrector.submit(homework=3, question=\"2b\", test=1, token=token, answer=weight)\n",
        "optimizer.step()\n",
        "corrector.submit(homework=3, question=\"2b\", test=2, token=token, answer=weight)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cached test data\n",
            "Correct Test!\n",
            "Correct Test!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WNYoTIHXDS0"
      },
      "source": [
        "## 2c) RMSProp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjLGmGoXV_kG"
      },
      "source": [
        "# Tu código acá\n",
        "\n",
        "class RMSProp():\n",
        "  def __init__(self, red, lr=0.001, beta=0.9, epsilon=1e-8):\n",
        "    # en este caso debes inicializar la variable que acumula\n",
        "    # el promedio exponencial de los cuadrados\n",
        "    # lo que sea necesario inicializar\n",
        "    self.params = [p for p in red if p is not None]\n",
        "    self.speed = [torch.zeros_like(p) for p in red if p is not None]\n",
        "    self.lr = lr\n",
        "    self.beta = beta\n",
        "    self.epsilon = epsilon\n",
        "  \n",
        "  def step(self):\n",
        "    # actualiza acá los parámetros a partir de los gradientes\n",
        "    # y la corrección según S\n",
        "    for p, s in zip(self.params, self.speed):\n",
        "      s.data = self.beta * s.data + (1-self.beta)*(p.grad * p.grad)\n",
        "      p.data -= self.lr*(1/(torch.sqrt(s.data)+ self.epsilon))*p.grad"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNnw6obOWDC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f29f4c2-88ba-4ca6-c964-8f2028a1c6f2"
      },
      "source": [
        "# Tests del API del curso\n",
        "weight, grad = corrector.get_test_data(homework=3, question=\"2c\", test=1, token=token)\n",
        "\n",
        "weight = torch.tensor(weight, requires_grad=True)\n",
        "weight.grad = torch.tensor(grad)\n",
        "\n",
        "optimizer = RMSProp([weight], lr=0.001, beta=0.9, epsilon=1e-8)\n",
        "optimizer.step()\n",
        "\n",
        "# Submit\n",
        "corrector.submit(homework=3, question=\"2c\", test=1, token=token, answer=weight)\n",
        "optimizer.step()\n",
        "corrector.submit(homework=3, question=\"2c\", test=2, token=token, answer=weight)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cached test data\n",
            "Correct Test!\n",
            "Correct Test!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJTTDpeTWTwY"
      },
      "source": [
        "## 2d) Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ola7FuFdWZH6"
      },
      "source": [
        "# Tu código acá\n",
        "\n",
        "class Adam():\n",
        "  def __init__(self, red, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "    # en este caso debes inicializar la variable que acumula\n",
        "    # el promedio exponencial de los cuadrados\n",
        "    self.params = [p for p in red if p is not None]\n",
        "    self.speed = [torch.zeros_like(p) for p in red if p is not None]\n",
        "    self.moments = [torch.zeros_like(p) for p in red if p is not None]\n",
        "\n",
        "    self.lr = lr\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "    self.n = 1\n",
        "  \n",
        "  def step(self):\n",
        "    # actualiza acá los parámetros a partir de los gradientes\n",
        "    # y la corrección según S\n",
        "    for p, s, m in zip(self.params, self.speed, self.moments):\n",
        "      m.data = self.beta1*m.data + (1-self.beta1)*p.grad\n",
        "      s.data = self.beta2 * s.data + (1-self.beta2)*(p.grad * p.grad)\n",
        "\n",
        "      m_mean = m.data / (1-self.beta1**self.n)\n",
        "      s_mean = s.data / (1-self.beta2**self.n)\n",
        "      p.data -= self.lr*(1/(torch.sqrt(s_mean)+ self.epsilon))*m_mean\n",
        "    self.n += 1"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OdCG4RjWeMJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7af2ec6c-5f77-483c-ca4d-018fd8bb30ff"
      },
      "source": [
        "# Tests del API del curso\n",
        "weight, grad = corrector.get_test_data(homework=3, question=\"2d\", test=1, token=token)\n",
        "\n",
        "weight = torch.tensor(weight, requires_grad=True)\n",
        "weight.grad = torch.tensor(grad)\n",
        "\n",
        "optimizer = Adam([weight], lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8)\n",
        "optimizer.step()\n",
        "\n",
        "# Submit\n",
        "corrector.submit(homework=3, question=\"2d\", test=1, token=token, answer=weight)\n",
        "optimizer.step()\n",
        "corrector.submit_check_some(homework=3, question=\"2d\", tests=[2, 3], token=token,\n",
        "                            answer_dict={2: weight, 3: weight}, required_number=1)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cached test data\n",
            "Correct Test!\n",
            "Correct Test!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isKfnQt_Wszx"
      },
      "source": [
        "## 2e) Entrenamiento en MNIST \n",
        "\n",
        "Usa tu red neuronal para entrenar con los datos de MNIST y compara cómo cambian las curvas de entrenamiento dependiendo de factores como la inicialización y los algoritmos que utilices. Presenta al menos dos gráficos en donde compares. Por ejemplo, puedes presentar uno que para la misma estrategia de inicialización, los tres algoritmos de optimización para varias épocas y cómo evoluciona la pérdida y el acierto. En cada caso comenta que conclusiones puedes sacar. Algunos ejemplos de preguntas que podrías tratar de responder son:\n",
        "* ¿cómo afecta el algoritmo de optimización al tiempo de convergencia de la red para los datos de entrenamiento?\n",
        "* ¿cómo afecta el algoritmo de optimización en el acierto alcanzado por la red en los datos de prueba?\n",
        "* Si haces la parte opcional de Batch Normalization, puedes también preguntarte cosas como si aplicar, o no, BN afecta a todos los algoritmos de optimización por igual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccy31ejpWt3P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9556cf0f-14df-45f7-c56e-9ad3a44e3052"
      },
      "source": [
        "# Aqui el codigo para entrenar en MNIST\n",
        "mnist_model = FFNN2(784, [512, 1024,  128], [relu, relu,  relu], 10, keep_prob=[1, .5, .5, .25])\n",
        "\n",
        "mnist_optimizer = Adam(mnist_model.parameters(), lr=1e-5)\n",
        "with torch.no_grad():\n",
        "  mnist_loss, mnist_acc = entrenar_FFNN(mnist_model, mnist_dataset, mnist_optimizer, epochs=30, batch_size=32)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:030 Acc:10.33% Loss:16.5175 Tiempo/epoch:14.815s"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jD6D95_ZgN3"
      },
      "source": [
        "## Sobre los resultados obtenidos:\n",
        "Notemos que los resultados obtenidos son pesimos. A pesar de tener tres implementaciones distintas de la capa de `DropOut` (que se cree es lo que esta mal implementado) no se logro superar el  $~15\\%$ de _accuracy_ y las perdidas era muy altas. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n29s-6zXDS1"
      },
      "source": [
        "# Parte 3 (Opcional): Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytN2y-FMXDS3"
      },
      "source": [
        "# Tu código debiera continuar como sigue\n",
        "\n",
        "class FFNN():\n",
        "  def __init__(self, F, l_h, l_a, C, keep_prob=None, bn=None):\n",
        "    # debes crear los parámetros necesarios para las capas de\n",
        "    # batch normalizacion\n",
        "    pass\n",
        "  \n",
        "  def forward(x, predict=False):\n",
        "    # debes modificar esta función para considerar las capas para las que se\n",
        "    # usará batch normalization\n",
        "    # también debes preocuparte de guardar los datos estadísticos que se\n",
        "    # usaran en tiempo de test (predict=True)\n",
        "    pass\n",
        "  \n",
        "  def backward(x,y,y_pred):\n",
        "    # computar acá todos los gradientes considerando las capas de \n",
        "    # batch normalization\n",
        "    # no olvides considerar los nuevos parámetros entrenables.\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}