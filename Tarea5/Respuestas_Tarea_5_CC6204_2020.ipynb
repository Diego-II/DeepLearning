{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Respuestas_Tarea_5_CC6204_2020",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zhjpqvcdo5o"
      },
      "source": [
        "# Tarea 5: Redes Recurrentes <br/> CC6204 Deep Learning, Universidad de Chile <br/> Hoja de Respuestas\n",
        "\n",
        "## Nombre: \n",
        "Fecha de entrega: 30 de diciembre de 2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64BkmYga3UN_"
      },
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch import tensor\n",
        "from torch.nn import Embedding, RNN, Linear, CrossEntropyLoss\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import get_tokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Aqui descargamos algunas funciones utiles para resolver la tarea\n",
        "if not os.path.exists('utils.py'):\n",
        "    !wget https://raw.githubusercontent.com/dccuchile/CC6204/master/2020/tareas/tarea5/utils.py -q --show-progress"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS9LeqZRJ5zn"
      },
      "source": [
        "from utils import extract_text_from_set, extract_text_from_set, tokenize_text \n",
        "from utils import encode_sentences, pad_sequence_with_lengths, pad_sequence_with_images\n",
        "from utils import TextDataset, CaptioningDataset"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNoC8iTiKtg0"
      },
      "source": [
        "# Parte 1: Generación de texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnPZTm8HMaNn"
      },
      "source": [
        "### Datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTiSVBGoK0Xo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62cdd56a-1126-4cee-f6c0-a1d85fcaf540"
      },
      "source": [
        "##############################################################################\n",
        "# Todo este código sirve para descargar, preprocesar y dejar los datos\n",
        "# listos para usar después. Después de ejecutar las dos celdas siguientes\n",
        "# tendrás los datos en train_flickr_tripletset y similar para val y test\n",
        "##############################################################################\n",
        "\n",
        "folder_path = './data/flickr8k'\n",
        "if not os.path.exists(f'{folder_path}/images'):\n",
        "    print('*** Descargando y extrayendo Flickr8k, siéntese y relájese 4 mins...')\n",
        "    print('****** Descargando las imágenes...')\n",
        "    !wget https://s06.imfd.cl/04/CC6204/tareas/tarea4/Flickr8k_Dataset.zip -P $folder_path/images -q --show-progress \n",
        "    print('********* Extrayendo las imágenes...\\n  Si te sale mensaje de colab, dale Ignorar\\n')\n",
        "    !unzip -q $folder_path/images/Flickr8k_Dataset.zip -d $folder_path/images\n",
        "    print('*** Descargando anotaciones de las imágenes...')\n",
        "    !wget http://hockenmaier.cs.illinois.edu/8k-pictures.html -P $folder_path/annotations -q --show-progress\n",
        "\n",
        "print('Inicializando pytorch Flickr8k dataset')\n",
        "full_flickr_set = torchvision.datasets.Flickr8k(root=f'{folder_path}/images/Flicker8k_Dataset', ann_file = f'{folder_path}/annotations/8k-pictures.html')\n",
        "\n",
        "print('Creando train, val y test splits...')\n",
        "train_flickr_set, val_flickr_set, test_flickr_set = [], [], []\n",
        "for i, item in enumerate(full_flickr_set):\n",
        "  if i<6000:\n",
        "    train_flickr_set.append(item)\n",
        "  elif i<7000:\n",
        "    val_flickr_set.append(item)\n",
        "  else:\n",
        "    test_flickr_set.append(item)\n",
        "\n",
        "print('Listo!')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inicializando pytorch Flickr8k dataset\n",
            "Creando train, val y test splits...\n",
            "Listo!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDKp92H8OnZB"
      },
      "source": [
        "#### Extrae los textos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2q2_dNcMda2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6f277c9-963d-478e-eeeb-e75414959e0e"
      },
      "source": [
        "train_text = extract_text_from_set(train_flickr_set)\n",
        "val_text = extract_text_from_set(val_flickr_set)\n",
        "test_text = extract_text_from_set(test_flickr_set)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6000/6000 [00:00<00:00, 622546.61it/s]\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 462947.46it/s]\n",
            "100%|██████████| 663/663 [00:00<00:00, 387031.81it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5HI9-O0OqIT"
      },
      "source": [
        "#### Genera los tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFt8moUHNtII"
      },
      "source": [
        "tokenizer = get_tokenizer('spacy')\n",
        "counter = Counter()  # para llevar la cuenta de los tokens y su ocurrencia\n",
        "\n",
        "train_tokens, counter = tokenize_text(train_text, tokenizer, counter)\n",
        "test_tokens, counter = tokenize_text(test_text, tokenizer, counter)\n",
        "val_tokens, counter = tokenize_text(val_text, tokenizer, counter)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hatUPnGOwdi"
      },
      "source": [
        "#### Define el vocabulario y agrega `<pad>` y `<sos>`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTJ-7AXmOSOw"
      },
      "source": [
        "vocab = list(counter.keys())\n",
        "vocab.append('<pad>')\n",
        "vocab.append('<sos>')\n",
        "word2idx = {word: i for i, word in enumerate(vocab)}\n",
        "pad_idx = word2idx['<pad>']"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WikFbU1LPA_G"
      },
      "source": [
        "#### Convierte oraciones a ids y genera los dataset de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDcBWuesOhrJ"
      },
      "source": [
        "train_sentences = encode_sentences(train_tokens, vocab, word2idx)\n",
        "test_sentences = encode_sentences(test_tokens, vocab, word2idx)\n",
        "val_sentences = encode_sentences(val_tokens, vocab, word2idx)\n",
        "\n",
        "train_dataset = TextDataset(train_sentences)\n",
        "test_dataset = TextDataset(test_sentences)\n",
        "val_dataset = TextDataset(val_sentences)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSvpUdxURgLR"
      },
      "source": [
        "Con todo lo anterior, además de tener los dataset para entrenamiento, podemos también obtener identificadores correspondientes a textos que nosotros decidamos, haciendo algo como lo siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCml7LsLPpNl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b540d213-3501-4085-f1db-d5bec1370793"
      },
      "source": [
        "s1 = 'A woman holding a cup of tea.'\n",
        "s2 = 'A man with a dog.'\n",
        "\n",
        "S = [s1,s2]\n",
        "tokens, _ = tokenize_text(S, tokenizer)\n",
        "D = encode_sentences(tokens, vocab, word2idx)\n",
        "\n",
        "print('tokens:', tokens)\n",
        "print('ids:', D)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokens: [['<sos>', 'a', 'woman', 'holding', 'a', 'cup', 'of', 'tea', '.'], ['<sos>', 'a', 'man', 'with', 'a', 'dog', '.']]\n",
            "ids: [[8460, 0, 238, 94, 0, 1570, 9, 1022, 14], [8460, 0, 78, 36, 0, 27, 14]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8oMGt5CpSNQ"
      },
      "source": [
        "#### Creamos los data loaders (puedes cambiar el tamaño del batch si lo deseas)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LS1ADBtQscp"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, \n",
        "    collate_fn=lambda data_list: pad_sequence_with_lengths(data_list, pad_idx))\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False, \n",
        "    collate_fn=lambda data_list: pad_sequence_with_lengths(data_list, pad_idx))\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False, \n",
        "    collate_fn=lambda data_list: pad_sequence_with_lengths(data_list, pad_idx))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mipSa7R1jAkd"
      },
      "source": [
        "**IMPORTANTE**: Nuestros datasets y dataloaders consideran también los largos de las secuencias. El siguiente código obtiene el primer elemento del dataset y el primer elemento del dataloader de prueba. Nota que lo que entregan en ambos casos es un par: la primera componente del par tiene los datos (los índices) mientras que la segunda componente tiene información de los largos de las secuencias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC-g5Ceyichj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c2282bc-5e07-4237-d8c7-2d831dad1835"
      },
      "source": [
        "d, length = test_dataset[0]\n",
        "print('len(d):', len(d))\n",
        "print('length:', length)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(d): 13\n",
            "length: 13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e31bJzWwQsah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11dc6d23-81a2-47ea-c769-4c2bad99eada"
      },
      "source": [
        "# Obtiene un paquete desde el dataloader\n",
        "for data in test_dataloader:\n",
        "  D, Lengths = data\n",
        "  break\n",
        "\n",
        "print(D.size())\n",
        "print(Lengths.size())\n",
        "\n",
        "# La primera dimensión de D corresponde al largo\n",
        "# máximo de las secuencias en el batch\n",
        "assert D.size()[0] == torch.max(Lengths)\n",
        "\n",
        "# La segunda dimensión de D corresponde al tamaño del\n",
        "# batch, al igual que la dimensión de Lengths\n",
        "assert D.size()[1] == batch_size \n",
        "assert Lengths.size()[0] == batch_size"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([25, 64])\n",
            "torch.Size([64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFRqoHLxrzYR"
      },
      "source": [
        "## 1a) Red recurrente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZswYBYer0kz"
      },
      "source": [
        "# Acá empieza tu código\n",
        "\n",
        "class RedRecurrente(torch.nn.Module):\n",
        "  def __init__(self, core = 'rnn',\n",
        "               vocab_size=10000, \n",
        "               emb_dim=100, \n",
        "               rec_dim=30, \n",
        "               rec_layers=3, \n",
        "               pad_idx=8459, \n",
        "               drop=.5, \n",
        "               batch_size=64): # Piensa en todo lo que necesitas para incializar.\n",
        "    '''\n",
        "    Args: \n",
        "    core: 'rnn', 'gru', 'lstm'\n",
        "    '''\n",
        "    super(RedRecurrente, self).__init__()\n",
        "    self.core = core\n",
        "    self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "    self.rnn = self.get_recurrente(emb_dim, rec_dim, rec_layers)\n",
        "    self.fc1 = nn.Linear(rec_dim, int(vocab_size/2))\n",
        "    self.drop1 = nn.Dropout(drop)\n",
        "    self.fc2 = nn.Linear(int(vocab_size/2), vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def get_recurrente(self, emb_dim, rec_dim, rec_layers):\n",
        "    if self.core == 'rnn':\n",
        "      return nn.RNN(emb_dim, rec_dim, rec_layers)\n",
        "    elif self.core == 'gru':\n",
        "      return nn.GRU(emb_dim, rec_dim, rec_layers)\n",
        "    elif self.core == 'lstm':\n",
        "      return nn.LSTM(emb_dim, rec_dim, rec_layers)\n",
        "    else:\n",
        "      return nn.RNN(emb_dim, rec_dim, rec_layers)\n",
        "\n",
        "  def forward(self, x, h_0=None):\n",
        "    # Acá debes programar la pasada hacia adelante.\n",
        "    # El vector h_0 deberías simplemente pasarlo directo\n",
        "    b_size = x.size()[1]\n",
        "    x = self.embedding(x)\n",
        "    x, h = self.rnn(x)\n",
        "    logits = self.fc2(self.drop1(self.fc1(x)))\n",
        "    # a tu red recurrente (RNN, o GRU, o LSTM) y será necesario\n",
        "    # para trabajar en la sección (1c) y en la parte 2. \n",
        "    # También puedes usar dropout, batch normalization o lo que necesites.\n",
        "    return logits"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTKju5aPsA6H"
      },
      "source": [
        "## 1b) Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0YB5e-NLQZC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30dfff59-de2c-4034-8ac9-1c04f279b68b"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from torch import optim\n",
        "import sys\n",
        "# indice de padding y tamaño de vocabulario de ejemplo\n",
        "pad_idx = 8459\n",
        "voc_size = 10000\n",
        "device = 'cuda'\n",
        "epochs = 10\n",
        "\n",
        "net = RedRecurrente(core='lstm',\n",
        "                    vocab_size=voc_size,\n",
        "                    emb_dim=100,\n",
        "                    rec_dim=120,\n",
        "                    rec_layers=3,\n",
        "                    pad_idx=pad_idx)\n",
        "\n",
        "\n",
        "# función de error considerando padding y promedio (mean)\n",
        "loss_fn = CrossEntropyLoss(ignore_index=pad_idx, reduction='mean')\n",
        "optimizer = optim.Adam(net.parameters()) \n",
        "\n",
        "train_loss, val_loss = [], []\n",
        "\n",
        "net.to(device)\n",
        "for epoch in range(epochs):\n",
        "  net.train()\n",
        "  temp_loss = 0\n",
        "  \n",
        "  for i, data in enumerate(train_dataloader):\n",
        "    X, length = data\n",
        "    X, y = X[:-1, :].to(device), X[1:,:].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    logits = net(X).view(-1, voc_size)\n",
        "    loss = loss_fn(logits, y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    items = (i+1) * batch_size\n",
        "    temp_loss += loss.item()\n",
        "    sys.stdout.write(f'''\\rEpoch {epoch+1}: ({items}/{len(train_dataloader)*batch_size}), Train_loss = {temp_loss/(i+1):02.5f}''')\n",
        "\n",
        "  train_loss.append(temp_loss/len(train_dataloader))\n",
        "  sys.stdout.write(f'''\\nTrain Epoch Loss = {temp_loss/len(train_dataloader)}''')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    print(f'\\nValidando epoch {epoch+1}')\n",
        "    for i, data in enumerate(val_dataloader):\n",
        "      X, y = data\n",
        "      X, y = X[:-1, :].to(device), X[1:,:].to(device)\n",
        "      logits = net(X).view(-1, voc_size)\n",
        "      loss = loss_fn(logits, y.view(-1))\n",
        "\n",
        "      sys.stdout.write(f'''\\rValidando ({(i+1)*batch_size}/{len(val_dataloader)*batch_size}), Val loss = {loss}''')\n",
        "    val_loss.append(loss)\n",
        "    print('\\n')\n",
        "    "
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: (30016/30016), Train_loss = 4.39257\n",
            "Train Epoch Loss = 4.392572347543387\n",
            "Validando epoch 1\n",
            "Validando (5056/5056), Val loss = 5.820651531219482\n",
            "\n",
            "Epoch 2: (30016/30016), Train_loss = 3.57468\n",
            "Train Epoch Loss = 3.574680325827365\n",
            "Validando epoch 2\n",
            "Validando (5056/5056), Val loss = 5.629904270172119\n",
            "\n",
            "Epoch 3: (30016/30016), Train_loss = 3.32718\n",
            "Train Epoch Loss = 3.327182056553074\n",
            "Validando epoch 3\n",
            "Validando (5056/5056), Val loss = 5.67933988571167\n",
            "\n",
            "Epoch 4: (30016/30016), Train_loss = 3.16546\n",
            "Train Epoch Loss = 3.16545699603522\n",
            "Validando epoch 4\n",
            "Validando (5056/5056), Val loss = 5.6281914710998535\n",
            "\n",
            "Epoch 5: (30016/30016), Train_loss = 3.03457\n",
            "Train Epoch Loss = 3.0345716435772014\n",
            "Validando epoch 5\n",
            "Validando (5056/5056), Val loss = 5.5612592697143555\n",
            "\n",
            "Epoch 6: (30016/30016), Train_loss = 2.92246\n",
            "Train Epoch Loss = 2.9224585226095563\n",
            "Validando epoch 6\n",
            "Validando (5056/5056), Val loss = 5.669826507568359\n",
            "\n",
            "Epoch 7: (30016/30016), Train_loss = 2.82326\n",
            "Train Epoch Loss = 2.823264302983721\n",
            "Validando epoch 7\n",
            "Validando (5056/5056), Val loss = 5.863853454589844\n",
            "\n",
            "Epoch 8: (30016/30016), Train_loss = 2.72963\n",
            "Train Epoch Loss = 2.729626059277988\n",
            "Validando epoch 8\n",
            "Validando (5056/5056), Val loss = 5.932770729064941\n",
            "\n",
            "Epoch 9: (30016/30016), Train_loss = 2.64945\n",
            "Train Epoch Loss = 2.649450337200531\n",
            "Validando epoch 9\n",
            "Validando (5056/5056), Val loss = 6.157528400421143\n",
            "\n",
            "Epoch 10: (30016/30016), Train_loss = 2.57157\n",
            "Train Epoch Loss = 2.5715714024582397\n",
            "Validando epoch 10\n",
            "Validando (5056/5056), Val loss = 6.198164939880371\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUjoDB5ScV04",
        "outputId": "60e122fe-762c-4732-8528-21493b017339"
      },
      "source": [
        "train_loss"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.392572347543387,\n",
              " 3.574680325827365,\n",
              " 3.327182056553074,\n",
              " 3.16545699603522,\n",
              " 3.0345716435772014,\n",
              " 2.9224585226095563,\n",
              " 2.823264302983721,\n",
              " 2.729626059277988,\n",
              " 2.649450337200531,\n",
              " 2.5715714024582397]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "gEfKFvLfZG7s",
        "outputId": "778b8984-8787-45df-b696-090e020f8aea"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "plt.plot(range(1, epochs + 1), train_loss, '--o', label='Train Loss')\r\n",
        "plt.plot(range(1, epochs + 1), val_loss, '--+', label='Validation Loss')\r\n",
        "plt.xlabel('Epochs')\r\n",
        "plt.ylabel('Loss')\r\n",
        "plt.legend()\r\n",
        "plt.title('Perdidas en entrenamiento')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Perdidas en entrenamiento')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXgV5fXA8e/JvhCSkIQlJDHsm7IIgogoS11RROuCRQvaavVnAW3FSqsWt4rWtmqtUqt1V1xLQUVcALGugOyb7BDWJJCQQEK28/tjJslNuAkBcnMT7vk8z31yZ7kz506SOfPO+877iqpijDEmcAX5OwBjjDH+ZYnAGGMCnCUCY4wJcJYIjDEmwFkiMMaYAGeJwBhjApwlAtMgRGS+iPzSfT9GRD6py7rGv0QkX0Ta+zsO41uWCEwFEdkiIgXuP/8eEXlJRJrV935U9XVVPb++t9sUNLUkp6rNVHXTiW7H/Vt6qD5iMvXPEoGp7lJVbQacDvQD7jmWD4vD/q6Ok4iE+DsGE3jsH9Z4pao7gNnAqQAicqaIfC0iOSKyTESGlK/rXuU+LCJfAYeA9iJynoisFZFcEXkaEI/1x4nI/zyma1u3g4jMFZFsEckSkddFJM5j+e9EZIeI5InIOhEZ7u37iEi4iDwuItvc0s40EYl0lw0RkQwR+a2I7BWRXSJyQ03HRkRiReQFd70dIvKQiAR7fjd3X/tFZLOIXOQuexgYDDztlrqedueriNwmIuuB9e68S0RkqXu8vxaRnh773yIid4rIcveYvSUiEe6yeBH5QEQy3f1/ICIp1X5XD7nbzBeRWSKS4B7XAyKyUETSPdZXEel4IsdQRG4GxgB3le/Tnd/NjSdHRFaJyMiajrnxMVW1l71QVYAtwE/c96nAKuBBoC2QDVyMc/Fwnjud5K47H9gG9ABCgCQgD7gSCAXuAEqAX7rrjwP+575PPMq6Hd39hbvbXQA84S7rAmwHkt3pdKBDDd/tb8BMoAUQA8wCHnGXDXH3+YAbw8U4CS2+hm39B/gnEA20BL4HfuXx3YqBm4Bg4FZgJyAex+qX1banwKdubJFAH2AvMMDdxlj3dxPu8Xv6Hkh2P7MGuMVdlgD8FIhyv+c7wAyPfc0HNgAdgFhgNfAj8BP3d/cK8GK12Dqe6DEEXgIe8thuqBvH74EwYJj7d9DF3/8HgfjyewD2ajwv9wSTD+QAW4Fn3BPT74BXq607Bxjrvp8PPOCx7OfAtx7TAmTgPRHUuq6XGEcBS9z3Hd0T5k+A0Fq+lwAH8UgSwEBgs/t+CFAAhHgs3wuc6WVbrYDDQKTHvGuBeR7fbYPHsij3ZNra41h5SwTDPKafBR6sts464FyP39N1HsseA6bV8N17A/s9pucDf/CY/gsw22P6UmBptdg6nugx5MhEMBjYDQR5zHsTmOLv/4NAfNn9SFPdKFX9zHOGiJwCXCUil3rMDgXmeUxv93if7Dmtqioinsup67oi0gp4EufEEYNTItnvrrtBRG4HpgA9RGQO8BtV3VltH0k4J+TFIpV3nXCutstlq2qJx/QhwFtF+Sk4332Xx7aCqn3/3R7f55C73tEq3T0/fwowVkTGe8wLwzlWR+zDjTUZQESicK7cLwTi3eUxIhKsqqXu9B6PzxZ4mfYWa30eQ9x4t6tqmce8rTilT9PArI7A1MV2nBJBnMcrWlWneqzj2Y3tLpxbS4BTgew5Xc3R1v2Tu+3TVLU5cB0edQiq+oaqno1z8lTgUS/7yMI5wfXwiD9WnUrxY7Udp0SQ6LGt5qrao46fr6m7X8/524GHqx3vKFV9sw7b/y3OLbMB7vE6x50vNX+kTk70GFb/3juB1GoNC9KAHScYpzkOlghMXbwGXCoiF4hIsIhEuJWDKTWs/yHOFfoV4rSCmQC0Ps51Y3BuV+WKSFtgUvkCEekiIsNEJBwoxDlReV5hAuBedf4L+JuItHQ/21ZELqj7IajY1i7gE+AvItJcRILcCu1z67iJPcDR2uX/C7hFRAaII1pERohITB22H4NzHHJEpAXwxzrGVat6OIbVv/d3OCWGu0QkVJzGB5cC0+sjXnNsLBGYo1LV7cBlOBV7mThXrJOo4e9HVbOAq4CpOJXKnYCvjnPd+3GasubiJI33PZaFu5/LwrlV0hKYXMPX+B1O5eS3InIA+Aznyvl4/BznVs1qnNtU7wJt6vjZJ4Er3RY9T3lbQVUX4VQ2P+1ufwNO3UNdPIFTr5MFfAt8XMfP1cWJHMMXgO5uC6EZqlqEc+K/yI31GeDnqrq2HuM1dVTeksEYY0yAshKBMcYEOEsExhgT4CwRGGNMgLNEYIwxAa7JPVCWmJio6enp/g7DGGOalMWLF2epapK3ZU0uEaSnp7No0SJ/h2GMMU2KiGytaZndGjLGmABnicAYYwKcJQJjjAlwlgiMMSbAWSIwxpgAZ4nAGGOainmP+GSzlgiMMaap+GLq0dc5DpYIjDHmaHx0JX6E4gI4sAv2roFt30L2Rmf+4TxY5ruhGprcA2XGGNPgvpgKQ2sa6sKDqnPSLsyFwhwIiYTEjs6yb5+FQ9lQkFO5vP1QGPh/UFIEj6RA6eGq2ztrAoRGVS0JTIl1fp57d91iqgNLBMYYUxcr34Ocbc5JvCDHOZEndIJhf3CWPzMQMtdBxdDQQPfL4OpXnPfzpzqfjYiFyDiIiIOSQmdZSBic9WsIa1Z1eUIHiE+HIXdDWSk8mABTcuv9q1kiMMaY6g7sghm3wqZ5lfPevdF9EwTRCc6JOrx55fIelzsn9oi4yhN5/CmVy+9YCaHREFTDHfnh99UcjwgE++50bYnAGGMADufDmlmw/C3Y/AVoGaScAeM+hIdawqRNEBrh3KoROfLz595V+/bD6zLk9FGce/eJb8MLSwTGmMBVWuKc8EPC4IdXYM5kiDsFBt8JPa+GxE6V60Yn+C/OcvVUJ1CdJQJjTGBRhd3LYdlbsPJdGP5H6DMGel4DyX0g7cwjr/h9dCXeWFgiMMYEhrJS+OpJWP42ZK6BoFDofIFTGQvOFX/0QO+f9dGVeGNhicAYc/IqPOBc/aefDUHBsOo/ENEcRvzVqdyNauHvCBsFnyYCEYkDngdOBRS4UVW/8VguwJPAxcAhYJyq/uDLmIwxJ7nSYtg416n0XfshSBDcuR7Cm8GNcyAsyt8RNjq+LhE8CXysqleKSBhQ/TdwEdDJfQ0AnnV/GmPMsVv7EcwcD4eyIDIe+lwHPUdDWLSz3JKAVz5LBCISC5wDjANQ1SKgqNpqlwGvqKoC34pInIi0UdVdvorLGHMS2b8VVrwN6YOdSt74U+CUs6DXaOh4ntMayByVL0sE7YBM4EUR6QUsBiaq6kGPddoC2z2mM9x5VRKBiNwM3AyQlpbmw5CNMY1eQQ6snuFU+m79ypk39A9OImjVA6551b/xNUG+7HQuBDgdeFZV+wAHgeNqg6Wqz6lqP1Xtl5SUVJ8xGmMas/LO3lQrf/7zHJg1EfL3wrB7YOLyoz/MZWrlyxJBBpChqt+50+9yZCLYAaR6TKe484wxga6szOls7VCW0xPnrxY4LX8ufARiWkPy6d6f8DXHzGeJQFV3i8h2EemiquuA4cDqaqvNBH4tItNxKolzrX7AmABQVub0xHkgAw7sdF7dR0GzJFj5Pnw2BfLcU8GS16DrCKfDtqgWzntTr3zdamg88LrbYmgTcIOI3AKgqtOAj3Cajm7AaT56g4/jMcbUxbxHjv8hqrIy5yr+wA7I3eGe6Hc4Fbgtu8GPc+Ct66C0WtuRxM7Q7FxY/ynkbK2cX1Lo9PyZ0Omkf7DLX0TL7701Ef369dNFixb5OwxjTm5TYmvu7rikCPauck7wuTuck3z5ib7jTyBjETw/vOpngkLhp/9yHuLK3gg/vAzNU6B5MsS2heZtISrxyJ45a4vDHBMRWayq/bwtsyeLjTFVFRc4P797rvIkf2CncxLvf5Nztf/ckMr1g0KdE3qHYc50Qge46DHn5N48GWJTqp7kEzrAeQ806FcytQusRHAixV1jTmb7t8CCv8CSVyrnzZ7k/AyPhdanQki4M92sFYx+wznJe7uSj4yHAb+qn7hO8s7eGovASgR1HW7OmJOdKuxe4XTBsPZD2LMCWvesvA0zJdbplsHb7Zqg4IarsLX/1wYRWIkA4P1fQdeLnWJsfQwUYUxTUVZWeVL/722w9HVAIG0gnP+w83/hqVnLBg/R+MfJnwjmPVJ14Ofl052XBEGH4dBjlNMfiTEno6JDTgdsaz+E9XPglv85t3ROu9JJAJ0vdJpsVme3ZAJKYLUamhIL92bD9m+dzqnWfeg0SbvuXWf54pchpR+07G4PqpimLWs9fPpHJwmUFDgDone+0BkEvUV7f0dn/MBaDXkKDnH6Jk8/Gy54GIrynfkHs+GD251h6+JOce6BdrkI0s7y6aDRxtSL/Vth3UfQogN0Ph/Cmjn98J9+vfO3fMogCA71d5SmkQqsM1z14q5IZT1BdAL8Zg2sm+38Qy18Ab59Bi75G/S7EYoOOknC6hVMY1FR2fuB8x6g3y+cRNC8Ddy+wkq2pk4C69bQsTic7xSr0850Ks0Wvwwf3QntzoEuFzulhebJvo/DmHKlJZD1I7Tq7kz/8xzYtdz5G+06wvm7TOjg3xhNo1XbrSFLBHW1eyUse9O5Atu/2ZmX3AdumA2hkQ0fT32w5yoaJ8/fS9Eh2DTP+btbN9t52OuuTc4AK7uWO52vWeseUwdWR1AfWp8KrR+G8x+CzHVORXP2xsok8OGdEBTiNMFLG9h478cWFzqdfRUdrHyuoqzUaRtuGofy38uKd+G/v3Yqe8NjnYHWu46o/F216enfOM1JwxLBsRKBll2dVzlV57H7tR/Bd89CRBx0Oh96X1v52L2n+r4SP7DTqSw8lO2+spwryeH3Oss/vc8ZtPvQvsrK8Zg2lZ+fPga2fAlRCRCd6DxE1LJrZTcAG+c548BGJzjLohMrh/6rD4FcMikucC4s9q6GPatg55LKZS27O02byyt7bbQt4yOWCOqDCFz1knOVvXGuU4T/8WOIT3cSQXEhLH0NOl/kdLBV0xPOxQVwMKvyhJ5+tvNY//rPnArBihO9+7pjlbP8f0/A9/+suq2wZs6oTUFBTjcAaQOdk/juFbBlQWUXv1NinZ9t+znNCg9lVS4r99kfYdeyqvPSB8O4D5z3H9zhxO6ZSJK6QGp/Z3nRQQiNqrniMhCe+C4rhX2bnRN+TGvn2BzYBX/r7jRCqK7893Lu3dBhaMPGagKOJYL6FBYN3S51XmWllZ13bfsGPvyt82rT25n33k0w/D6IS3Uqoj++G4oPVd3exGVOMtm7GtbMck+yCZDYCaIGOt34hoRD37HObYPyE3Fki6qDdNfU70tde3a85jXI2+MkiYNZzs9oj4eQ9m91KjEPZjm3McDpW748Efy1u/PdyksT0YnOVe4Zv6zcxsa5EJPsVMBHND96TI2VqvN7D4ty3s+a4NzLz1xXeWx6j3GOTUxrGDLZ6X65VQ+Ib+c0VbYeN00Ds8rihqDqtDha+PyRy8692yk1rJnpnMgrrqoToE0v31ZE++KEU3QQDmY6T27HueNLf/13Z1jBQ9mViSQ0yrkd5U1YDJzzWzj7DucW15d/cZpDlvdmGZPsHJ/qfeD4w86lsGsp7FldeXunTS/4+Qxn+YsjnJN7yx5Oa5+W3SGpa9VEXZ0lAuMDVlnsbyIw4i/OC7z/o6cNaPi4fNGNQFj0kfUHZ42vef2yMnggHsZ95NySOrDDuWWS1M1Znr8b/vfXI2+fXDgVzrwVcrY5o1mVJ4jyHjGTOjtP0x6LmuoqSooge4Nzot+72hkpq/x3+em9sHmBcyuuZTfodgmknln52Rs+PLYYwLp3MA3OEkEgawz35cuv6tMHeV/eoj3cm+WUKA7shDx3WMP0s53lh7KdCta1HzojWZW7+hXofpkz1u3Hk90EkVyZMDoOd0peqpV1F19Mhd4/g8y1TmW/iJNkvn4ayordeEOcJFXegdtFjzmltti0+iuhNIbfiwkolgj8wa74qjra8QgKdm8NtQH6Vl2W3AcmLHFO6AX7K8e/TXbrYhCnf/zsjbD5SzjslsR+OddJBEvfgE/+4PSxD/Ck2yTzjtVOxX7y6TDwNucefsvuTv1Meb/84JQCjGnifFpHICJbgDygFCipfn9KRIYA/wXcJ7R4X1VrHbqoSdYRmMbjcL5zCyo2FUIjYMb/ud0xVzP4zsrmt8acBPxdRzBUVbNqWf6lql7SAHEYA+HNILxT5fSoZ5wXWCWtCViNoNmFMcYYf/J1IlDgExFZLCI317DOQBFZJiKzRaSHtxVE5GYRWSQiizIzM30XrQlsVndjApSv6wjaquoOEWkJfAqMV9UFHsubA2Wqmi8iFwNPqmqnmrYHVkdgjDHHo7Y6Ap+WCFR1h/tzL/AfoH+15QdUNd99/xEQKiKJvozJGGNMVT5LBCISLSIx5e+B84GV1dZpLeI04haR/m482b6KyRhjzJF82WqoFfAf9zwfAryhqh+LyC0AqjoNuBK4VURKgAJgtDa1Pi+MMaaJ81kiUNVNQC8v86d5vH8aeNpXMRhjjDk6az5qjDEBzhKBMcYEOEsExhgT4CwRGGNMgLNEYIwxAc4SgTHGBDhLBMYYE+AsERhjTICzRGCMMQHOEoExxgQ4SwTGGBPgLBEYY0yAs0RgjDEBzhKBMcYEOEsExhgT4CwRGGNMgLNEYIwxAc4SgTHGBDifJgIR2SIiK0RkqYgs8rJcROQpEdkgIstF5HRfxmOMMeZIvhy8vtxQVc2qYdlFQCf3NQB41v1pjDGmgfj71tBlwCvq+BaIE5E2fo7JGGMCiq8TgQKfiMhiEbnZy/K2wHaP6Qx3njHGmAbi61tDZ6vqDhFpCXwqImtVdcGxbsRNIjcDpKWl1XeMxhgT0HxaIlDVHe7PvcB/gP7VVtkBpHpMp7jzqm/nOVXtp6r9kpKSfBWuMcYEJJ8lAhGJFpGY8vfA+cDKaqvNBH7uth46E8hV1V2+iskYY8yRfHlrqBXwHxEp388bqvqxiNwCoKrTgI+Ai4ENwCHgBh/GY4wxxgufJQJV3QT08jJ/msd7BW7zVQzGGGOOzt/NR40xxviZJQJjjAlwlgiMMSbAWSIwxpgAZ4nAGGMCnCUCY4wJcJYIjDEmwFkiMMaYAGeJwBhjApwlAmOMCXCWCIwxJsBZIjDGmABnicAYYwKcJQJjjAlwlgiMMSbAWSIwxpgAZ4nAGGMCnCUCY4wJcJYIjDEmwPk8EYhIsIgsEZEPvCwbJyKZIrLUff3S1/EYY4ypqk6D14tINFCgqmUi0hnoCsxW1eI6fHwisAZoXsPyt1T113WK1hhjTL2ra4lgARAhIm2BT4DrgZeO9iERSQFGAM8fb4DGGGN8q66JQFT1EHAF8IyqXgX0qMPnngDuAspqWeenIrJcRN4VkVSvOxe5WUQWiciizMzMOoZsjDGmLuqcCERkIDAG+NCdF3yUD1wC7FXVxbWsNgtIV9WewKfAy95WUtXnVLWfqvZLSkqqY8jGGGPqoq6J4HZgMvAfVV0lIu2BeUf5zCBgpIhsAaYDw0TkNc8VVDVbVQ+7k88DfescuTHGmHpRp8piVf0C+AJARIKALFWdcJTPTMZJHojIEOBOVb3Ocx0RaaOqu9zJkTiVyvVuxpId/HnOOnbmFJAcF8mkC7owqk9bX+zKGGOanDqVCETkDRFp7rYeWgmsFpFJx7NDEXlAREa6kxNEZJWILAMmAOOOZ5u1mbFkB5PfX8GOnAIU2JFTwOT3VzBjyY763pUxxjRJoqpHX0lkqar2FpExwOnA3cBi995+g+rXr58uWrSozusPmjqXHTkFR8xvGxfJV3cPq8/QjDGm0RKRxaraz9uyutYRhIpIKDAKmOk+P3D0DNII7PSSBGqbb4wxgaauieCfwBYgGlggIqcAB3wVVH1Kjov0Or9NXEQDR2KMMY1TnRKBqj6lqm1V9WJ1bAWG+ji2ejHpgi5Ehh7Z0vXU5JoedDbGmMBS18riWBH5a/lDXSLyF5zSQaM3qk9bHrniNNrGRSI4dQMD2sXz2Zq9bM466O/wjDHG7+rUfBT4N05roavd6euBF3GeNG70RvVpW6W5aGFxKYu37qddYpPIZcYY41N1TQQdVPWnHtP3i8hSXwTUECJCgxnUMRGA5Rk5dGvTnNBg65HbGBOY6nr2KxCRs8snRGQQ0OSb3WzKzOfyZ77msY/X+jsUY4zxm7qWCG4BXhGRWHd6PzDWNyE1nPZJzRgzII1/fbmZfuktuKBHa3+HZIwxDa6urYaWqWovoCfQU1X7ACfF01h/GNGNnimx3PnOMrZlH/J3OMYY0+CO6ca4qh5Q1fLnB37jg3gaXHhIMP/42ekI8H9vLOZwSam/QzLGmAZV11tD3ki9ReFnqS2i+MvVvflxTx6hQVZpbIwJLCeSCJpEFxN1dV73VpzXvRUAxaVl1orIGBMwaj3biUieiBzw8soDkhsoxga1cMs+hj4+n42Z+f4OxRhjGkStiUBVY1S1uZdXjKqeSGmi0UqJj+Tg4RJue/0HCoqsvsAYc/Kz+x/VtImN5InRfVi3J4/7/rvS3+EYY4zPWSLw4tzOSYwf2pF3Fmfw9qLt/g7HGGN8yhJBDSb+pDMD2yfw/eZ9/g7FGGN86qS8z18fgoOEF8b189qFtTHGnEx8XiIQkWARWSIiH3hZFi4ib4nIBhH5TkTSfR3PsYgKC0FE2JSZz18+WUddhvU0xpimpiFuDU0E1tSw7BfAflXtCPwNeLQB4jlmn6zew9/nbuC177b5OxRjjKl3Pk0EIpICjACer2GVy4CX3ffvAsNFpNE9sXzz4PYM6ZLEg7NWsyIj19/hGGNMvfJ1ieAJ4C6grIblbYHtAKpaAuQCCdVXEpGby0dHy8zM9FWsNQoKEv52dW8Sm4Xxf28sJreguMFjMMYYX/FZIhCRS4C9qrr4RLelqs+paj9V7ZeUlFQP0R27+Ogw/v6z09mVU8g/5m3wSwzGGOMLvmw1NAgYKSIXAxFAcxF5TVWv81hnB5AKZIhICBALZPswphPS95R4XrqhP/3S4/0dijHG1BuflQhUdbKqpqhqOjAamFstCQDMpHKAmyvddRp105yzOyUSERpMXmExP+7J83c4xhhzwhr8gTIReUBERrqTLwAJIrIBZ3yDuxs6nuP16zeWMPbf37PvYJG/QzHGmBPSIIlAVeer6iXu+/tUdab7vlBVr1LVjqraX1U3NUQ89eHO87uQnV/EHW8tpaysURdijDGmVtbFxHE6LSWW+y7tzhc/ZvLsFxv9HY4xxhw3SwQnYMyANEb2SuYvn6zjm42Nto7bGGNqZX0NnQAR4U9XnEZIkHBKQpS/wzHGmONiieAENQsP4a/X9AagrExRnA7rjDGmqbBbQ/XkcEkpN7y0kCc++9HfoRhjzDGxRFBPwkOCadU8nL/P3cD8dXv9HY4xxtSZJYJ6dP/IU+naOoY73lrKzpwCf4djjDF1YomgHkWGBfPMmNMpKinj12/8QHFpTX3tGWNM42GJoJ61T2rG1J/2ZGdOIRn7rVRgjGn8rNWQD1zaK5nh3VoSFWaH1xjT+FmJwEeiwkIoKS3jr5+sY1v2IX+HY4wxNbJE4EN78g7z0tdbuO2NHzhcUurvcIwxxitLBD7UNi6Sx6/qxYoduTz8YU3DNhtjjH9ZIvCx83u05qbB7Xjlm63MWrbT3+EYY8wRpJGPA3OEfv366aJFi/wdxjEpLi1j9HPfsnbXAZpHhrI7t5DkuEgmXdCFUX3a+js8Y0wAEJHFqtrP2zIrETSA0OAgLu3VhpIyZVduIQrsyClg8vsrmLFkh7/DM8YEOEsEDeRfCzZzuKTqA2YFxaX8ec46P0VkjDEOSwQNpKYuJ3bkFNgIZ8YYv/JZIhCRCBH5XkSWicgqEbnfyzrjRCRTRJa6r1/6Kh5/S46LrHHZkMfns3b3gQaMxhhjKvmyRHAYGKaqvYDewIUicqaX9d5S1d7u63kfxuNXky7oQmRocJV5EaFBXH9mGh2Soklr4QxsM3ftHhb8mGmlBGNMg/FZHwjqNEfKdydD3VfAnt3KWwf9ec46duYU1NhqaNr8TXy/ZR+pLSIZfUYaV/VLoWVMhD9CNsYECJ82HxWRYGAx0BH4h6r+rtryccAjQCbwI3CHqm73sp2bgZsB0tLS+m7dutVnMfvb4ZJS5qzaw5vfbeObTdmEBAnjh3Vi4k86+Ts0Y0wTVlvz0QZ5jkBE4oD/AONVdaXH/AQgX1UPi8ivgGtUdVht22qKzxEcr02Z+by1cDv90ltwXvdW7M0r5O2F27mqXyqtmlspwRhTd35PBG4Q9wGHVPXxGpYHA/tUNba27QRSIqju/R8y+M3bywgOEoZ3bcm1A9I4p1OSjZFsjDmq2hKBz+oIRCQJKFbVHBGJBM4DHq22ThtV3eVOjgSsQ55aXHF6Cn3S4pm+cBvvLsrgk9V7SGsRxSd3nENEtYpoY4ypK192mN8GeNm90g8C3lbVD0TkAWCRqs4EJojISKAE2AeM82E8J4V2idFMvqgbvz2vC5+u3sO63QcqksBTn6/ntLaxnNPZSgnGmLqzvoZOEoeKShjy5/nszTtMcmwE15yRxtVnpNAmtubnF4wxgcP6GgoAUWEh/O93w3h2zOl0aNmMv332I4OmzuXD5buO/mFjTECzsRRPImEhQVx0WhsuOq0N27IPMX3hNvq3awHAZ6v3sGJHLteckcr3m/cd9XkGY0zgsERwkkpLiOKuC7tWTC/aup9/LtjIk5+vJ0ig/MHl8l5QAUsGxgQouzUUIO6+qCsLJg0lJjyE6r1XFBSXMnX2GppafZExpn5YiSCApLaIIv9widdluw8c5qypcxnYIYFBHRIZ1DGR1rH20JoxgcASQYBJjotkh5cuseMiQzk9LZ55a/fy/g/OYDkPjTqV6848hcLiUgqLS4mLCmvocI0xDcASQYCZdEEXJr+/goLi0op5kaHBTBnZg1F92lJWpqzdncfXG7M4s30CAPPXZXLr64vpkdycQR0SGdghgf7tWhAVZn8+xsdXoPMAABk3SURBVJwM7DmCADRjyY5jajW0JesgM5ft5KsNWSzZlkNRaRmhwcK8O4eQEh/FvoNFNAsPISzEqpyMaawaRV9D9cUSgX8VFJWycMs+fti2n4nDOyEi/PbtZXy0YhdntGvBoA4JnNUhke7Jze3pZmMaEb/0NWROTpFhwZzTOYlzOidVzBvVJ5lm4cF8tTGbR2avBaBXSiz//fXZAOw9UEhSTDgilhiMaYwsEZgTNrhTEoM7OYlh74FCvt6YTanbRrWsTDn/iQVEhARzVocEzuqYyKCOCbSJjTzmW1TGGN+wW0PGpw6XlPLe4h18tTGLbzZms+9gEQAjTmvN3LWZR1RaP3LFaZYMjPEBuzVk/CY8JJifDUjjZwPSKCtT1u3J46sNWfxzwaYqSQCcB9vu++9KWsdG0Csljsgw61rbmIZgzTxMgwkKErq1ac4vB7cnK++w13UOFJYw+rlvWb0rF4AVGbm8/t1WVu7Ipbi0rCHDNSZgWInA+EVND7a1jo3gkctPo0eyM1Ddp6t389TcDQCEhwRxattYeqXEMemCLlZiMKaeWB2B8YsZS3Z4fbCteh2BqrJ9XwFLM3JYtt15Zewv4JvJwxARHvxgNRv25tMrNY7eqbH0TIkjsVm4P76SMY2a1RGYRqf8ZH+0VkMiQlpCFGkJUYzslQw4LZHKm6LGRISw50AhT89dX9GZ3tkdE3ntlwMA+HFPHinxkfYUtDG1OClKBMXFxWRkZFBYWOinqMzxiIiIICUlhdDQ0BPe1sHDJazckcuyjBwiQoP5+cB0VJW+D31GzqEiOreKoXdqHL1S4xjQrgXtk5pVfNaasZpA4Jcni0UkAlgAhOOUPN5V1T9WWycceAXoC2QD16jqltq26y0RbN68mZiYGBISEuyhpSZCVcnOziYvL4927dr5ZB9lZcq8dXtZtj2HpRm5LNueQ25BMTcNbscfRnTncEkpv3plEV9tzKa4tPL/wJqxmpORv24NHQaGqWq+iIQC/xOR2ar6rcc6vwD2q2pHERkNPApcc6w7KiwsJD093ZJAEyIiJCQkkJmZ6bN9BAUJw7u1Yni3VoCTfLZmHyLU7RNpa/Yh5v+YdcTnCopL+fOcdVx4amvKVO22kjnp+ewvXJ2iRr47Geq+qhc/LgOmuO/fBZ4WEdHjKKZYEmh6Gvp3JiKkJ0ZXTHduFYNw5B8lwM6cAr5cn8XNry7ilBZRdGvTnK6tm9O1TQxndUggJuLEb2cZ01j49FJHRIKBxUBH4B+q+l21VdoC2wFUtUREcoEE4MjLNGN8oKZmrMlxkbRLjOb24Z1Zu/sAa3fn8fGq3ajCZ785l5iIUD5ZtZsvfsykW5vmdGsTQ5fWzWkWbqUH0/T49K9WVUuB3iISB/xHRE5V1ZXHuh0RuRm4GSAtLe2E46rvysHs7GyGDx8OwO7duwkODiYpyel75/vvvycsrOYBXRYtWsQrr7zCU089Vef9paens2jRIhITE487ZuOoaXyGSRd0oWPLZkz8SaeK+QcPl/DjnjzSE6IA59bSzGU7ef27bRXrpLWI4pM7ziEiNJj1e/IICQ7ilBZRBFlPrKYRa5DLF1XNEZF5wIWAZyLYAaQCGSISAsTiVBpX//xzwHPgVBafSCzV26/Xx+DtCQkJLF26FIApU6bQrFkz7rzzzorlJSUlhIR4P9T9+vWjXz+v9TemAdS1GStAdHgIfdLiK6ZvOqc9vxzcjp25hazd5ZQaduUWEBHqPOj26Mfr+GzNHiJDg+nSOoZubWLokxrP1Wekeo3FWi8Zf/FZIhCRJKDYTQKRwHk4lcGeZgJjgW+AK4G5x1M/UN01//zmiHmX9GzD9QPTeezjtV77uJkyaxWj+rRl38Eibn1tcZXlb/1q4DHHMG7cOCIiIliyZAmDBg1i9OjRTJw4kcLCQiIjI3nxxRfp0qUL8+fP5/HHH+eDDz5gypQpbNu2jU2bNrFt2zZuv/12JkyYUKf9bdmyhRtvvJGsrCySkpJ48cUXSUtL45133uH+++8nODiY2NhYFixYwKpVq7jhhhsoKiqirKyM9957j06dOh19JyepUX3aHvcJV0RoGxdJ27jIikrpcr89vzPnd2/Fmt0HWLsrj9krd7Nud15FIrjplUWoKl1bNyevsJjpC7dzuMTpRqM+LlCMqStflgjaAC+79QRBwNuq+oGIPAAsUtWZwAvAqyKyAdgHjPZhPADsyvX+rEHOoeJ631dGRgZff/01wcHBHDhwgC+//JKQkBA+++wzfv/73/Pee+8d8Zm1a9cyb9488vLy6NKlC7feemud2tmPHz+esWPHMnbsWP79738zYcIEZsyYwQMPPMCcOXNo27YtOTk5AEybNo2JEycyZswYioqKKC0tPcrWzfFw6g6aV0yrKgeLKo91fFQoS7fnMG9dZkW33Z4Kiku5f9YquraJoV1iNOEh1qWG8Q1fthpaDvTxMv8+j/eFwFX1ve/aruBrqhxsGxcJQIvosOMqAXhz1VVXERzs/PPm5uYyduxY1q9fj4hQXOw98YwYMYLw8HDCw8Np2bIle/bsISUl5aj7+uabb3j//fcBuP7667nrrrsAGDRoEOPGjePqq6/miiuuAGDgwIE8/PDDZGRkcMUVVwR0aaAhiUiVyuTHruwFOF11d73nY6+tl/YfKubCJ74kSJz6h1vO7cDo/mkUlZSxcmcuHVs2o7m1YDInKOB6H510QRciQ6teWZVXDta36OjKpor33nsvQ4cOZeXKlcyaNavGp6DDwyv7yQkODqakpOSEYpg2bRoPPfQQ27dvp2/fvmRnZ/Ozn/2MmTNnEhkZycUXX8zcuXNPaB/mxISHBJPsXohUlxQTzpOje/ProR3pntyc2EjnpL8pK58rnvmanlM+YcCfPmPM89/yx/+uZPXOA4DzMF1T6zXA+E/AtXU7lsrB+pSbm0vbts4+XnrppXrf/llnncX06dO5/vrref311xk8eDAAGzduZMCAAQwYMIDZs2ezfft2cnNzad++PRMmTGDbtm0sX76cYcOG1XtMpu5qar30h4u7cVnvI/82k+Miee76vmzIzGfD3nw2Zh7kvR92MKRrS7rTnC/WZzLxzSV0aNmMjknN6NjSefVv16LWZyCswjowBVwigBOrHDxed911F2PHjuWhhx5ixIgRJ7y9nj17EhTkFOiuvvpq/v73v3PDDTfw5z//uaKyGGDSpEmsX78eVWX48OH06tWLRx99lFdffZXQ0FBat27N73//+xOOx5yYY71AaR4Ryvk9WnO+xzxVreh4L6lZOCN7J7Nhbz7z1mXyzuIMAD6+fTBdW4fy8crdfLB8Jx08ksTKHbnc999V9dqizjQNJ0Wnc2vWrKFbt25+isicCPvdNYzcQ8VsyMzjtLZxhIUE8eb323h2/ka27z/E0U4BrWMj+ObuYfb0fhNn3VAbE+Bio0Lpe0qLiulr+6dxbf80CotL2ejeXpo4fanXz+7OLaTX/Z9wattY3rjpTACWZ+QQGhxEu8ToiucmTNNlicCYABYRGkyP5Fh6JMfy2MfrvLaoi40M4dJeyXi2cL1/1moWb92PCCTHRtI+KZoz2ydw29COAGTlH6ZFVJg9Ud1EWCIwxgA1V1jfP/LUI+oI/nT5aazfm8emzINsysxnc9ZB1u/Jq1g+8u//I/tgEekJ0bRPiq5IFIM7JdUpFqu0bliWCIwxwLFVWHdpHUOX1jFet6OqTBjeiY2Z+WzKPMi63Xl8unoPBwpKGNwpiZLSMgY9OpfU+CjaJ0XTLrEZ7ZOi6ZkSS5vYSJ90A2NqZ4nAGFOhPlrUiQij+1ftHLK4tKzixF5QXMrQLi3ZlHWQuWszycp3WjRNuqALtw3tyNTZ3ruBeezjtZYIfMQSgTHG50KDgwgNdpo7x0SEMvWnPSuW5RYUsyXrIIkxzsOUew54f9hyp9s9zNrdB/jnF5tIjY8kpUUUqfFRpLaIpE1sJMFWJ3FcAu7J4irmPVIvmxk6dChz5sypMu+JJ57g1ltvrfEzQ4YMobwZ7MUXX1zRD5CnKVOm8Pjjj9e67xkzZrB69eqK6fvuu4/PPvvsWML3av78+VxyySUnvB1jjiY2MpReqXEV3bzU9JR1q+ZOosjMO8z3m/fx9LwN3PXucq7917ec/eg8Fm3ZB8A3G7OZ/P5y/jFvAzOX7WTJtv1k5R+2J61rEdglgi+mwtDJJ7yZa6+9lunTp3PBBRdUzJs+fTqPPfZYnT7/0UcfHfe+Z8yYwSWXXEL37t0BeOCBB457W8Y0BjVVWk++yHneZHCnJL66exhFJWXsyi1g275DbN9XUFFnkbH/EJ+u3kNWflGV7X5511BSW0QxZ9VuvtmYTUp8JKkeJQpvT1wHSqX1yZkIXvTy5G6PUdD/Jig6BK9fdeS6vX8GfcbAwWx4++dVP3vDh7Xu7sorr+See+6hqKiIsLAwtmzZws6dOxk8eDC33norCxcupKCggCuvvJL777//iM97DjTz8MMP8/LLL9OyZUtSU1Pp27cvAP/617947rnnKCoqomPHjrz66qssXbqUmTNn8sUXX/DQQw/x3nvv8eCDD3LJJZdw5ZVX8vnnn3PnnXdSUlLCGWecwbPPPkt4eDjp6emMHTuWWbNmUVxczDvvvEPXrl3rdGjffPNN/vSnP6GqjBgxgkcffZTS0lJ+8YtfsGjRIkSEG2+8kTvuuIOnnnqKadOmERISQvfu3Zk+fXqd9mECW10rrcNCgjglIZpTEqKrzL+qXypX9Uvl4OESMvYXsH3fIbbvP0Sb2AgA1u/J493FGeQfruzHSwTWPngh4SHBvLNoOxv25pN9sIiZS3dSVHrydw1+ciaC2ix4HLb+r3K6/H1EcycRHIcWLVrQv39/Zs+ezWWXXcb06dO5+uqrEREefvhhWrRoQWlpKcOHD2f58uX07NnT63YWL17M9OnTWbp0KSUlJZx++ukVieCKK67gpptuAuCee+7hhRdeYPz48YwcObLixO+psLCQcePG8fnnn9O5c2d+/vOf8+yzz3L77bcDkJiYyA8//MAzzzzD448/zvPPP3/U77lz505+97vfsXjxYuLj4zn//POZMWMGqamp7Nixg5UrnTGHym9zTZ06lc2bNxMeHu711pcxNamPSuvo8BCvrZt+PawTtw3tSM6hYrbvd0oT2QcPV3TzvWR7Du8uyqhIAJ4Kikv548yVbNibT0p8JCnxUaTER5IcF0lYSNO9035yJoLaruB/cp/zApgSC1Nyqy6PTjhqCcCb8ttD5YnghRdeAODtt9/mueeeo6SkhF27drF69eoaE8GXX37J5ZdfTlSUMxTiyJEjK5atXLmSe+65h5ycHPLz86vchvJm3bp1tGvXjs6dOwMwduxY/vGPf1QkgvIuqfv27VvRffXRLFy4kCFDhlQMwzlmzBgWLFjAvffey6ZNmxg/fjwjRozg/POdHnB69uzJmDFjGDVqFKNGjarTPoxpCCJCfHQY8dFh9EyJq7LsT5efxkOXnUqH33/ktWvw3IISnv1iY5UxJHqlxvHf2wYB8JdP1qFKxa2nlHinIrsxJ4qTMxH4wWWXXcYdd9zBDz/8wKFDh+jbty+bN2/m8ccfZ+HChcTHxzNu3Lgau58+mnHjxjFjxgx69erFSy+9xPz5808o3vLuruujq+v4+HiWLVvGnDlzmDZtGm+//Tb//ve/+fDDD1mwYAGzZs3i4YcfZsWKFTUO2WlMYxIUJLWOXfLFpCHsPlBIxv4CMvYXVOnafv66TFbvOlAlUVzYozXTrndK9/fOWEl8VGhFaSIlPoo2cREVraq88XVdRWD/V557d71tqlmzZgwdOpQbb7yRa6+9FoADBw4QHR1NbGwse/bsYfbs2QwZMqTGbZxzzjmMGzeOyZMnU1JSwqxZs/jVr34FQF5eHm3atKG4uJjXX3+9okvrmJgY8vLyjthWly5d2LJlCxs2bKioUzj33HNP6Dv279+fCRMmkJWVRXx8PG+++Sbjx48nKyuLsLAwfvrTn9KlSxeuu+46ysrK2L59O0OHDuXss89m+vTp5OfnExcXd/QdGdMI1FRpPemCLoQEB7kn8qgjPjdr/NmUlJaxK7c8URyiZXOnfqKopIy5a/eyK7egSpcd485KZ8rIHhwuKWXyeytIaVGeJCJZs+sAf56zjsJi39VVBHYiqIcWQ56uvfZaLr/88opK0V69etGnTx+6du1KamoqgwYNqvXzp59+Otdccw29evWiZcuWnHHGGRXLHnzwQQYMGEBSUhIDBgyoOPmPHj2am266iaeeeop33323Yv2IiAhefPFFrrrqqorK4ltuueWYvs/nn39eZXS0d955h6lTpzJ06NCKyuLLLruMZcuWccMNN1BW5vyhPvLII5SWlnLdddeRm5vrPGk6YYIlAdOknMjYJSHBQU6LpBZRQELF/LCQIL66exjFpWXszi1k+/5DZOwvoENSMwD2HSzi203Z7F66Ay+jl1YoKC7lz3PW1VsisG6ojV/Z786YIxWVOIkiY/8hfvb8d17XEWDz1LqPbVJbN9Q+q70QkVQRmSciq0VklYhM9LLOEBHJFZGl7us+b9syxphAEhYSRFpCFGd1TKx40K66mh68Ox6+rMYuAX6rqt2BM4HbRKS7l/W+VNXe7suehjLGGA8NMc66z+oIVHUXsMt9nycia4C2wOpaP3j8+7MRlJqYpnZb0hh/aIhx1hukslhE0oE+gLebXQNFZBmwE7hTVVd5+fzNwM0AaWlp1RcTERFBdnY2CQkJlgyaCFUlOzubiIgIf4diTKPn63HWfV5ZLCLNgC+Ah1X1/WrLmgNlqpovIhcDT6pqp9q2562yuLi4mIyMjONuo2/8IyIigpSUFEJDj+zjxRhTv/w2ZrGIhALvAa9XTwIAqnrA4/1HIvKMiCSqatax7Cc0NJR27dqdeMDGGBOAfNlqSIAXgDWq+tca1mntroeI9HfjyfZVTMYYY47kyxLBIOB6YIWILHXn/R5IA1DVacCVwK0iUgIUAKPVahCNMaZB+bLV0P9wnnmobZ2ngad9FYMxxpija3JPFotIJrDV33GcoETgmOpBTnJ2PKqy41HJjkVVJ3I8TlHVJG8LmlwiOBmIyKKaau8DkR2Pqux4VLJjUZWvjkfj7SDbGGNMg7BEYIwxAc4SgX885+8AGhk7HlXZ8ahkx6IqnxwPqyMwxpgAZyUCY4wJcJYIjDEmwFkiaEB1Gawn0IhIsIgsEZEP/B2Lv4lInIi8KyJrRWSNiAz0d0z+JCJ3uP8nK0XkTREJqK5qReTfIrJXRFZ6zGshIp+KyHr3Z3x97MsSQcOq62A9gWQisMbfQTQSTwIfq2pXoBcBfFxEpC0wAeinqqcCwcBo/0bV4F4CLqw2727gc7eX5s/d6RNmiaABqeouVf3BfZ+H84/uu07GGzkRSQFGAM/7OxZ/E5FY4BycjhpR1SJVzfFvVH4XAkSKSAgQhTNmScBQ1QXAvmqzLwNedt+/DIyqj31ZIvCTowzWEyieAO4CyvwdSCPQDsgEXnRvlT0vItH+DspfVHUH8DiwDWekw1xV/cS/UTUKrdzRHwF2A63qY6OWCPzAHaznPeB2zzEZAomIXALsVdXF/o6lkQgBTgeeVdU+wEHqqdjfFLn3vi/DSZDJQLSIXOffqBoXt6fmemn/b4mggR1tsJ4AMggYKSJbgOnAMBF5zb8h+VUGkKGq5SXEd3ESQ6D6CbBZVTNVtRh4HzjLzzE1BntEpA2A+3NvfWzUEkEDqstgPYFCVSeraoqqpuNUAs5V1YC94lPV3cB2EenizhoOrPZjSP62DThTRKLc/5vhBHDluYeZwFj3/Vjgv/WxUUsEDat8sJ5hIrLUfV3s76BMozEeeF1ElgO9gT/5OR6/cUtG7wI/ACtwzlUB1d2EiLwJfAN0EZEMEfkFMBU4T0TW45SaptbLvqyLCWOMCWxWIjDGmABnicAYYwKcJQJjjAlwlgiMMSbAWSIwxpgAZ4nAGJeIlHo0610qIvX2ZK+IpHv2ImlMYxLi7wCMaUQKVLW3v4MwpqFZicCYoxCRLSLymIisEJHvRaSjOz9dROaKyHIR+VxE0tz5rUTkPyKyzH2Vd40QLCL/cvvY/0REIt31J7hjVCwXkel++pomgFkiMKZSZLVbQ9d4LMtV1dOAp3F6TQX4O/CyqvYEXgeecuc/BXyhqr1w+gta5c7vBPxDVXsAOcBP3fl3A33c7dziqy9nTE3syWJjXCKSr6rNvMzfAgxT1U1up4G7VTVBRLKANqpa7M7fpaqJIpIJpKjqYY9tpAOfugOKICK/A0JV9SER+RjIB2YAM1Q138df1ZgqrERgTN1oDe+PxWGP96VU1tGNAP6BU3pY6A7EYkyDsURgTN1c4/HzG/f911QOnzgG+NJ9/zlwK1SMyRxb00ZFJAhIVdV5wO+AWOCIUokxvmRXHsZUihSRpR7TH6tqeRPSeLdX0MPAte688Tgjik3CGV3sBnf+ROA5t7fIUpyksAvvgoHX3GQhwFM2RKVpaFZHYMxRuHUE/VQ1y9+xGOMLdmvIGGMCnJUIjDEmwFmJwBhjApwlAmOMCXCWCIwxJsBZIjDGmABnicAYYwLc/wMk1u42r3taiAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3fdjct4b8Pt"
      },
      "source": [
        "## 1c) Generación de texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq_6_M_PcFQ0"
      },
      "source": [
        "# Acá tu código para generar texto usando el modelo\n",
        "from torch.distributions.categorical import Categorical\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def generate_sentence(model, \n",
        "                      init_sentence= None, \n",
        "                      max_len=None):\n",
        "  # Usa acá lo que necesites para crear una secuencia de\n",
        "  # salida. Muy posiblemente tendrás que usar un tokenizador\n",
        "  # y el diccionario para pasar de índices a tokens (palabras).\n",
        "  # model.predict()\n",
        "  if max_len is None:\n",
        "    max_len = random.randint(5,15)\n",
        "  if init_sentence is None:\n",
        "    init_sentence = test_text[random.randint(0, len(test_text))]\n",
        "\n",
        "  sentence = init_sentence\n",
        "  for k in range(max_len):\n",
        "    init_tokens, _ = tokenize_text([sentence], tokenizer)\n",
        "    Data = torch.tensor(encode_sentences(init_tokens, vocab, word2idx)).cuda()\n",
        "    logits_data = model.forward(Data.T).view(-1, voc_size)\n",
        "    logits_index_max = torch.max(F.softmax(logits_data, dim=1), dim=1)[1]\n",
        "    \n",
        "    next_word = vocab[logits_index_max[-2]]\n",
        "    \n",
        "    if next_word == '.':\n",
        "      sentence += next_word\n",
        "      break\n",
        "    else:\n",
        "      sentence += ' ' + next_word \n",
        "  return sentence"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G85XmCnJdPoY"
      },
      "source": [
        "### Para las pruebas, se utilizaran oraciones tanto del `test set` como custom:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUeQ6NAAd4oX"
      },
      "source": [
        "model = net"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AffoPmIDfQZI",
        "outputId": "fc0590fa-34ea-410a-9846-e177a4296162"
      },
      "source": [
        "for i in range(4):\r\n",
        "  oracion_inicial = test_text[random.randint(0, len(test_text))]\r\n",
        "  largo_inicial = random.randint(1, len(oracion_inicial)-1)\r\n",
        "  print(f'Oracion generada: \\n {generate_sentence(model, oracion_inicial)}')"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Oracion generada: \n",
            " The young girl is standing on a skateboard..\n",
            "Oracion generada: \n",
            " A woman grabbing her feet while jumping in front of trees..\n",
            "Oracion generada: \n",
            " A possible woman dressed as a belly dancer holds out a cloth with beads on it..\n",
            "Oracion generada: \n",
            " Two girls play outside with chalk..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxGuruxRbacp",
        "outputId": "f1ad563e-c7ca-48b1-c776-00e4f19106b5"
      },
      "source": [
        "print(generate_sentence(model))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A black puppy is playing with an orange on a carpeted floor..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxr7W07Rb_Zx"
      },
      "source": [
        "## 1d) Opcional: Beam Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELk5bZgycCFz"
      },
      "source": [
        "# Acá tu código para generar texto usando beam search\n",
        "\n",
        "def beam_search_generation(model, init_sentence, K, ...):\n",
        "  # El K representa al ancho del beam para la búsqueda.\n",
        "  return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mw8PKh-uF7P"
      },
      "source": [
        "# Parte 2 (Opcional): Subtitulado de imágenes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-jqTnsbSjpZ"
      },
      "source": [
        "#### Generamos transformación para el dataset\n",
        "\n",
        "Algo importante es que estamos usando la normalización estándar para los modelos pre-entrenados que provee pytoch. Si vas a usar algún otro modelo (o incluso uno generado por ti), podrías necesitar otra normalización. También nota que estamos usando el tamaño estándar de `224x224` para las imágenes que reciben los modelos pre-entrenados de pytorch. Si no quieres usar esos modelos o si quieres hacer el entrenamiento más rápido, puedes cambiarle la resolución a las imágenes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97n_3naFlAO9"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "            [\n",
        "              transforms.ToTensor(), \n",
        "              transforms.Resize((224, 224)),\n",
        "              transforms.Normalize(\n",
        "                  mean=[0.485, 0.456, 0.406], \n",
        "                  std=[0.229, 0.224, 0.225])\n",
        "            ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_HkFYB-sQ0t"
      },
      "source": [
        "#### Creamos los data loaders (puedes cambiar el tamaño del batch si lo deseas)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1djUvkSpmlT"
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    CaptioningDataset(\n",
        "        train_flickr_set, transform, tokenizer, word2idx, \"<sos>\", \".\"),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda x: pad_sequence_with_images(x, pad_idx)\n",
        "    )\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    CaptioningDataset(\n",
        "        test_flickr_set, transform, tokenizer, word2idx, \"<sos>\", \".\"),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=lambda x: pad_sequence_with_images(x, pad_idx)\n",
        "    )\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    CaptioningDataset(\n",
        "        val_flickr_set, transform, tokenizer, word2idx, \"<sos>\", \".\"),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=lambda x: pad_sequence_with_images(x, pad_idx)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHH-FpoZsVEE"
      },
      "source": [
        "**IMPORTANTE**: Nuestros dataloaders ahora contienen las secuencias de identificadores de los tokens del texto, los largos de las secuencias y las imágenes correspondientes. El siguiente código obtiene el primer elemento del dataloader de prueba. Nota que lo que entregan en ambos casos es una tripleta: la primera componente tiene los datos desde los textos (los índices), la segunda componente tiene información de los largos de las secuencias, y la tercera componente la información de las imágenes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTQQhZZWqPWF"
      },
      "source": [
        "# Obtiene un paquete desde el dataloader\n",
        "for data in test_dataloader:\n",
        "  Text, Lengths, Img = data\n",
        "  break\n",
        "\n",
        "print(Text.size())\n",
        "print(Lengths.size())\n",
        "print(Img.size())\n",
        "\n",
        "# La primera dimensión de Text corresponde al largo\n",
        "# máximo de las secuencias en el batch\n",
        "assert Text.size()[0] == torch.max(Lengths)\n",
        "\n",
        "# La segunda dimensión de D corresponde al tamaño del\n",
        "# batch, al igual que la dimensión de Lengths y la primera\n",
        "# dimensión de Img\n",
        "assert Text.size()[1] == batch_size \n",
        "assert Lengths.size()[0] == batch_size\n",
        "assert Img.size()[0] == batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-CnRQBOTQjw"
      },
      "source": [
        "### Usando modelos pre-entrenados\n",
        "\n",
        "El siguiente código carga VGG16 (pre-entrenado), pasa el modelo a la GPU. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2i2IL4-s0PJ"
      },
      "source": [
        "import torchvision.models as models\n",
        "vgg16 = models.vgg16(pretrained=True)\n",
        "vgg16 = vgg16.to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_aVkA4tTr9H"
      },
      "source": [
        "Con un codigo como el siguiente podemos calcular las características para las imágenes `Img` del batch que obtuvimos más arriba. Nota el uso de `.eval()` y `with torch.no_grad()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StImMpdss0F8"
      },
      "source": [
        "Img = Img.to('cuda')\n",
        "\n",
        "vgg16.eval()\n",
        "with torch.no_grad():\n",
        "  F = vgg16.features(Img)\n",
        "\n",
        "print(F.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqi-CEqkuVM-"
      },
      "source": [
        "Finalmente y por si lo necesitas, puedes acceder a las imágenes originales del dataloader haciendo algo como esto:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_y725BJs0UK"
      },
      "source": [
        "val_dataloader.dataset.original_image(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtaST3BqXOks"
      },
      "source": [
        "## 2a) Red convolucional + recurrente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTcuwtNEs0D1"
      },
      "source": [
        "class CaptioningModel(torch.nn.Module):\n",
        "    def __init__(self, ...): \n",
        "        # Crea las capas considerando una parte que procese debe procesar\n",
        "        # la imagen de entrada y otra que debe producir el texto (índices)\n",
        "        # de salida.\n",
        "        pass\n",
        "        \n",
        "    def forward(self, ...):\n",
        "        # Acá debes programar la pasada hacia adelante.\n",
        "        # Debes decidir qué le pasarás a la red y cómo haras la \n",
        "        # computación hacia adelante. Considera que no solo\n",
        "        # debes entrenar los parámetros sino que además debes\n",
        "        # después ser capaz de generar una secuencia de salida\n",
        "        # desde una imagen de entrada.\n",
        "        return ...   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5ifPKElXPk_"
      },
      "source": [
        "## 2b) Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xb6B5CFs0AU"
      },
      "source": [
        "# Acá tu código para el loop de entrenamiento\n",
        "# y los gráficos de la pérdida"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYtj8aw4XQSX"
      },
      "source": [
        "## 2c) Generando texto desde imágenes de prueba\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYQ_LUEmsz97"
      },
      "source": [
        "# Acá tu código para generar texto usando desde imágenes\n",
        "# y un par de ejemplos con las imágenes del conjunto de prueba"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}