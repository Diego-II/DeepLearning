{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Respuestas_Tarea_5_CC6204_2020 [PUBLICADA]",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zhjpqvcdo5o"
      },
      "source": [
        "# Tarea 5: Redes Recurrentes <br/> CC6204 Deep Learning, Universidad de Chile <br/> Hoja de Respuestas\n",
        "\n",
        "## Nombre: \n",
        "Fecha de entrega: 30 de diciembre de 2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64BkmYga3UN_"
      },
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from collections import Counter\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import get_tokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Aqui descargamos algunas funciones utiles para resolver la tarea\n",
        "if not os.path.exists('utils.py'):\n",
        "    !wget https://raw.githubusercontent.com/dccuchile/CC6204/master/2020/tareas/tarea5/utils.py -q --show-progress"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS9LeqZRJ5zn"
      },
      "source": [
        "from utils import extract_text_from_set, extract_text_from_set, tokenize_text \n",
        "from utils import encode_sentences, pad_sequence_with_lengths, pad_sequence_with_images\n",
        "from utils import TextDataset, CaptioningDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNoC8iTiKtg0"
      },
      "source": [
        "# Parte 1: Generación de texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnPZTm8HMaNn"
      },
      "source": [
        "### Datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTiSVBGoK0Xo"
      },
      "source": [
        "##############################################################################\n",
        "# Todo este código sirve para descargar, preprocesar y dejar los datos\n",
        "# listos para usar después. Después de ejecutar las dos celdas siguientes\n",
        "# tendrás los datos en train_flickr_tripletset y similar para val y test\n",
        "##############################################################################\n",
        "\n",
        "folder_path = './data/flickr8k'\n",
        "if not os.path.exists(f'{folder_path}/images'):\n",
        "    print('*** Descargando y extrayendo Flickr8k, siéntese y relájese 4 mins...')\n",
        "    print('****** Descargando las imágenes...')\n",
        "    !wget https://s06.imfd.cl/04/CC6204/tareas/tarea4/Flickr8k_Dataset.zip -P $folder_path/images -q --show-progress \n",
        "    print('********* Extrayendo las imágenes...\\n  Si te sale mensaje de colab, dale Ignorar\\n')\n",
        "    !unzip -q $folder_path/images/Flickr8k_Dataset.zip -d $folder_path/images\n",
        "    print('*** Descargando anotaciones de las imágenes...')\n",
        "    !wget http://hockenmaier.cs.illinois.edu/8k-pictures.html -P $folder_path/annotations -q --show-progress\n",
        "\n",
        "print('Inicializando pytorch Flickr8k dataset')\n",
        "full_flickr_set = torchvision.datasets.Flickr8k(root=f'{folder_path}/images/Flicker8k_Dataset', ann_file = f'{folder_path}/annotations/8k-pictures.html')\n",
        "\n",
        "print('Creando train, val y test splits...')\n",
        "train_flickr_set, val_flickr_set, test_flickr_set = [], [], []\n",
        "for i, item in enumerate(full_flickr_set):\n",
        "  if i<6000:\n",
        "    train_flickr_set.append(item)\n",
        "  elif i<7000:\n",
        "    val_flickr_set.append(item)\n",
        "  else:\n",
        "    test_flickr_set.append(item)\n",
        "\n",
        "print('Listo!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDKp92H8OnZB"
      },
      "source": [
        "#### Extrae los textos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2q2_dNcMda2"
      },
      "source": [
        "train_text = extract_text_from_set(train_flickr_set)\n",
        "val_text = extract_text_from_set(val_flickr_set)\n",
        "test_text = extract_text_from_set(test_flickr_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5HI9-O0OqIT"
      },
      "source": [
        "#### Genera los tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFt8moUHNtII"
      },
      "source": [
        "tokenizer = get_tokenizer('spacy')\n",
        "counter = Counter()  # para llevar la cuenta de los tokens y su ocurrencia\n",
        "\n",
        "train_tokens, counter = tokenize_text(train_text, tokenizer, counter)\n",
        "test_tokens, counter = tokenize_text(test_text, tokenizer, counter)\n",
        "val_tokens, counter = tokenize_text(val_text, tokenizer, counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hatUPnGOwdi"
      },
      "source": [
        "#### Define el vocabulario y agrega `<pad>` y `<sos>`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTJ-7AXmOSOw"
      },
      "source": [
        "vocab = list(counter.keys())\n",
        "vocab.append('<pad>')\n",
        "vocab.append('<sos>')\n",
        "word2idx = {word: i for i, word in enumerate(vocab)}\n",
        "pad_idx = word2idx['<pad>']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WikFbU1LPA_G"
      },
      "source": [
        "#### Convierte oraciones a ids y genera los dataset de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDcBWuesOhrJ"
      },
      "source": [
        "train_sentences = encode_sentences(train_tokens, vocab, word2idx)\n",
        "test_sentences = encode_sentences(test_tokens, vocab, word2idx)\n",
        "val_sentences = encode_sentences(val_tokens, vocab, word2idx)\n",
        "\n",
        "train_dataset = TextDataset(train_sentences)\n",
        "test_dataset = TextDataset(test_sentences)\n",
        "val_dataset = TextDataset(val_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSvpUdxURgLR"
      },
      "source": [
        "Con todo lo anterior, además de tener los dataset para entrenamiento, podemos también obtener identificadores correspondientes a textos que nosotros decidamos, haciendo algo como lo siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCml7LsLPpNl"
      },
      "source": [
        "s1 = 'A woman holding a cup of tea.'\n",
        "s2 = 'A man with a dog.'\n",
        "\n",
        "S = [s1,s2]\n",
        "tokens, _ = tokenize_text(S, tokenizer)\n",
        "D = encode_sentences(tokens, vocab, word2idx)\n",
        "\n",
        "print('tokens:', tokens)\n",
        "print('ids:', D)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8oMGt5CpSNQ"
      },
      "source": [
        "#### Creamos los data loaders (puedes cambiar el tamaño del batch si lo deseas)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LS1ADBtQscp"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, \n",
        "    collate_fn=lambda data_list: pad_sequence_with_lengths(data_list, pad_idx))\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False, \n",
        "    collate_fn=lambda data_list: pad_sequence_with_lengths(data_list, pad_idx))\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False, \n",
        "    collate_fn=lambda data_list: pad_sequence_with_lengths(data_list, pad_idx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mipSa7R1jAkd"
      },
      "source": [
        "**IMPORTANTE**: Nuestros datasets y dataloaders consideran también los largos de las secuencias. El siguiente código obtiene el primer elemento del dataset y el primer elemento del dataloader de prueba. Nota que lo que entregan en ambos casos es un par: la primera componente del par tiene los datos (los índices) mientras que la segunda componente tiene información de los largos de las secuencias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC-g5Ceyichj"
      },
      "source": [
        "d, length = test_dataset[0]\n",
        "print('len(d):', len(d))\n",
        "print('length:', length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e31bJzWwQsah"
      },
      "source": [
        "# Obtiene un paquete desde el dataloader\n",
        "for data in test_dataloader:\n",
        "  D, Lengths = data\n",
        "  break\n",
        "\n",
        "print(D.size())\n",
        "print(Lengths.size())\n",
        "\n",
        "# La primera dimensión de D corresponde al largo\n",
        "# máximo de las secuencias en el batch\n",
        "assert D.size()[0] == torch.max(Lengths)\n",
        "\n",
        "# La segunda dimensión de D corresponde al tamaño del\n",
        "# batch, al igual que la dimensión de Lengths\n",
        "assert D.size()[1] == batch_size \n",
        "assert Lengths.size()[0] == batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFRqoHLxrzYR"
      },
      "source": [
        "## 1a) Red recurrente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZswYBYer0kz"
      },
      "source": [
        "# Acá empieza tu código\n",
        "\n",
        "class RedRecurrente(torch.nn.Module):\n",
        "    def __init__(self, ...): # Piensa en todo lo que necesitas para incializar.\n",
        "        # Crea las capas considerando al menos los puntos de arriba.\n",
        "        pass    \n",
        "\n",
        "    def forward(self, x, h_0=None):\n",
        "        # Acá debes programar la pasada hacia adelante.\n",
        "        # El vector h_0 deberías simplemente pasarlo directo\n",
        "        # a tu red recurrente (RNN, o GRU, o LSTM) y será necesario\n",
        "        # para trabajar en la sección (1c) y en la parte 2. \n",
        "        # También puedes usar dropout, batch normalization o lo que necesites.\n",
        "        logits = None\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTKju5aPsA6H"
      },
      "source": [
        "## 1b) Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0YB5e-NLQZC"
      },
      "source": [
        "# Acá tu código para el loop de entrenamiento\n",
        "# y los gráficos de la pérdida"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3fdjct4b8Pt"
      },
      "source": [
        "## 1c) Generación de texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq_6_M_PcFQ0"
      },
      "source": [
        "# Acá tu código para generar texto usando el modelo\n",
        "\n",
        "def generate_sentence(model, init_sentence, ...):\n",
        "  # Usa acá lo que necesites para crear una secuencia de\n",
        "  # salida. Muy posiblemente tendrás que usar un tokenizador\n",
        "  # y el diccionario para pasar de índices a tokens (palabras).\n",
        "  sentence = None\n",
        "  return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxr7W07Rb_Zx"
      },
      "source": [
        "## 1d) Opcional: Beam Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELk5bZgycCFz"
      },
      "source": [
        "# Acá tu código para generar texto usando beam search\n",
        "\n",
        "def beam_search_generation(model, init_sentence, K, ...):\n",
        "  # El K representa al ancho del beam para la búsqueda.\n",
        "  return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mw8PKh-uF7P"
      },
      "source": [
        "# Parte 2 (Opcional): Subtitulado de imágenes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-jqTnsbSjpZ"
      },
      "source": [
        "#### Generamos transformación para el dataset\n",
        "\n",
        "Algo importante es que estamos usando la normalización estándar para los modelos pre-entrenados que provee pytoch. Si vas a usar algún otro modelo (o incluso uno generado por ti), podrías necesitar otra normalización. También nota que estamos usando el tamaño estándar de `224x224` para las imágenes que reciben los modelos pre-entrenados de pytorch. Si no quieres usar esos modelos o si quieres hacer el entrenamiento más rápido, puedes cambiarle la resolución a las imágenes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97n_3naFlAO9"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "            [\n",
        "              transforms.ToTensor(), \n",
        "              transforms.Resize((224, 224)),\n",
        "              transforms.Normalize(\n",
        "                  mean=[0.485, 0.456, 0.406], \n",
        "                  std=[0.229, 0.224, 0.225])\n",
        "            ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_HkFYB-sQ0t"
      },
      "source": [
        "#### Creamos los data loaders (puedes cambiar el tamaño del batch si lo deseas)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1djUvkSpmlT"
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    CaptioningDataset(\n",
        "        train_flickr_set, transform, tokenizer, word2idx, \"<sos>\", \".\"),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda x: pad_sequence_with_images(x, pad_idx)\n",
        "    )\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    CaptioningDataset(\n",
        "        test_flickr_set, transform, tokenizer, word2idx, \"<sos>\", \".\"),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=lambda x: pad_sequence_with_images(x, pad_idx)\n",
        "    )\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    CaptioningDataset(\n",
        "        val_flickr_set, transform, tokenizer, word2idx, \"<sos>\", \".\"),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=lambda x: pad_sequence_with_images(x, pad_idx)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHH-FpoZsVEE"
      },
      "source": [
        "**IMPORTANTE**: Nuestros dataloaders ahora contienen las secuencias de identificadores de los tokens del texto, los largos de las secuencias y las imágenes correspondientes. El siguiente código obtiene el primer elemento del dataloader de prueba. Nota que lo que entregan en ambos casos es una tripleta: la primera componente tiene los datos desde los textos (los índices), la segunda componente tiene información de los largos de las secuencias, y la tercera componente la información de las imágenes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTQQhZZWqPWF"
      },
      "source": [
        "# Obtiene un paquete desde el dataloader\n",
        "for data in test_dataloader:\n",
        "  Text, Lengths, Img = data\n",
        "  break\n",
        "\n",
        "print(Text.size())\n",
        "print(Lengths.size())\n",
        "print(Img.size())\n",
        "\n",
        "# La primera dimensión de Text corresponde al largo\n",
        "# máximo de las secuencias en el batch\n",
        "assert Text.size()[0] == torch.max(Lengths)\n",
        "\n",
        "# La segunda dimensión de D corresponde al tamaño del\n",
        "# batch, al igual que la dimensión de Lengths y la primera\n",
        "# dimensión de Img\n",
        "assert Text.size()[1] == batch_size \n",
        "assert Lengths.size()[0] == batch_size\n",
        "assert Img.size()[0] == batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-CnRQBOTQjw"
      },
      "source": [
        "### Usando modelos pre-entrenados\n",
        "\n",
        "El siguiente código carga VGG16 (pre-entrenado), pasa el modelo a la GPU. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2i2IL4-s0PJ"
      },
      "source": [
        "import torchvision.models as models\n",
        "vgg16 = models.vgg16(pretrained=True)\n",
        "vgg16 = vgg16.to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_aVkA4tTr9H"
      },
      "source": [
        "Con un codigo como el siguiente podemos calcular las características para las imágenes `Img` del batch que obtuvimos más arriba. Nota el uso de `.eval()` y `with torch.no_grad()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StImMpdss0F8"
      },
      "source": [
        "Img = Img.to('cuda')\n",
        "\n",
        "vgg16.eval()\n",
        "with torch.no_grad():\n",
        "  F = vgg16.features(Img)\n",
        "\n",
        "print(F.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqi-CEqkuVM-"
      },
      "source": [
        "Finalmente y por si lo necesitas, puedes acceder a las imágenes originales del dataloader haciendo algo como esto:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_y725BJs0UK"
      },
      "source": [
        "val_dataloader.dataset.original_image(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtaST3BqXOks"
      },
      "source": [
        "## 2a) Red convolucional + recurrente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTcuwtNEs0D1"
      },
      "source": [
        "class CaptioningModel(torch.nn.Module):\n",
        "    def __init__(self, ...): \n",
        "        # Crea las capas considerando una parte que procese debe procesar\n",
        "        # la imagen de entrada y otra que debe producir el texto (índices)\n",
        "        # de salida.\n",
        "        pass\n",
        "        \n",
        "    def forward(self, ...):\n",
        "        # Acá debes programar la pasada hacia adelante.\n",
        "        # Debes decidir qué le pasarás a la red y cómo haras la \n",
        "        # computación hacia adelante. Considera que no solo\n",
        "        # debes entrenar los parámetros sino que además debes\n",
        "        # después ser capaz de generar una secuencia de salida\n",
        "        # desde una imagen de entrada.\n",
        "        return ...   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5ifPKElXPk_"
      },
      "source": [
        "## 2b) Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xb6B5CFs0AU"
      },
      "source": [
        "# Acá tu código para el loop de entrenamiento\n",
        "# y los gráficos de la pérdida"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYtj8aw4XQSX"
      },
      "source": [
        "## 2c) Generando texto desde imágenes de prueba\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYQ_LUEmsz97"
      },
      "source": [
        "# Acá tu código para generar texto usando desde imágenes\n",
        "# y un par de ejemplos con las imágenes del conjunto de prueba"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}