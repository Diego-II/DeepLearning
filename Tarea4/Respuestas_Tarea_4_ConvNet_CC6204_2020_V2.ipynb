{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Respuestas_Tarea_4_ConvNet_CC6204_2020",
      "provenance": [],
      "collapsed_sections": [
        "sJG69heDRag8",
        "tE6uDwmJ94-W",
        "kmALm7EtpFow"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fa3ca844b289457fa56c862e34e7d988": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6fee87b018a446c9bbb34a47cced253e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c6c3c35b26d544559ee704127b18db07",
              "IPY_MODEL_9e7e145a531e41738f5b352e589bd878"
            ]
          }
        },
        "6fee87b018a446c9bbb34a47cced253e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c6c3c35b26d544559ee704127b18db07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_38cca41e746446fbbc78ee4804c190f9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_832cbd9df69b49a8a5e5181c377ac746"
          }
        },
        "9e7e145a531e41738f5b352e589bd878": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_36f88d05f0d946fd94fbf50f5b9704f0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 100262197.47it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_db1d469cbf334002940c6dcb8ed90a5d"
          }
        },
        "38cca41e746446fbbc78ee4804c190f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "832cbd9df69b49a8a5e5181c377ac746": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "36f88d05f0d946fd94fbf50f5b9704f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "db1d469cbf334002940c6dcb8ed90a5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zhjpqvcdo5o"
      },
      "source": [
        "# Tarea 4: Redes Convolucionales <br/> CC6204 Deep Learning, Universidad de Chile <br/> Hoja de Respuestas\n",
        "\n",
        "## Nombre: \n",
        "Fecha de entrega: 11 de diciembre de 2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64BkmYga3UN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2a579e-5d3e-4037-8bee-43f3ffd6d645"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Aqui descargamos algunas funciones utiles para resolver la tarea\n",
        "if not os.path.exists('utils.py'):\n",
        "  !wget https://raw.githubusercontent.com/dccuchile/CC6204/master/2020/tareas/tarea4/utils.py\n",
        "\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "from utils import ImageCaptionDataset, train_for_classification, train_for_retrieval\n",
        "\n",
        "!pip install ipdb\n",
        "import ipdb"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-13 20:07:47--  https://raw.githubusercontent.com/dccuchile/CC6204/master/2020/tareas/tarea4/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7403 (7.2K) [text/plain]\n",
            "Saving to: â€˜utils.pyâ€™\n",
            "\n",
            "\rutils.py              0%[                    ]       0  --.-KB/s               \rutils.py            100%[===================>]   7.23K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-12-13 20:07:47 (105 MB/s) - â€˜utils.pyâ€™ saved [7403/7403]\n",
            "\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "Collecting ipdb\n",
            "  Downloading https://files.pythonhosted.org/packages/44/8c/76b33b115f4f2c090e2809a0247fe777eb3832f9d606479bf0139b29ca2c/ipdb-0.13.4.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from ipdb) (50.3.2)\n",
            "Requirement already satisfied: ipython>=5.1.0 in /usr/local/lib/python3.6/dist-packages (from ipdb) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.3.3)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (2.6.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.4.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=5.1.0->ipdb) (0.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=5.1.0->ipdb) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.1.0->ipdb) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.1.0->ipdb) (0.6.0)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipdb: filename=ipdb-0.13.4-cp36-none-any.whl size=10971 sha256=40a4b0bb51169b324cdd2c41cacd6dad6cf6069f8c058fc58ccec27efaf14d14\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/51/e4/c91c61e3481a1a967beb18c4ea7a2b138a63cce94170b2e206\n",
            "Successfully built ipdb\n",
            "Installing collected packages: ipdb\n",
            "Successfully installed ipdb-0.13.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtwf7_btPh7n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c480f618-2b83-4651-b2dc-092a38434009"
      },
      "source": [
        "# Aqui instalamos la libreria de correccion del curso\n",
        "!pip install -U \"git+https://github.com/dccuchile/CC6204.git@master#egg=cc6204&subdirectory=autocorrect\"\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "# importamos las herramientas del curso\n",
        "from cc6204 import AutoCorrect, FailedTest\n",
        "\n",
        "# En caso que se les indique, cambia el host y port que posteamos en u-cursos\n",
        "corrector = AutoCorrect(host=\"cc6204.dcc.uchile.cl\", port=443)\n",
        "\n",
        "# En caso que se les indique, cambia el token que te daremos en u-cursos\n",
        "token = \"]ye/Ox;nsz\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cc6204\n",
            "  Cloning https://github.com/dccuchile/CC6204.git (to revision master) to /tmp/pip-install-l6kd7a4g/cc6204\n",
            "  Running command git clone -q https://github.com/dccuchile/CC6204.git /tmp/pip-install-l6kd7a4g/cc6204\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from cc6204) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from cc6204) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from cc6204) (1.7.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->cc6204) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->cc6204) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->cc6204) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->cc6204) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->cc6204) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->cc6204) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch->cc6204) (0.16.0)\n",
            "Building wheels for collected packages: cc6204\n",
            "  Building wheel for cc6204 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cc6204: filename=cc6204-0.5.0-cp36-none-any.whl size=5800 sha256=d0b0241d13088c16a6a56e6294c7a5cff8ce1725b77f847c0dc815e0afb17ca8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-194jp4ms/wheels/62/f0/30/aadcb7ce24a2f9c935890518e902d4e23bf97b80f47bb64414\n",
            "Successfully built cc6204\n",
            "Installing collected packages: cc6204\n",
            "Successfully installed cc6204-0.5.0\n",
            "Connection stablished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnux9hNPSVYv"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_results(loss, score1, score1_title='Accuracy', score2=None, score2_title=None):\n",
        "  f1 = plt.figure(1)\n",
        "  ax1 = f1.add_subplot(111)\n",
        "  ax1.set_title(\"Loss\")    \n",
        "  ax1.set_xlabel('epochs')\n",
        "  ax1.set_ylabel('loss')\n",
        "  ax1.plot(loss, c='r')\n",
        "  ax1.legend(['train-loss'])\n",
        "  f1.show()\n",
        "\n",
        "  f2 = plt.figure(2)\n",
        "  ax2 = f2.add_subplot(111)\n",
        "  ax2.set_title(score1_title)    \n",
        "  ax2.set_xlabel('epochs')\n",
        "  ax2.set_ylabel(score1_title.lower())\n",
        "  ax2.plot(score1[0], c='b')\n",
        "  ax2.plot(score1[1], c='g')\n",
        "  ax2.legend([f'train-{score1_title.lower()}', f'val-{score1_title.lower()}'])\n",
        "  f2.show()\n",
        "\n",
        "  if score2:\n",
        "    f3= plt.figure(3)\n",
        "    ax3 = f3.add_subplot(111)\n",
        "    ax3.set_title(score2_title)    \n",
        "    ax3.set_xlabel('epochs')\n",
        "    ax3.set_ylabel(score2_title.lower())\n",
        "    ax3.plot(score2[0], c='b')\n",
        "    ax3.plot(score2[1], c='g')\n",
        "    ax3.legend([f'train-{score2_title.lower()}', f'val-{score2_title.lower()}'])\n",
        "    f3.show()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mw8PKh-uF7P"
      },
      "source": [
        "# Parte 1: Arquitectura Convolucional GoogLeNet (y otras) para CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9qzrGbKRRxh"
      },
      "source": [
        "## 1a) Inception Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVhRKwtJ55fg"
      },
      "source": [
        "# Como todas las capas convolucionales del paper usan ReLu, se va a hacer una clase general para facilitar esto:\r\n",
        "class Conv2dRelu(nn.Module):\r\n",
        "  def __init__(\r\n",
        "      self, \r\n",
        "      in_channels,\r\n",
        "      out_channels, \r\n",
        "      **kwargs\r\n",
        "  ):\r\n",
        "    super(Conv2dRelu, self).__init__()\r\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    x = self.conv(x)\r\n",
        "    return F.relu(x)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVeNZDV8RlDr"
      },
      "source": [
        "class InceptionModule(nn.Module):\n",
        "  def __init__(self, \n",
        "               in_channels, \n",
        "               ch_3x3_reduce=96, \n",
        "               ch_5x5_reduce=16,\n",
        "               ch_3x3=128,\n",
        "               ch_5x5=32,\n",
        "               ch_pool_proj=32,\n",
        "               ch_1x1=64\n",
        "    ):\n",
        "    super(InceptionModule, self).__init__()\n",
        "    # AcÃ¡ inicializa todos los parÃ¡metros\n",
        "    # Rama 1: Conv2d 1x1 -> Conv2d 3x3\n",
        "    self.branch1 = nn.Sequential(\n",
        "        Conv2dRelu(in_channels, ch_3x3_reduce, kernel_size=1),\n",
        "        Conv2dRelu(ch_3x3_reduce, ch_3x3, kernel_size=3, padding=1)\n",
        "    )\n",
        "    # Rama 2: Conv2dRelu 1x1 -> Conv2dRelu 5x5\n",
        "    self.branch2 = nn.Sequential(\n",
        "        Conv2dRelu(in_channels, ch_5x5_reduce, kernel_size=1),\n",
        "        Conv2dRelu(ch_5x5_reduce, ch_5x5, kernel_size=5, padding=2)\n",
        "    )\n",
        "\n",
        "    # Rama 3: Max_Pool -> Conv2dRelu 1x1\n",
        "    self.branch3 = nn.Sequential(\n",
        "        nn.MaxPool2d(kernel_size=3, stride=1, padding=1, ceil_mode=True),\n",
        "        Conv2dRelu(in_channels, ch_pool_proj, kernel_size=1)\n",
        "    )\n",
        "\n",
        "    # Rama 4: Conv2dRelu 1x1\n",
        "    self.branch4 = Conv2dRelu(in_channels, ch_1x1, kernel_size=1)\n",
        "        \n",
        "\n",
        "  def _forward(self, x):\n",
        "    # aux para forward:\n",
        "    branch1 = self.branch1(x)\n",
        "    branch2 = self.branch2(x)\n",
        "    branch3 = self.branch3(x)\n",
        "    branch4 = self.branch4(x)\n",
        "\n",
        "    outputs = [branch1, branch2, branch3, branch4]\n",
        "    return outputs\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Calcula la salida como un tensor con cantidad de canales de\n",
        "    # salida dado por ch_3x3 + ch_5x5 + ch_pool_proj + ch_1x1\n",
        "    outputs = self._forward(x)\n",
        "    return torch.cat(outputs, 1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-net4tfvdce"
      },
      "source": [
        "Esta celda se utiliza para chequear que los tamanos del output sean los correctos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sstah0kkjiUY",
        "outputId": "c1bf92c4-b2d9-4799-e2c3-c9e67e5f6fe7"
      },
      "source": [
        "# Obtengamos algunos parametros para probar tu implementaciÃ³n\r\n",
        "x, in_chs, ch_1x1, ch_3x3_red, ch_3x3, ch_5x5_red, ch_5x5, ch_pool_proj = corrector.get_test_data(homework=4, question=\"1a\", test=1, token=token)\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "  model = InceptionModule(in_chs, ch_1x1, ch_3x3_red, ch_3x3, ch_5x5_red, ch_5x5, ch_pool_proj)\r\n",
        "  s = timer()\r\n",
        "  result = model._forward(torch.tensor(x))\r\n",
        "  for r in result:\r\n",
        "    print(r.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 128, 28, 28])\n",
            "torch.Size([5, 16, 28, 28])\n",
            "torch.Size([5, 32, 28, 28])\n",
            "torch.Size([5, 32, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5AxECptzqin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b4aa08c-9524-40c3-d057-152400ad9e99"
      },
      "source": [
        "# Tests del API del curso para el InceptionModule\n",
        "\n",
        "# Obtengamos algunos parametros para probar tu implementaciÃ³n\n",
        "x, in_chs, ch_1x1, ch_3x3_red, ch_3x3, ch_5x5_red, ch_5x5, ch_pool_proj = corrector.get_test_data(homework=4, question=\"1a\", test=1, token=token)\n",
        "\n",
        "# Corramos tu implementaciÃ³n de InseptionModule para ver como se comporta\n",
        "with torch.no_grad():\n",
        "  model = InceptionModule(in_chs, ch_3x3_red, ch_5x5_red, ch_3x3, ch_5x5, ch_pool_proj, ch_1x1)\n",
        "  s = timer()\n",
        "  result = model(torch.tensor(x))\n",
        "  t = timer()-s\n",
        "\n",
        "# Veamos si todo fue OK :)\n",
        "corrector.submit(homework=4, question=\"1a\", test=1, token=token, answer=list(result.size()), time=t)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cached test data\n",
            "Correct Test!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJG69heDRag8"
      },
      "source": [
        "## 1b) GoogLeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz6dtn7BMMPS"
      },
      "source": [
        "# Modulo auxiliar de prediccion intermedia:\r\n",
        "class InceptionAux(nn.Module):\r\n",
        "  def __init__(\r\n",
        "      self, \r\n",
        "      in_channels, \r\n",
        "      num_classes\r\n",
        "  ):\r\n",
        "    super(InceptionAux,self).__init__()\r\n",
        "    #self.avgpool = nn.AvgPool2d(kernel_size= 5,stride=3)\r\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\r\n",
        "\r\n",
        "    self.conv = Conv2dRelu(in_channels, 128, kernel_size=1)\r\n",
        "\r\n",
        "    self.fc1 = nn.Linear(128,1024)\r\n",
        "    self.fc2 = nn.Linear(1024, num_classes)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    x = self.avgpool(x)\r\n",
        "    x = self.conv(x)\r\n",
        "    x = torch.flatten(x,1)\r\n",
        "    x = self.fc1(x)\r\n",
        "    x = F.relu(x, inplace=True)\r\n",
        "    x = F.dropout(x, 0.7, training=self.training)\r\n",
        "    x = self.fc2(x)\r\n",
        "\r\n",
        "    return x\r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd3iSAd7oXKN"
      },
      "source": [
        "# basado en la documentacion de Pytorch/Vision/GoogLeNet. \n",
        "class GoogLeNet(nn.Module):\n",
        "  def __init__(self, n_classes, use_aux_logits=True):\n",
        "    super(GoogLeNet, self).__init__()\n",
        "    self.use_aux_logits = use_aux_logits\n",
        "    # Define las capas de convoluciÃ³n y pooling de GoogLeNet\n",
        "    # Conv 1\n",
        "    self.conv1 = Conv2dRelu(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3)\n",
        "    # Max Pool 1\n",
        "    self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n",
        "    \n",
        "    # Conv 2\n",
        "    self.conv2 = Conv2dRelu(in_channels=64, out_channels=192, kernel_size=3, stride=1, padding=1)\n",
        "    # Max Pool 2\n",
        "    self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n",
        "\n",
        "    # in_channels, ch_3x3_reduce=96, ch_5x5_reduce=16,\n",
        "    # ch_3x3=128, ch_5x5=32, ch_pool_proj=32, ch_1x1=64\n",
        "\n",
        "    # Inception 3a\n",
        "    self.inception3a = InceptionModule(in_channels=192, ch_3x3_reduce=96, ch_5x5_reduce=16, ch_3x3=128, ch_5x5=32, ch_pool_proj=32, ch_1x1=64)\n",
        "    # Inception 3b\n",
        "    self.inception3b = InceptionModule(in_channels=256, ch_3x3_reduce=128, ch_5x5_reduce=32, ch_3x3=192, ch_5x5=96, ch_pool_proj=64, ch_1x1=128)\n",
        "\n",
        "    # Max Pool 3\n",
        "    self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n",
        "\n",
        "    # Inception 4a\n",
        "    self.inception4a = InceptionModule(in_channels=480, ch_3x3_reduce=96, ch_5x5_reduce=16, ch_3x3=208, ch_5x5=48, ch_pool_proj=64, ch_1x1=192)\n",
        "    # Inception 4b\n",
        "    self.inception4b = InceptionModule(in_channels=512, ch_3x3_reduce=112, ch_5x5_reduce=24, ch_3x3=224, ch_5x5=64, ch_pool_proj=64, ch_1x1=160)\n",
        "    # Inception 4c\n",
        "    self.inception4c = InceptionModule(in_channels=512, ch_3x3_reduce=128, ch_5x5_reduce=24, ch_3x3=256, ch_5x5=64, ch_pool_proj=64, ch_1x1=128)\n",
        "    # Inception 4d\n",
        "    self.inception4d = InceptionModule(in_channels=512, ch_3x3_reduce=144, ch_5x5_reduce=32, ch_3x3=288, ch_5x5=64, ch_pool_proj=64, ch_1x1=112)\n",
        "    # Inception 4e\n",
        "    self.inception4e = InceptionModule(in_channels=528, ch_3x3_reduce=160, ch_5x5_reduce=32, ch_3x3=320, ch_5x5=128, ch_pool_proj=128, ch_1x1=256)\n",
        "\n",
        "    # Max Pool 4\n",
        "    self.maxpool4  = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n",
        "\n",
        "    # Inception 5a\n",
        "    self.inception5a = InceptionModule(in_channels=832, ch_3x3_reduce=160, ch_5x5_reduce=32, ch_3x3=320, ch_5x5=128, ch_pool_proj=128, ch_1x1=256)\n",
        "    # Inception 5b\n",
        "    self.inception5b = InceptionModule(in_channels=832, ch_3x3_reduce=192, ch_5x5_reduce=48, ch_3x3=384, ch_5x5=128, ch_pool_proj=128, ch_1x1=384)\n",
        "\n",
        "    # Avg Pool\n",
        "    #self.avgpool = nn.AvgPool2d(kernel_size=7, stride=1)\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    # Dropout .4\n",
        "    self.dropout = nn.Dropout2d(0.4)\n",
        "\n",
        "    # Linear\n",
        "    # Capa de salida (antes de la funciÃ³n de salida)\n",
        "    self.fc_out = nn.Linear(1024, n_classes)\n",
        "\n",
        "    # Decide si usar la clasificaciÃ³n auxiliar\n",
        "    if self.use_aux_logits:\n",
        "      self.aux1 = InceptionAux(512, n_classes)\n",
        "      self.aux2 = InceptionAux(528, n_classes)\n",
        "    else:\n",
        "      self.aux1 = None  # type: ignore[assignment]\n",
        "      self.aux2 = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.use_aux_logits and self.training:\n",
        "      aux_logits = []\n",
        "    else:\n",
        "      aux_logits = None\n",
        "    x = self.conv1(x)\n",
        "    \n",
        "    x = self.maxpool1(x)\n",
        "    \n",
        "    x = self.conv2(x)\n",
        "    \n",
        "    x = self.maxpool2(x)\n",
        "\n",
        "\n",
        "    x = self.inception3a(x)\n",
        "  \n",
        "    x = self.inception3b(x)\n",
        "\n",
        "    x = self.maxpool3(x)\n",
        "    \n",
        "    x = self.inception4a(x)\n",
        "\n",
        "    # Crea una lista para los logits auxiliares si fuera necesario\n",
        "    if self.use_aux_logits:\n",
        "      if self.training:\n",
        "        aux1_out = self.aux1(x)\n",
        "        aux_logits.append(aux1_out)\n",
        "    else:\n",
        "      aux1_out = None\n",
        "    \n",
        "\n",
        "    x = self.inception4b(x)\n",
        "    \n",
        "    x = self.inception4c(x)\n",
        "    \n",
        "    x = self.inception4d(x)\n",
        "\n",
        "    # Si se usa la clasificaciÃ³n auxiliar, computa logits auxiliares\n",
        "    if self.use_aux_logits:\n",
        "      if self.training:\n",
        "        aux2_out = self.aux2(x)\n",
        "        aux_logits.append(aux2_out)\n",
        "    else:\n",
        "      aux2_out = None\n",
        "\n",
        "    # ContinÃºa computando las representaciones internas de la red\n",
        "    x = self.inception4e(x)\n",
        "    \n",
        "    x = self.maxpool4(x)\n",
        "\n",
        "\n",
        "    x = self.inception5a(x)\n",
        "\n",
        "    hidden = torch.squeeze(x)\n",
        "\n",
        "    # hidden = self.hidden_fc(hidden)\n",
        "\n",
        "    x = self.inception5b(x)\n",
        "    \n",
        "\n",
        "    x = self.avgpool(x)\n",
        "    \n",
        "    x = torch.flatten(x, 1)\n",
        "    \n",
        "    x = self.dropout(x)\n",
        "    x = self.fc_out(x)\n",
        "    \n",
        "    #return x, aux2_out, aux1_out\n",
        "\n",
        "    # En hidden debes devolver alguna de las capas oculta de la red\n",
        "    return {'hidden': hidden, 'logits': x, 'aux_logits': aux_logits}"
      ],
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWs77G8OXURU",
        "outputId": "f5649a14-8197-4cc9-ddd4-2eed779fbd8a"
      },
      "source": [
        "ver_resumen = True\r\n",
        "if ver_resumen:\r\n",
        "  device = 'cuda'\r\n",
        "  # Instanciamos la red\r\n",
        "  test_model = GoogLeNet(n_classes=1000, use_aux_logits=True).to(device)\r\n",
        "  # Le pasamos un tensor de prueba para verificar que las dimensiones esten bien\r\n",
        "  summary(test_model, input_size=(3, 32, 32))"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
            "        Conv2dRelu-2           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-3             [-1, 64, 8, 8]               0\n",
            "            Conv2d-4            [-1, 192, 8, 8]         110,592\n",
            "        Conv2dRelu-5            [-1, 192, 8, 8]               0\n",
            "         MaxPool2d-6            [-1, 192, 4, 4]               0\n",
            "            Conv2d-7             [-1, 96, 4, 4]          18,432\n",
            "        Conv2dRelu-8             [-1, 96, 4, 4]               0\n",
            "            Conv2d-9            [-1, 128, 4, 4]         110,592\n",
            "       Conv2dRelu-10            [-1, 128, 4, 4]               0\n",
            "           Conv2d-11             [-1, 16, 4, 4]           3,072\n",
            "       Conv2dRelu-12             [-1, 16, 4, 4]               0\n",
            "           Conv2d-13             [-1, 32, 4, 4]          12,800\n",
            "       Conv2dRelu-14             [-1, 32, 4, 4]               0\n",
            "        MaxPool2d-15            [-1, 192, 4, 4]               0\n",
            "           Conv2d-16             [-1, 32, 4, 4]           6,144\n",
            "       Conv2dRelu-17             [-1, 32, 4, 4]               0\n",
            "           Conv2d-18             [-1, 64, 4, 4]          12,288\n",
            "       Conv2dRelu-19             [-1, 64, 4, 4]               0\n",
            "  InceptionModule-20            [-1, 256, 4, 4]               0\n",
            "           Conv2d-21            [-1, 128, 4, 4]          32,768\n",
            "       Conv2dRelu-22            [-1, 128, 4, 4]               0\n",
            "           Conv2d-23            [-1, 192, 4, 4]         221,184\n",
            "       Conv2dRelu-24            [-1, 192, 4, 4]               0\n",
            "           Conv2d-25             [-1, 32, 4, 4]           8,192\n",
            "       Conv2dRelu-26             [-1, 32, 4, 4]               0\n",
            "           Conv2d-27             [-1, 96, 4, 4]          76,800\n",
            "       Conv2dRelu-28             [-1, 96, 4, 4]               0\n",
            "        MaxPool2d-29            [-1, 256, 4, 4]               0\n",
            "           Conv2d-30             [-1, 64, 4, 4]          16,384\n",
            "       Conv2dRelu-31             [-1, 64, 4, 4]               0\n",
            "           Conv2d-32            [-1, 128, 4, 4]          32,768\n",
            "       Conv2dRelu-33            [-1, 128, 4, 4]               0\n",
            "  InceptionModule-34            [-1, 480, 4, 4]               0\n",
            "        MaxPool2d-35            [-1, 480, 2, 2]               0\n",
            "           Conv2d-36             [-1, 96, 2, 2]          46,080\n",
            "       Conv2dRelu-37             [-1, 96, 2, 2]               0\n",
            "           Conv2d-38            [-1, 208, 2, 2]         179,712\n",
            "       Conv2dRelu-39            [-1, 208, 2, 2]               0\n",
            "           Conv2d-40             [-1, 16, 2, 2]           7,680\n",
            "       Conv2dRelu-41             [-1, 16, 2, 2]               0\n",
            "           Conv2d-42             [-1, 48, 2, 2]          19,200\n",
            "       Conv2dRelu-43             [-1, 48, 2, 2]               0\n",
            "        MaxPool2d-44            [-1, 480, 2, 2]               0\n",
            "           Conv2d-45             [-1, 64, 2, 2]          30,720\n",
            "       Conv2dRelu-46             [-1, 64, 2, 2]               0\n",
            "           Conv2d-47            [-1, 192, 2, 2]          92,160\n",
            "       Conv2dRelu-48            [-1, 192, 2, 2]               0\n",
            "  InceptionModule-49            [-1, 512, 2, 2]               0\n",
            "AdaptiveAvgPool2d-50            [-1, 512, 1, 1]               0\n",
            "           Conv2d-51            [-1, 128, 1, 1]          65,536\n",
            "       Conv2dRelu-52            [-1, 128, 1, 1]               0\n",
            "           Linear-53                 [-1, 1024]         132,096\n",
            "           Linear-54                 [-1, 1000]       1,025,000\n",
            "     InceptionAux-55                 [-1, 1000]               0\n",
            "           Conv2d-56            [-1, 112, 2, 2]          57,344\n",
            "       Conv2dRelu-57            [-1, 112, 2, 2]               0\n",
            "           Conv2d-58            [-1, 224, 2, 2]         225,792\n",
            "       Conv2dRelu-59            [-1, 224, 2, 2]               0\n",
            "           Conv2d-60             [-1, 24, 2, 2]          12,288\n",
            "       Conv2dRelu-61             [-1, 24, 2, 2]               0\n",
            "           Conv2d-62             [-1, 64, 2, 2]          38,400\n",
            "       Conv2dRelu-63             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-64            [-1, 512, 2, 2]               0\n",
            "           Conv2d-65             [-1, 64, 2, 2]          32,768\n",
            "       Conv2dRelu-66             [-1, 64, 2, 2]               0\n",
            "           Conv2d-67            [-1, 160, 2, 2]          81,920\n",
            "       Conv2dRelu-68            [-1, 160, 2, 2]               0\n",
            "  InceptionModule-69            [-1, 512, 2, 2]               0\n",
            "           Conv2d-70            [-1, 128, 2, 2]          65,536\n",
            "       Conv2dRelu-71            [-1, 128, 2, 2]               0\n",
            "           Conv2d-72            [-1, 256, 2, 2]         294,912\n",
            "       Conv2dRelu-73            [-1, 256, 2, 2]               0\n",
            "           Conv2d-74             [-1, 24, 2, 2]          12,288\n",
            "       Conv2dRelu-75             [-1, 24, 2, 2]               0\n",
            "           Conv2d-76             [-1, 64, 2, 2]          38,400\n",
            "       Conv2dRelu-77             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-78            [-1, 512, 2, 2]               0\n",
            "           Conv2d-79             [-1, 64, 2, 2]          32,768\n",
            "       Conv2dRelu-80             [-1, 64, 2, 2]               0\n",
            "           Conv2d-81            [-1, 128, 2, 2]          65,536\n",
            "       Conv2dRelu-82            [-1, 128, 2, 2]               0\n",
            "  InceptionModule-83            [-1, 512, 2, 2]               0\n",
            "           Conv2d-84            [-1, 144, 2, 2]          73,728\n",
            "       Conv2dRelu-85            [-1, 144, 2, 2]               0\n",
            "           Conv2d-86            [-1, 288, 2, 2]         373,248\n",
            "       Conv2dRelu-87            [-1, 288, 2, 2]               0\n",
            "           Conv2d-88             [-1, 32, 2, 2]          16,384\n",
            "       Conv2dRelu-89             [-1, 32, 2, 2]               0\n",
            "           Conv2d-90             [-1, 64, 2, 2]          51,200\n",
            "       Conv2dRelu-91             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-92            [-1, 512, 2, 2]               0\n",
            "           Conv2d-93             [-1, 64, 2, 2]          32,768\n",
            "       Conv2dRelu-94             [-1, 64, 2, 2]               0\n",
            "           Conv2d-95            [-1, 112, 2, 2]          57,344\n",
            "       Conv2dRelu-96            [-1, 112, 2, 2]               0\n",
            "  InceptionModule-97            [-1, 528, 2, 2]               0\n",
            "AdaptiveAvgPool2d-98            [-1, 528, 1, 1]               0\n",
            "           Conv2d-99            [-1, 128, 1, 1]          67,584\n",
            "      Conv2dRelu-100            [-1, 128, 1, 1]               0\n",
            "          Linear-101                 [-1, 1024]         132,096\n",
            "          Linear-102                 [-1, 1000]       1,025,000\n",
            "    InceptionAux-103                 [-1, 1000]               0\n",
            "          Conv2d-104            [-1, 160, 2, 2]          84,480\n",
            "      Conv2dRelu-105            [-1, 160, 2, 2]               0\n",
            "          Conv2d-106            [-1, 320, 2, 2]         460,800\n",
            "      Conv2dRelu-107            [-1, 320, 2, 2]               0\n",
            "          Conv2d-108             [-1, 32, 2, 2]          16,896\n",
            "      Conv2dRelu-109             [-1, 32, 2, 2]               0\n",
            "          Conv2d-110            [-1, 128, 2, 2]         102,400\n",
            "      Conv2dRelu-111            [-1, 128, 2, 2]               0\n",
            "       MaxPool2d-112            [-1, 528, 2, 2]               0\n",
            "          Conv2d-113            [-1, 128, 2, 2]          67,584\n",
            "      Conv2dRelu-114            [-1, 128, 2, 2]               0\n",
            "          Conv2d-115            [-1, 256, 2, 2]         135,168\n",
            "      Conv2dRelu-116            [-1, 256, 2, 2]               0\n",
            " InceptionModule-117            [-1, 832, 2, 2]               0\n",
            "       MaxPool2d-118            [-1, 832, 1, 1]               0\n",
            "          Conv2d-119            [-1, 160, 1, 1]         133,120\n",
            "      Conv2dRelu-120            [-1, 160, 1, 1]               0\n",
            "          Conv2d-121            [-1, 320, 1, 1]         460,800\n",
            "      Conv2dRelu-122            [-1, 320, 1, 1]               0\n",
            "          Conv2d-123             [-1, 32, 1, 1]          26,624\n",
            "      Conv2dRelu-124             [-1, 32, 1, 1]               0\n",
            "          Conv2d-125            [-1, 128, 1, 1]         102,400\n",
            "      Conv2dRelu-126            [-1, 128, 1, 1]               0\n",
            "       MaxPool2d-127            [-1, 832, 1, 1]               0\n",
            "          Conv2d-128            [-1, 128, 1, 1]         106,496\n",
            "      Conv2dRelu-129            [-1, 128, 1, 1]               0\n",
            "          Conv2d-130            [-1, 256, 1, 1]         212,992\n",
            "      Conv2dRelu-131            [-1, 256, 1, 1]               0\n",
            " InceptionModule-132            [-1, 832, 1, 1]               0\n",
            "          Conv2d-133            [-1, 192, 1, 1]         159,744\n",
            "      Conv2dRelu-134            [-1, 192, 1, 1]               0\n",
            "          Conv2d-135            [-1, 384, 1, 1]         663,552\n",
            "      Conv2dRelu-136            [-1, 384, 1, 1]               0\n",
            "          Conv2d-137             [-1, 48, 1, 1]          39,936\n",
            "      Conv2dRelu-138             [-1, 48, 1, 1]               0\n",
            "          Conv2d-139            [-1, 128, 1, 1]         153,600\n",
            "      Conv2dRelu-140            [-1, 128, 1, 1]               0\n",
            "       MaxPool2d-141            [-1, 832, 1, 1]               0\n",
            "          Conv2d-142            [-1, 128, 1, 1]         106,496\n",
            "      Conv2dRelu-143            [-1, 128, 1, 1]               0\n",
            "          Conv2d-144            [-1, 384, 1, 1]         319,488\n",
            "      Conv2dRelu-145            [-1, 384, 1, 1]               0\n",
            " InceptionModule-146           [-1, 1024, 1, 1]               0\n",
            "AdaptiveAvgPool2d-147           [-1, 1024, 1, 1]               0\n",
            "       Dropout2d-148                 [-1, 1024]               0\n",
            "          Linear-149                 [-1, 1000]       1,025,000\n",
            "================================================================\n",
            "Total params: 9,434,488\n",
            "Trainable params: 9,434,488\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.44\n",
            "Params size (MB): 35.99\n",
            "Estimated Total Size (MB): 37.44\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQs327QW0Fnd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f4019ae-cc7f-4563-c318-ea74cf996600"
      },
      "source": [
        "# Tests del API del curso para el InceptionModule\n",
        "\n",
        "# Obtengamos algunos parametros para probar tu implementaciÃ³n\n",
        "x, n_classes, use_aux_logits = corrector.get_test_data(homework=4, question=\"1b\", test=1, token=token)\n",
        "\n",
        "# Corramos tu implementaciÃ³n de InseptionModule para ver como se comporta\n",
        "with torch.no_grad():\n",
        "  model = GoogLeNet(n_classes=n_classes, use_aux_logits=True)\n",
        "  s = timer()\n",
        "  result = model(torch.tensor(x))\n",
        "  t = timer()-s\n",
        "\n",
        "# Veamos si todo fue OK :)\n",
        "sizes = [result['hidden'].shape[0]] + list(result['logits'].size()) + [d for a in result['aux_logits'] for d in a.size()]\n",
        "corrector.submit(homework=4, question=\"1b\", test=1, token=token, answer=sizes, time=t)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct Test!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEJRMYJ6iwhm"
      },
      "source": [
        "## 1c) DenseNet\r\n",
        "https://arxiv.org/pdf/1608.06993.pdf\r\n",
        "\r\n",
        "Se debe destacar que adicionalmente se intento implementar la red `ResNet` y hacer transferencia de aprendizaje. Como fue copy paste, se decidio implementar `DenseNet` que era mas sencilla. \r\n",
        "\r\n",
        "Se deja de todas formas el link a la `ResNet` implementada, que obtuvo mucho mejor resultado: [Repo con ResNet.](https://github.com/Diego-II/DeepLearning/blob/master/Tarea4/Respuestas_Tarea_4_ConvNet_CC6204_2020.ipynb)\r\n",
        "\r\n",
        "Cabe destacar que el paper y repositorio de donde se saco utilizaba pocas imagenes para el fine tuning, 5 por clases logrando accuracy superior al 90% ([Big Transfer](https://paperswithcode.com/paper/large-scale-learning-of-general-visual)). \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYUtVYIszUgx"
      },
      "source": [
        "class Conv2dBnRelu(nn.Module):\r\n",
        "  def __init__(\r\n",
        "      self, \r\n",
        "      in_channels,\r\n",
        "      out_channels, \r\n",
        "      **kwargs\r\n",
        "  ):  \r\n",
        "    super(Conv2dBnRelu, self).__init__()\r\n",
        "    self.bn = nn.BatchNorm2d(in_channels)\r\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, bias = False,**kwargs)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    x = self.conv(F.relu(self.bn(x)))\r\n",
        "    return x"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niUSRqn7zvQZ",
        "outputId": "ff2fa563-d280-41be-92e7-13087ab362a2"
      },
      "source": [
        "ver_resumen_dense_layer = True\r\n",
        "if ver_resumen_dense_layer:\r\n",
        "  device = 'cuda'\r\n",
        "  # Instanciamos la red\r\n",
        "  test_model = Conv2dBnRelu(3, 3, kernel_size=3, padding=1).to(device)\r\n",
        "  # Le pasamos un tensor de prueba para verificar que las dimensiones esten bien\r\n",
        "  summary(test_model, input_size=(3, 32, 32))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "       BatchNorm2d-1            [-1, 3, 32, 32]               6\n",
            "            Conv2d-2            [-1, 3, 32, 32]              81\n",
            "================================================================\n",
            "Total params: 87\n",
            "Trainable params: 87\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.05\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.06\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYpUbi9diwHC"
      },
      "source": [
        "# Vamos primero con el Dense Block\r\n",
        "class DenseConv(nn.Module):\r\n",
        "  def __init__(self, in_channels, growth_rate):\r\n",
        "    super(DenseConv, self).__init__()\r\n",
        "    self.conv1 = Conv2dBnRelu(in_channels, 4*growth_rate, kernel_size=1, stride=1)\r\n",
        "    self.conv2 = Conv2dBnRelu(4*growth_rate, growth_rate, kernel_size=3, padding=1, stride=1)\r\n",
        "\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    out = self.conv2(self.conv1(x))\r\n",
        "    return torch.cat([out, x], 1)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D1iwsvOtxHr",
        "outputId": "2c150abf-24b3-43f7-a471-5b7487774cb3"
      },
      "source": [
        "ver_resumen_dense_conv = True\r\n",
        "if ver_resumen_dense_conv:\r\n",
        "  device = 'cuda'\r\n",
        "  # Instanciamos la red\r\n",
        "  test_model = DenseConv(3, 12).to(device)\r\n",
        "  # Le pasamos un tensor de prueba para verificar que las dimensiones esten bien\r\n",
        "  summary(test_model, input_size=(3, 32, 32))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "       BatchNorm2d-1            [-1, 3, 32, 32]               6\n",
            "            Conv2d-2           [-1, 48, 32, 32]             144\n",
            "      Conv2dBnRelu-3           [-1, 48, 32, 32]               0\n",
            "       BatchNorm2d-4           [-1, 48, 32, 32]              96\n",
            "            Conv2d-5           [-1, 12, 32, 32]           5,184\n",
            "      Conv2dBnRelu-6           [-1, 12, 32, 32]               0\n",
            "================================================================\n",
            "Total params: 5,430\n",
            "Trainable params: 5,430\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.34\n",
            "Params size (MB): 0.02\n",
            "Estimated Total Size (MB): 1.37\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-t2xB519oO6"
      },
      "source": [
        "class DenseLayer(nn.Module):\r\n",
        "  def __init__(self, num_layers, in_channels, growth_rate):\r\n",
        "    super(DenseLayer, self).__init__()\r\n",
        "    self.layer = nn.Sequential()\r\n",
        "    for i in range(num_layers):\r\n",
        "      layer = DenseConv(\r\n",
        "          in_channels=in_channels + i * growth_rate,\r\n",
        "          growth_rate = growth_rate\r\n",
        "      )\r\n",
        "      self.layer.add_module('denselayer%d' % (i + 1), layer)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    x = self.layer(x)\r\n",
        "    return x"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWvqmehu_y1v",
        "outputId": "5a94698b-34d4-4202-92c8-e42fb9d24f91"
      },
      "source": [
        "ver_resumen_dense_layer = True\r\n",
        "if ver_resumen_dense_layer:\r\n",
        "  device = 'cuda'\r\n",
        "  # Instanciamos la red\r\n",
        "  test_model = DenseLayer(4, 3, 12).to(device)\r\n",
        "  # Le pasamos un tensor de prueba para verificar que las dimensiones esten bien\r\n",
        "  summary(test_model, input_size=(3, 32, 32))"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "       BatchNorm2d-1            [-1, 3, 32, 32]               6\n",
            "            Conv2d-2           [-1, 48, 32, 32]             144\n",
            "      Conv2dBnRelu-3           [-1, 48, 32, 32]               0\n",
            "       BatchNorm2d-4           [-1, 48, 32, 32]              96\n",
            "            Conv2d-5           [-1, 12, 32, 32]           5,184\n",
            "      Conv2dBnRelu-6           [-1, 12, 32, 32]               0\n",
            "         DenseConv-7           [-1, 15, 32, 32]               0\n",
            "       BatchNorm2d-8           [-1, 15, 32, 32]              30\n",
            "            Conv2d-9           [-1, 48, 32, 32]             720\n",
            "     Conv2dBnRelu-10           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-11           [-1, 48, 32, 32]              96\n",
            "           Conv2d-12           [-1, 12, 32, 32]           5,184\n",
            "     Conv2dBnRelu-13           [-1, 12, 32, 32]               0\n",
            "        DenseConv-14           [-1, 27, 32, 32]               0\n",
            "      BatchNorm2d-15           [-1, 27, 32, 32]              54\n",
            "           Conv2d-16           [-1, 48, 32, 32]           1,296\n",
            "     Conv2dBnRelu-17           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-18           [-1, 48, 32, 32]              96\n",
            "           Conv2d-19           [-1, 12, 32, 32]           5,184\n",
            "     Conv2dBnRelu-20           [-1, 12, 32, 32]               0\n",
            "        DenseConv-21           [-1, 39, 32, 32]               0\n",
            "      BatchNorm2d-22           [-1, 39, 32, 32]              78\n",
            "           Conv2d-23           [-1, 48, 32, 32]           1,872\n",
            "     Conv2dBnRelu-24           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-25           [-1, 48, 32, 32]              96\n",
            "           Conv2d-26           [-1, 12, 32, 32]           5,184\n",
            "     Conv2dBnRelu-27           [-1, 12, 32, 32]               0\n",
            "        DenseConv-28           [-1, 51, 32, 32]               0\n",
            "================================================================\n",
            "Total params: 25,320\n",
            "Trainable params: 25,320\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 6.94\n",
            "Params size (MB): 0.10\n",
            "Estimated Total Size (MB): 7.05\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4iadPESyqVW"
      },
      "source": [
        "class TransitionLayer(nn.Module):\r\n",
        "  def __init__(self, in_channels, out_channels):\r\n",
        "    super(TransitionLayer, self).__init__()\r\n",
        "    self.conv = Conv2dBnRelu(in_channels, out_channels, kernel_size =1, stride=1)\r\n",
        "    self.avgpool = nn.AvgPool2d(kernel_size=2, stride=2)\r\n",
        "  \r\n",
        "  def forward(self, x):\r\n",
        "    x = self.avgpool(self.conv(x))\r\n",
        "    return x"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhBFocBp4SlY",
        "outputId": "5b32f19c-47e1-4f07-f5b6-a43761680daa"
      },
      "source": [
        "ver_resumen_transition_layer = True\r\n",
        "if ver_resumen_transition_layer:\r\n",
        "  device = 'cuda'\r\n",
        "  # Instanciamos la red\r\n",
        "  test_model = TransitionLayer(3, 12).to(device)\r\n",
        "  # Le pasamos un tensor de prueba para verificar que las dimensiones esten bien\r\n",
        "  summary(test_model, input_size=(3, 32, 32))"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "       BatchNorm2d-1            [-1, 3, 32, 32]               6\n",
            "            Conv2d-2           [-1, 12, 32, 32]              36\n",
            "      Conv2dBnRelu-3           [-1, 12, 32, 32]               0\n",
            "         AvgPool2d-4           [-1, 12, 16, 16]               0\n",
            "================================================================\n",
            "Total params: 42\n",
            "Trainable params: 42\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.23\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.25\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1Q8zAFC4f8I"
      },
      "source": [
        "class DenseNet(nn.Module):\r\n",
        "  def __init__(self, num_init_filters=32, blocks = None, growth_rate=12, n_classes=10):\r\n",
        "    super(DenseNet, self).__init__()\r\n",
        "    self.gr = growth_rate\r\n",
        "    if blocks is None:\r\n",
        "      blocks = [6,12,24,16]\r\n",
        "    self.blocks = blocks\r\n",
        "    self.input_layer = nn.Sequential(\r\n",
        "        Conv2dBnRelu(in_channels = 3, out_channels=num_init_filters, kernel_size=7, stride = 2, padding = 3),\r\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n",
        "    )\r\n",
        "    \r\n",
        "    self.body = nn.Sequential()\r\n",
        "    features_num = num_init_filters\r\n",
        "    for i, b in enumerate(self.blocks):\r\n",
        "      block = DenseLayer(\r\n",
        "          num_layers = b, \r\n",
        "          in_channels = features_num,\r\n",
        "          growth_rate = self.gr\r\n",
        "      )\r\n",
        "      self.body.add_module('denseblock%d' % (i + 1), block)\r\n",
        "      features_num = features_num + b * growth_rate\r\n",
        "\r\n",
        "      if i  != len(self.blocks) - 1:\r\n",
        "        transition = TransitionLayer(\r\n",
        "            in_channels = features_num,\r\n",
        "            out_channels = features_num // 2\r\n",
        "        )\r\n",
        "        self.body.add_module('transition%d' % (i + 1), transition)\r\n",
        "        features_num = features_num // 2\r\n",
        "    \r\n",
        "    # self.final_pool = nn.AdaptiveAvgPool2d((7, 7))\r\n",
        "\r\n",
        "    self.classifier = nn.Linear(features_num, n_classes)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    x = self.input_layer(x)\r\n",
        "    out = self.body(x)\r\n",
        "    hidden = torch.squeeze(out)\r\n",
        "    out = F.adaptive_avg_pool2d(out, (1, 1))\r\n",
        "    out = torch.flatten(out, 1)\r\n",
        "    out = self.classifier(out)\r\n",
        "    \r\n",
        "    return {'hidden': hidden, 'logits': out}\r\n"
      ],
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U9fRBN57ONF"
      },
      "source": [
        "ver_resumen_densenet = True\r\n",
        "if ver_resumen_densenet:\r\n",
        "  device = 'cuda'\r\n",
        "  # Instanciamos la red\r\n",
        "  test_model = DenseNet().to(device)\r\n",
        "  # Le pasamos un tensor de prueba para verificar que las dimensiones esten bien\r\n",
        "  summary(test_model, input_size=(3, 32, 32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1-vU8oCDWfy"
      },
      "source": [
        "test_model.parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE6uDwmJ94-W"
      },
      "source": [
        "## 1d) ClasificaciÃ³n de ImÃ¡genes en CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1InBxxn28TJ4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99,
          "referenced_widgets": [
            "fa3ca844b289457fa56c862e34e7d988",
            "6fee87b018a446c9bbb34a47cced253e",
            "c6c3c35b26d544559ee704127b18db07",
            "9e7e145a531e41738f5b352e589bd878",
            "38cca41e746446fbbc78ee4804c190f9",
            "832cbd9df69b49a8a5e5181c377ac746",
            "36f88d05f0d946fd94fbf50f5b9704f0",
            "db1d469cbf334002940c6dcb8ed90a5d"
          ]
        },
        "outputId": "22c3c85d-5e43-4ffc-98d9-f1d9885852c0"
      },
      "source": [
        "##############################################################################\n",
        "# Todo este cÃ³digo sirve para descargar, preprocesar y dejar los datos\n",
        "# listos para usar despuÃ©s. DespuÃ©s de ejecutar las celdas tendrÃ¡s los datos \n",
        "# trainset, trainloader y similar para test.\n",
        "##############################################################################\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa3ca844b289457fa56c862e34e7d988",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIbHp7RTNd75"
      },
      "source": [
        "Se utilizara el siguiente `lr_scheduler`:\r\n",
        " [CosineAnnealingLR](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.CosineAnnealingLR). \r\n",
        "\r\n",
        "Este se propone en el paper [SGDR: STOCHASTIC GRADIENT DESCENT WITH WARM RESTARTS](https://arxiv.org/pdf/1608.03983.pdf). En este, afirman que utilizando los `Warm Restarts`, se pueden lograr resultados competitivos en `CIFAR10` y `CIFAR100` aproximadamente dos a cuatro veces mas rapido. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecHwyZa6oxMc"
      },
      "source": [
        "# Definamos algunos hiper-parÃ¡metros\n",
        "BATCH_SIZE = 32\n",
        "LR = 0.003\n",
        "EPOCHS = 30\n",
        "REPORTS_EVERY = 1\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                          shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(testset, batch_size=4*BATCH_SIZE,\n",
        "                         shuffle=False, num_workers=2)\n",
        "\n",
        "google_net =  GoogLeNet(n_classes=10, use_aux_logits=True)# tu modelo de CNN (para clasificar en 10 clases)\n",
        "optimizer = optim.Adam(google_net.parameters(), lr=LR) # optimizador, e.g., optim.SGD, optim.Adam, ...\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() # funciÃ³n de pÃ©rdida\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader))\n",
        "# (opcional) optim.lr_scheduler proporciona varios mÃ©todos para ajustar el lr segÃºn el nÃºmero de Ã©pocas\n",
        "\n",
        "# ipdb.set_trace()\n",
        "train_loss, acc = train_for_classification(google_net, train_loader, test_loader, optimizer, criterion, lr_scheduler=scheduler, epochs=EPOCHS)\n",
        "\n",
        "plot_results(train_loss, acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9AW8vJm-JRqy",
        "outputId": "f23e0b75-9129-46ed-f977-29cea4d96bae"
      },
      "source": [
        "# Definamos algunos hiper-parÃ¡metros\r\n",
        "BATCH_SIZE = 32\r\n",
        "LR = 0.003\r\n",
        "EPOCHS = 30\r\n",
        "REPORTS_EVERY = 1\r\n",
        "\r\n",
        "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE,\r\n",
        "                          shuffle=True, num_workers=2)\r\n",
        "test_loader = DataLoader(testset, batch_size=4*BATCH_SIZE,\r\n",
        "                         shuffle=False, num_workers=2)\r\n",
        "\r\n",
        "dense_net =  DenseNet(num_init_filters=64)# tu modelo de CNN (para clasificar en 10 clases)\r\n",
        "optimizer = optim.Adam(dense_net.parameters(), lr=LR) # optimizador, e.g., optim.SGD, optim.Adam, ...\r\n",
        "\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss() # funciÃ³n de pÃ©rdida\r\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader))\r\n",
        "# (opcional) optim.lr_scheduler proporciona varios mÃ©todos para ajustar el lr segÃºn el nÃºmero de Ã©pocas\r\n",
        "\r\n",
        "# ipdb.set_trace()\r\n",
        "train_loss, acc = train_for_classification(dense_net, train_loader, test_loader, optimizer, criterion ,lr_scheduler=scheduler, epochs=EPOCHS)\r\n",
        "\r\n",
        "plot_results(train_loss, acc)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1(50000/50000), lr:0.0030000, Loss:1.55936, Train Acc:42.4%, Validating..., Val Acc:53.20%, Avg-Time:168.362s.\n",
            "Epoch:2(50000/50000), lr:0.0030000, Loss:1.09657, Train Acc:60.8%, Validating..., Val Acc:60.29%, Avg-Time:169.126s.\n",
            "Epoch:3(50000/50000), lr:0.0030000, Loss:0.88915, Train Acc:68.8%, Validating..., Val Acc:67.75%, Avg-Time:169.480s.\n",
            "Epoch:4(50000/50000), lr:0.0030000, Loss:0.77182, Train Acc:72.9%, Validating..., Val Acc:72.16%, Avg-Time:169.129s.\n",
            "Epoch:5(50000/50000), lr:0.0030000, Loss:0.69017, Train Acc:75.9%, Validating..., Val Acc:70.96%, Avg-Time:169.299s.\n",
            "Epoch:6(50000/50000), lr:0.0029999, Loss:0.62152, Train Acc:78.4%, Validating..., Val Acc:75.37%, Avg-Time:170.308s.\n",
            "Epoch:7(50000/50000), lr:0.0029999, Loss:0.56616, Train Acc:80.2%, Validating..., Val Acc:76.38%, Avg-Time:170.818s.\n",
            "Epoch:8(50000/50000), lr:0.0029999, Loss:0.51388, Train Acc:82.0%, Validating..., Val Acc:75.69%, Avg-Time:170.410s.\n",
            "Epoch:9(50000/50000), lr:0.0029998, Loss:0.47240, Train Acc:83.4%, Validating..., Val Acc:77.40%, Avg-Time:169.876s.\n",
            "Epoch:10(50000/50000), lr:0.0029998, Loss:0.43401, Train Acc:84.8%, Validating..., Val Acc:77.18%, Avg-Time:169.493s.\n",
            "Epoch:11(50000/50000), lr:0.0029997, Loss:0.39580, Train Acc:86.0%, Validating..., Val Acc:76.33%, Avg-Time:169.027s.\n",
            "Epoch:12(50000/50000), lr:0.0029996, Loss:0.36477, Train Acc:87.2%, Validating..., Val Acc:76.88%, Avg-Time:168.657s.\n",
            "Epoch:13(50000/50000), lr:0.0029996, Loss:0.33378, Train Acc:88.2%, Validating..., Val Acc:75.45%, Avg-Time:168.067s.\n",
            "Epoch:14(50000/50000), lr:0.0029995, Loss:0.31313, Train Acc:88.9%, Validating..., Val Acc:77.42%, Avg-Time:167.617s.\n",
            "Epoch:15(50000/50000), lr:0.0029994, Loss:0.28406, Train Acc:89.8%, Validating..., Val Acc:77.21%, Avg-Time:167.102s.\n",
            "Epoch:16(50000/50000), lr:0.0029993, Loss:0.27110, Train Acc:90.4%, Validating..., Val Acc:77.92%, Avg-Time:166.793s.\n",
            "Epoch:17(50000/50000), lr:0.0029992, Loss:0.25423, Train Acc:91.2%, Validating..., Val Acc:75.62%, Avg-Time:166.484s.\n",
            "Epoch:18(50000/50000), lr:0.0029991, Loss:0.23455, Train Acc:91.5%, Validating..., Val Acc:76.29%, Avg-Time:166.192s.\n",
            "Epoch:19(50000/50000), lr:0.0029990, Loss:0.21868, Train Acc:92.1%, Validating..., Val Acc:76.42%, Avg-Time:165.888s.\n",
            "Epoch:20(50000/50000), lr:0.0029989, Loss:0.20924, Train Acc:92.5%, Validating..., Val Acc:77.27%, Avg-Time:165.728s.\n",
            "Epoch:21(50000/50000), lr:0.0029988, Loss:0.19770, Train Acc:93.1%, Validating..., Val Acc:76.80%, Avg-Time:165.582s.\n",
            "Epoch:22(50000/50000), lr:0.0029987, Loss:0.18203, Train Acc:93.5%, Validating..., Val Acc:76.60%, Avg-Time:165.389s.\n",
            "Epoch:23(50000/50000), lr:0.0029985, Loss:0.17856, Train Acc:93.8%, Validating..., Val Acc:77.12%, Avg-Time:165.093s.\n",
            "Epoch:24(50000/50000), lr:0.0029984, Loss:0.17073, Train Acc:94.0%, Validating..., Val Acc:76.74%, Avg-Time:164.927s.\n",
            "Epoch:25(50000/50000), lr:0.0029983, Loss:0.15908, Train Acc:94.3%, Validating..., Val Acc:77.63%, Avg-Time:164.755s.\n",
            "Epoch:26(50000/50000), lr:0.0029981, Loss:0.15342, Train Acc:94.6%, Validating..., Val Acc:77.47%, Avg-Time:164.554s.\n",
            "Epoch:27(50000/50000), lr:0.0029980, Loss:0.15022, Train Acc:94.6%, Validating..., Val Acc:76.72%, Avg-Time:164.491s.\n",
            "Epoch:28(50000/50000), lr:0.0029978, Loss:0.14166, Train Acc:94.9%, Validating..., Val Acc:77.43%, Avg-Time:164.325s.\n",
            "Epoch:29(50000/50000), lr:0.0029976, Loss:0.13850, Train Acc:95.1%, Validating..., Val Acc:77.29%, Avg-Time:164.182s.\n",
            "Epoch:30(50000/50000), lr:0.0029975, Loss:0.13552, Train Acc:95.3%, Validating..., Val Acc:77.90%, Avg-Time:164.096s.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpj00Mw3iN5y",
        "outputId": "b3dda4dd-a34c-4450-958b-677b94a95e3f"
      },
      "source": [
        "# Test\r\n",
        "x, y = list(test_loader)[0]\r\n",
        "net.cpu()\r\n",
        "net.eval()\r\n",
        "y_pred = net(x)['logits'].max(dim=1)[1]\r\n",
        "\r\n",
        "# Veamos como se comporta el modelo\r\n",
        "print(\"Correct Test!\" if (y==y_pred).sum()/len(x) >= .75 else \"Failed Test! [acc]\")"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct Test!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmALm7EtpFow"
      },
      "source": [
        "## 1e) Opcional: CIFAR100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prIQA-PjpqV7"
      },
      "source": [
        "##############################################################################\n",
        "# Toda esta parte es similar a la anterior pero para CIFAR100.\n",
        "##############################################################################\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data/cifar100', train=True,\n",
        "                                         download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root='./data/cifar100', train=False,\n",
        "                                        download=True, transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDSB4v2x8T3k"
      },
      "source": [
        "# Definamos algunos hiper-parÃ¡metros\n",
        "BATCH_SIZE = ...\n",
        "LR = ...\n",
        "EPOCHS = ...\n",
        "REPORTS_EVERY = 1\n",
        "\n",
        "net = ... # tu modelo de CNN (para clasificar en 100 clases)\n",
        "optimizer = ... # optimizador, e.g., optim.SGD, optim.Adam, ...\n",
        "criterion = nn.CrossEntropyLoss() # funciÃ³n de pÃ©rdida\n",
        "scheduler = ... # (opcional) optim.lr_scheduler proporciona varios mÃ©todos para ajustar el lr segÃºn el nÃºmero de Ã©pocas\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                          shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(testset, batch_size=4*BATCH_SIZE,\n",
        "                         shuffle=False, num_workers=2)\n",
        "\n",
        "train_loss, acc = train_for_classification(net, train_loader, \n",
        "                                           test_loader, optimizer, \n",
        "                                           criterion, lr_scheduler=scheduler, \n",
        "                                           epochs=EPOCHS, reports_every=REPORTS_EVERY)\n",
        "\n",
        "plot_results(train_loss, acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRNeU6di3NzC"
      },
      "source": [
        "# Parte 2: Subtitulado de ImÃ¡genes mediante RecuperaciÃ³n de Textos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twAZ0BtioCT0"
      },
      "source": [
        "## 2a) CodificaciÃ³n de ImÃ¡genes y Textos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCf2boQKEi1E"
      },
      "source": [
        "Veamos el tamano de las salidas de las capas ocultas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4sphvDSpMnc",
        "outputId": "0bf45c75-3288-48e9-d949-66ba809284cc"
      },
      "source": [
        "# GoogleNet HiddenSize:\r\n",
        "dense_net = DenseNet()\r\n",
        "out = dense_net(torch.randn(5,3,32,32))\r\n",
        "hid = out['hidden']\r\n",
        "hid_flt = torch.squeeze(hid)\r\n",
        "print(hid.size())\r\n",
        "print(hid_flt.size())"
      ],
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 385])\n",
            "torch.Size([5, 385])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJntLLRzuKXb",
        "outputId": "625a61f8-54e4-47df-99b4-db965183ecf6"
      },
      "source": [
        "# DenseNet HiddenSize:\r\n",
        "google_net = GoogLeNet(10)\r\n",
        "out = google_net(torch.randn(9,3,32,32))\r\n",
        "hid = out['hidden']\r\n",
        "hid_flt = torch.squeeze(hid)\r\n",
        "print(hid.size())\r\n",
        "print(hid_flt.size())"
      ],
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([9, 832])\n",
            "torch.Size([9, 832])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqLpt52C2tCt"
      },
      "source": [
        "class MyLinear(nn.Module):\r\n",
        "  def __init__(self, in_features, out_features, bias=False):\r\n",
        "    super(MyLinear, self).__init__()\r\n",
        "    self.linear = nn.Linear(in_features, out_features, bias)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    return F.relu(self.linear(x))"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9zd0b1MyAG8"
      },
      "source": [
        "class ImageEncoding(nn.Module):\n",
        "  def __init__(self, cnn_model, h_size, out_size=128):\n",
        "    super(ImageEncoding, self).__init__()\n",
        "    self.cnn_model = cnn_model\n",
        "\n",
        "    # Defina las capas de su MLP\n",
        "    # Hints: no usar mÃ¡s de 3 capas\n",
        "    #        incorpora alguna tÃ©cnica de regularizaciÃ³n que ya conoces\n",
        "    self.mlp = nn.Sequential(\n",
        "        MyLinear(h_size, 512),\n",
        "        nn.Dropout(.4),\n",
        "        MyLinear(512, 128)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.cnn_model(x)['hidden']\n",
        "    #out = torch.flatten(out)\n",
        "    \n",
        "    # Compute las capas de su MLP\n",
        "    out = self.mlp(out)\n",
        "\n",
        "    # En fc_out debe almacenar el encoding en R^d\n",
        "    return {'logits': out}"
      ],
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INp4seMgzrIl",
        "outputId": "c96ee1da-65eb-47f1-a4a4-13ad59a9e1d9"
      },
      "source": [
        "# probemos con la DenseNet que el paso forward funciona\r\n",
        "dense_net = DenseNet()\r\n",
        "test_model = ImageEncoding(dense_net, 385).cpu()\r\n",
        "test_model.eval()\r\n",
        "\r\n",
        "\r\n",
        "out_img_encoding = test_model(torch.randn(5,3,32,32))['logits']\r\n",
        "out_img_encoding.size()"
      ],
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 278
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMexnmB2xkRo",
        "outputId": "17ddae9f-91fa-463f-cd98-a9f3b378829f"
      },
      "source": [
        "# probemos con la googlenet que el paso forward funciona\r\n",
        "net_google = GoogLeNet(10)\r\n",
        "test_model = ImageEncoding(net_google, 832).cpu()\r\n",
        "test_model.eval()\r\n",
        "\r\n",
        "\r\n",
        "out_img_encoding = test_model(torch.randn(5,3,32,32))['logits']\r\n",
        "out_img_encoding.size()"
      ],
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 279
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ7NXhVY0xFr"
      },
      "source": [
        "class TextEncoding(nn.Module):\n",
        "  def __init__(self, text_embedding_size=4096, out_size=128, use_last_bn = True):\n",
        "    super(TextEncoding, self).__init__()\n",
        "\n",
        "    # Defina las capas de su MLP\n",
        "    # Hints: no usar mÃ¡s de 3 capas\n",
        "    #        incorpora alguna tÃ©cnica de regularizaciÃ³n que ya conoces\n",
        "    self.text_embedding_size = text_embedding_size\n",
        "    self.out_size = out_size\n",
        "    self.mlp = nn.Sequential(\n",
        "        MyLinear(in_features = self.text_embedding_size, out_features = 512), \n",
        "        nn.Dropout(.4),\n",
        "        MyLinear(in_features = 512, out_features = self.out_size)\n",
        "    )\n",
        "\n",
        "    self.use_last_bn = use_last_bn\n",
        "    if use_last_bn:\n",
        "      self.bn = nn.BatchNorm1d(out_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Compute las capas de su MLP\n",
        "    x = self.mlp(x)\n",
        "    if self.use_last_bn:\n",
        "      x = self.bn(x)\n",
        "\n",
        "    # En logits debe almacenar el encoding en R^d\n",
        "    return {'logits': x}"
      ],
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08sa7MG4c-GD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69674d4f-4689-4126-f659-42ef4e64766c"
      },
      "source": [
        "# Test\n",
        "OUT_SIZE = 128\n",
        "\n",
        "GoogLeNet_OUTSIZE = 832\n",
        "DenseNet_OUTSIZE = 385\n",
        "\n",
        "cnn_net = GoogLeNet(10)\n",
        "# cnn_model, h_size, out_size=128\n",
        "i_enc = ImageEncoding(cnn_model=cnn_net, h_size=GoogLeNet_OUTSIZE, out_size=OUT_SIZE)\n",
        "t_enc = TextEncoding(text_embedding_size=4096, out_size=OUT_SIZE)\n",
        "i_enc.eval()\n",
        "t_enc.eval()\n",
        "\n",
        "# Veamos como se comportan tus encoders\n",
        "print(\"Correct Test!\" if (i_enc(torch.randn(9,3,32,32))['logits'].size()==t_enc(torch.randn(9,4096))['logits'].size()) else \"Failed Test [size]\")\n",
        "print(\"Correct Test!\" if (i_enc(torch.randn(9,3,32,32))['logits'].size(-1)==OUT_SIZE) else \"Failed Test [size]\")"
      ],
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct Test!\n",
            "Correct Test!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR8AqpDi3ZJL"
      },
      "source": [
        "## 2b) Buenas codificaciones y la *Triplet Loss*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U2nT3HvHPaq",
        "outputId": "f046b31d-09b7-4dd7-ed08-248b56ffd7cc"
      },
      "source": [
        "pdist = nn.PairwiseDistance(p=2)\r\n",
        "input1 = torch.randn(100, 128)\r\n",
        "input2 = torch.randn(100, 128)\r\n",
        "output = pdist(input1, input2)\r\n",
        "print(output)"
      ],
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([14.9455, 15.7268, 14.2393, 14.8871, 15.3311, 16.0450, 16.0879, 16.2489,\n",
            "        17.8060, 16.5755, 17.0452, 15.6189, 15.9809, 14.9398, 14.7355, 13.2298,\n",
            "        14.9277, 17.2278, 15.4129, 17.7053, 15.9146, 16.2073, 16.0836, 15.5174,\n",
            "        15.8282, 16.2708, 17.7882, 16.6109, 14.1407, 18.2435, 15.8321, 15.4619,\n",
            "        16.9900, 15.0555, 14.9149, 15.9733, 15.8181, 18.2734, 16.0636, 14.6049,\n",
            "        15.6461, 15.3767, 14.5219, 15.6809, 14.5253, 15.7335, 15.9773, 16.6382,\n",
            "        16.5109, 15.0881, 14.5452, 16.4182, 17.4725, 14.9075, 15.6768, 17.7168,\n",
            "        15.4214, 16.0521, 16.3560, 15.3168, 16.1120, 15.8804, 16.3980, 15.0860,\n",
            "        15.8781, 15.0179, 14.3429, 17.2880, 17.1674, 16.6379, 14.8180, 14.7521,\n",
            "        15.4586, 16.9283, 17.0057, 16.3143, 16.4488, 16.3504, 14.6804, 15.8113,\n",
            "        14.6199, 15.8996, 15.8436, 15.4203, 15.9010, 16.1233, 15.4325, 14.7735,\n",
            "        15.3255, 16.9763, 15.4107, 16.3193, 16.4888, 15.8957, 16.5400, 15.5778,\n",
            "        15.1368, 14.7643, 15.6804, 16.2536])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv0oqLTwv68U"
      },
      "source": [
        "class TripletLoss(nn.Module):\n",
        "  def __init__(self, margin=.2, negative='max'):\n",
        "    super(TripletLoss, self).__init__()\n",
        "    self.margin = margin\n",
        "    self.negative = negative\n",
        "\n",
        "  def forward(self, anchor, positive):\n",
        "    # Posiblemente lo mÃ¡s simple es partir calculando la distancia Euclideana\n",
        "    # entre las imagenes ancla y todos los pares (B x B) de representaciones\n",
        "    # de textos (hint: usa torch.cdist)\n",
        "    pdist = nn.PairwiseDistance(p=2)\n",
        "    dists = pdist(anchor, positive)\n",
        "\n",
        "    # Obtener distancias \"positivas\" de la diagonal\n",
        "    p_dists = ...\n",
        "\n",
        "    # Ahora genera un tensor con todos los costos que se deben agregar\n",
        "    # dependiendo de la forma de encontrar los negativos\n",
        "    if self.negative == 'max':\n",
        "      cost = ...\n",
        "    elif self.negative == 'random':\n",
        "      cost = ...\n",
        "    elif self.negative == 'all':\n",
        "      cost = ...\n",
        "    else:\n",
        "      raise ValueError()\n",
        "    \n",
        "    # Retorna el promedio de los costos de todos los triples considerados\n",
        "    return ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HKUFhf70nbd"
      },
      "source": [
        "# Tests del API del curso para TripletLoss\n",
        "\n",
        "# Obtengamos algunos parametros para probar tu implementaciÃ³n\n",
        "for test in [1,2]:\n",
        "  a, p, m, n  = corrector.get_test_data(homework=4, question=\"2b\", test=test, token=token)\n",
        "\n",
        "  criterion = TripletLoss(margin=m, negative=n)\n",
        "  result = criterion(torch.tensor(a), torch.tensor(p)).item()\n",
        "\n",
        "  # Veamos si todo fue OK :)\n",
        "  corrector.submit(homework=4, question=\"2b\", test=test, token=token, answer=result, time=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZcwMyTAwz28"
      },
      "source": [
        "## 2c) Probando tu implementaciÃ³n en Flickr8k"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E06OpFAxfuU"
      },
      "source": [
        "##############################################################################\n",
        "# Todo este cÃ³digo sirve para descargar, preprocesar y dejar los datos\n",
        "# listos para usar despuÃ©s. DespuÃ©s de ejecutar las dos celdas siguientes\n",
        "# tendrÃ¡s los datos en train_flickr_tripletset y similar para val y test\n",
        "##############################################################################\n",
        "\n",
        "folder_path = './data/flickr8k'\n",
        "if not os.path.exists(f'{folder_path}/images'):\n",
        "  print('\\n*** Descargando y extrayendo Flickr8k, siÃ©ntese y relÃ¡jese 4 mins...')\n",
        "  print('****** Descargando las imÃ¡genes...\\n')\n",
        "  !wget https://s06.imfd.cl/04/CC6204/tareas/tarea4/Flickr8k_Dataset.zip -P $folder_path/images\n",
        "  print('\\n********* Extrayendo las imÃ¡genes...\\n  Si te sale mensaje de colab, dale Ignorar\\n')\n",
        "  !unzip -q $folder_path/images/Flickr8k_Dataset.zip -d $folder_path/images\n",
        "  print('\\n*** Descargando y anotaciones de la imÃ¡genes...\\n')\n",
        "  !wget http://hockenmaier.cs.illinois.edu/8k-pictures.html -P $folder_path/annotations\n",
        "\n",
        "transform=transforms.Compose([transforms.ToTensor(), \n",
        "                              transforms.Resize((32, 32)),\n",
        "                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "print('Inicializando pytorch Flickr8k dataset')\n",
        "full_flickr_set = torchvision.datasets.Flickr8k(root=f'{folder_path}/images/Flicker8k_Dataset',\n",
        "                                                ann_file = f'{folder_path}/annotations/8k-pictures.html',\n",
        "                                                transform=transform)\n",
        "print('Creando train, val y test splits...')\n",
        "\n",
        "train_flickr_set, val_flickr_set, test_flickr_set = [], [], []\n",
        "for i, item in enumerate(full_flickr_set):\n",
        "  if i<6000:\n",
        "    train_flickr_set.append(item)\n",
        "  elif i<7000:\n",
        "    val_flickr_set.append(item)\n",
        "  else:\n",
        "    test_flickr_set.append(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTY5bha_xgCj"
      },
      "source": [
        "##############################################################################\n",
        "# Descarguemos representaciones de los textos de 4096 dimensiones\n",
        "##############################################################################\n",
        "if not os.path.exists(f'{folder_path}/flickr_cap_encodings_4096d.pkl'):\n",
        "  !wget https://s06.imfd.cl/04/CC6204/tareas/tarea4/flickr_cap_encodings_4096d.pkl -P $folder_path\n",
        "\n",
        "with open(f'{folder_path}/flickr_cap_encodings_4096d.pkl', 'rb') as f:\n",
        "  train_cap_encs, val_cap_encs, test_cap_encs = pickle.load(f)\n",
        "\n",
        "# Creamos un dataset para cada uno de los splits con nuestro ImageCaptionDataset\n",
        "train_flickr_tripletset = ImageCaptionDataset(train_flickr_set, train_cap_encs)\n",
        "val_flickr_tripletset = ImageCaptionDataset(val_flickr_set, val_cap_encs)\n",
        "test_flickr_tripletset = ImageCaptionDataset(test_flickr_set, test_cap_encs)\n",
        "\n",
        "##############################################################################\n",
        "# AcÃ¡ termina el cÃ³digo para preparar los datos\n",
        "##############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THIoPtv-w2QY"
      },
      "source": [
        "##############################################################################\n",
        "# Esta es la parte donde tienes que modificar para poder probar tu \n",
        "# implementaciÃ³n. \n",
        "# En general sÃ³lo es necesario que modifiques los lugares con \"...\", pero \n",
        "# eres libre de hacer tus propias implementaciones de todo lo que aparece.\n",
        "##############################################################################\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "LR = 1e-4\n",
        "EPOCHS = ...\n",
        "REPORTS_EVERY = 1\n",
        "CNN_OUT_SIZE = ...\n",
        "EMBEDDING_SIZE = 4096\n",
        "OUT_SIZE = 512\n",
        "MARGIN = .2\n",
        "NEGATIVE = ...\n",
        "\n",
        "cnn_net = ...\n",
        "img_net = ImageEncoding(cnn_model=cnn_net, cnn_out_size=CNN_OUT_SIZE, \n",
        "                        out_size=OUT_SIZE)\n",
        "\n",
        "text_net = TextEncoding(text_embedding_size=EMBEDDING_SIZE, out_size=OUT_SIZE)\n",
        "\n",
        "optimizer = optim.Adam([{'params': ...},  # lista de parametros de img_net\n",
        "                        {'params': ...}],  # lista de parametros de text_net\n",
        "                       lr=LR)\n",
        "criterion = TripletLoss(margin=MARGIN, negative=NEGATIVE)\n",
        "scheduler = ... # (opcional) optim.lr_scheduler proporciona varios mÃ©todos \n",
        "                # para ajustar el lr segÃºn el nÃºmero de Ã©pocas\n",
        "\n",
        "train_triplets_loader = DataLoader(train_flickr_tripletset, batch_size=BATCH_SIZE,\n",
        "                                   shuffle=True, num_workers=2)\n",
        "val_triplets_loader = DataLoader(val_flickr_tripletset, batch_size=BATCH_SIZE,\n",
        "                                 shuffle=False, num_workers=2)\n",
        "\n",
        "train_loss, meanrr, r10 = train_for_retrieval(img_net, text_net, \n",
        "                                              train_triplets_loader, \n",
        "                                              val_triplets_loader, optimizer, \n",
        "                                              criterion, scheduler, EPOCHS, \n",
        "                                              REPORTS_EVERY, norm=False)\n",
        "\n",
        "plot_results(train_loss, meanrr, 'MRR', r10, 'R@10')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CYP1n6c6_c8"
      },
      "source": [
        "# Test\n",
        "from PIL import Image\n",
        "n_samples = 64\n",
        "\n",
        "# Tomemos n_samples ejemplos del conjunto de test\n",
        "samples = torch.stack([test_flickr_tripletset[i][0] for i in range(n_samples)]).cuda()\n",
        "refs = torch.stack([torch.from_numpy(test_flickr_tripletset[i][1]) for i in range(n_samples)]).cuda()\n",
        "test_caps = [caps[0] for _, caps in test_flickr_set][:n_samples]\n",
        "\n",
        "# Computamos las representaciones en el espacio compartido\n",
        "samples_enc = img_net(samples)['logits']\n",
        "refs_enc = text_net(refs)['logits']\n",
        "\n",
        "# Calculemos las distancias a cada uno de los textos de test y rankeamos\n",
        "dists = torch.cdist(samples_enc.unsqueeze(0), refs_enc.unsqueeze(0), p=2).squeeze(0)\n",
        "ranks = torch.argsort(dists, dim=1)[:,:10]\n",
        "r10 = len([i for i in range(len(ranks)) if len(torch.where(ranks[i,:] == i)[0])]) / len(ranks)\n",
        "\n",
        "# Veamos como se comporta el modelo\n",
        "print(\"Correct Test!\" if r10 >= .25 else \"Failed Test! [R@10]\")\n",
        "\n",
        "# Mostremos las 10 descripciones mÃ¡s cercanas\n",
        "fig, axs = plt.subplots(nrows=n_samples, figsize=(2,n_samples*5))\n",
        "for i in range(n_samples):\n",
        "  axs[i].imshow(Image.open(full_flickr_set.ids[7000+i]))\n",
        "  axs[i].text(600,0,\"EXPECTED:\\n{}: {}\".format(i, test_caps[i]), fontsize=12, fontweight='bold')\n",
        "  axs[i].text(600,750,\"PREDICTED RANK:\\n{}\".format('\\n'.join([f'{j}: {test_caps[j]}' for j in ranks[i]])), fontsize=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTd9gUB5xwta"
      },
      "source": [
        "## 2d) Opcional: COCO Captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf0hHCkKx0Kz"
      },
      "source": [
        "##############################################################################\n",
        "# Toda esta parte es similar a la anterior pero para COCO Captions.\n",
        "##############################################################################\n",
        "\n",
        "folder_path = './data/coco-caps'\n",
        "if not os.path.exists(f'{folder_path}/images/train2014'):\n",
        "  print('\\n*** Descargando y extrayendo COCO Captions, siÃ©ntese y relÃ¡jese unos 20 mins...')\n",
        "  print('****** Descargando training set...\\n')\n",
        "  !wget http://images.cocodataset.org/zips/train2014.zip -P $folder_path/images\n",
        "  print('\\n********* Extrayendo training set...\\n  Si te sale mensaje de colab, dale Ignorar\\n')\n",
        "  !unzip -q $folder_path/images/train2014.zip -d $folder_path/images && rm $folder_path/images/train2014.zip\n",
        "  print('\\n*** Descargando y extrayendo validation set...\\n')\n",
        "  !wget http://images.cocodataset.org/zips/val2014.zip -P $folder_path/images && unzip -q $folder_path/images/val2014.zip -d $folder_path/images && rm $folder_path/images/val2014.zip\n",
        "  # !wget http://images.cocodataset.org/zips/test2014.zip -P $folder_path/images && unzip -q $folder_path/images/test2014.zip -d $folder_path/images && rm $folder_path/images/test2014.zip\n",
        "  print('\\n*** Descargando y anotaciones de la imÃ¡genes...\\n')\n",
        "  !wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip -P $folder_path && unzip -q $folder_path/annotations_trainval2014.zip -d $folder_path && rm $folder_path/images/annotations_trainval2014.zip\n",
        "\n",
        "transform=transforms.Compose([transforms.ToTensor(), \n",
        "                              transforms.Resize((32, 32)),\n",
        "                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "train_coco_set = torchvision.datasets.CocoCaptions(root=f'{folder_path}/images/train2014',\n",
        "                                                   annFile = f'{folder_path}/annotations/captions_train2014.json',\n",
        "                                                   transform=transform)\n",
        "\n",
        "val_coco_set = torchvision.datasets.CocoCaptions(root=f'{folder_path}/images/val2014',\n",
        "                                                 annFile = f'{folder_path}/annotations/captions_val2014.json',\n",
        "                                                 transform=transform)\n",
        "\n",
        "# test_coco_set = torchvision.datasets.CocoCaptions(root=f'{folder_path}/images/test2014',\n",
        "#                                                   transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUZmJrvfJOxs"
      },
      "source": [
        "if not os.path.exists(f'{folder_path}/cap_encodings_512d.pkl'):\n",
        "  !wget https://s06.imfd.cl/04/CC6204/tareas/tarea4/cap_encodings_512d.pkl -P $folder_path\n",
        "\n",
        "with open(f'{folder_path}/cap_encodings_512d.pkl', 'rb') as f:\n",
        "  train_cap_encs, val_cap_encs = pickle.load(f)\n",
        "\n",
        "train_coco_tripletset = ImageCaptionDataset(train_coco_set, train_cap_encs)\n",
        "val_coco_tripletset = ImageCaptionDataset(val_coco_set, val_cap_encs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXxbiC0bJY6M"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "LR = 1e-4\n",
        "EPOCHS = 10\n",
        "REPORTS_EVERY = 1\n",
        "CNN_PREV_SIZE = 1024\n",
        "EMBEDDING_SIZE = 512\n",
        "OUT_SIZE = 512\n",
        "MARGIN = .2\n",
        "\n",
        "cnn_net = ...\n",
        "img_net = ImageEncoding(cnn_model=..., cnn_out_size=CNN_PREV_SIZE, \n",
        "                        out_size=OUT_SIZE) \n",
        "\n",
        "text_net = TextEncoding(text_embedding_size=EMBEDDING_SIZE, out_size=OUT_SIZE)\n",
        "\n",
        "optimizer = optim.Adam([{'params': ...},\n",
        "                        {'params': ...}], \n",
        "                       lr=LR)\n",
        "criterion = TripletLoss(margin=...)\n",
        "\n",
        "train_triplets_loader = DataLoader(train_coco_tripletset, batch_size=BATCH_SIZE,\n",
        "                                   shuffle=True, num_workers=2)\n",
        "val_triplets_loader = DataLoader(val_coco_tripletset, batch_size=BATCH_SIZE,\n",
        "                                 shuffle=False, num_workers=2)\n",
        "\n",
        "train_loss, meanrr, r10 = train_for_retrieval(img_net, text_net, \n",
        "                                                     train_triplets_loader, \n",
        "                                                     val_triplets_loader, \n",
        "                                                     optimizer, criterion, \n",
        "                                                     EPOCHS, REPORTS_EVERY, \n",
        "                                                     norm=False)\n",
        "\n",
        "plot_results(train_loss, meanrr, 'MRR', r10, 'R@10')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}